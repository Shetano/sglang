"""Test token ID alignment between SGLang and Hugging Face.

This test suite ensures that the token IDs generated by the SGLang Engine
are consistent with those of the Hugging Face tokenizer for a given set of
prompts. Specifically, it checks:
1. Whether the input token IDs from the SGLang Engine match those produced
   by the Hugging Face tokenizer for the same input string.
2. Whether the output token IDs from the SGLang Engine match those produced
   by the Hugging Face tokenizer (excluding the start token, which is
   typically added by Hugging Face but not returned by SGLang).
3. Whether the meta information, such as prompt token count and completion
   token count, aligns with the actual lengths of input and output token IDs.
4. Whether the completion length matches a specific expected max tokens (128
   in this example).

"""

import unittest

from transformers import AutoTokenizer

import sglang as sgl


def get_test_engine():
    """Returns a test engine with the 'Meta-Llama-3.1-8B-Instruct' model."""
    return sgl.Engine(
        model_path="meta-llama/Meta-Llama-3.1-8B-Instruct", return_token_ids=True
    )


class TestEngineTokenIds(unittest.TestCase):
    """Tests SGLang's token IDs against Hugging Face tokenizer."""

    def setUp(self):
        """Creates engine, tokenizer, and prompts."""
        self.llm = get_test_engine()
        self.tokenizer = AutoTokenizer.from_pretrained(
            "meta-llama/Meta-Llama-3.1-8B-Instruct"
        )
        self.prompts = [
            "Hello, my name is",
            "The president of the United States is",
            "The capital of France is",
            "The future of AI is",
        ]
        self.sampling_params = {"temperature": 0.8, "top_p": 0.95}

    def tearDown(self):
        """Shuts down the engine."""
        self.llm.shutdown()

    def test_token_ids_in_generate(self):
        """Checks token IDs and meta info from SGLang vs. Hugging Face."""
        outputs = self.llm.generate(self.prompts, self.sampling_params)

        for prompt, output in zip(self.prompts, outputs):
            hf_input_ids = self.tokenizer.encode(prompt)
            self.assertEqual(
                output["input_ids"],
                hf_input_ids,
                f"Input token IDs mismatch for: {prompt}",
            )

            # Remove start token from HuggingFace as SGLang output doesn't include it
            hf_output_ids = self.tokenizer.encode(output["text"])[1:]
            sgl_output_ids = output["output_ids"]
            self.assertEqual(
                sgl_output_ids,
                hf_output_ids,
                f"Output token IDs mismatch for: {output['text']}",
            )

            self.assertEqual(
                len(output["input_ids"]),
                output["meta_info"]["prompt_tokens"],
                "Prompt token count mismatch",
            )
            self.assertEqual(
                len(sgl_output_ids),
                output["meta_info"]["completion_tokens"],
                "Completion token count mismatch",
            )
            self.assertEqual(
                output["meta_info"]["completion_tokens"],
                128,
                "Unexpected completion length",
            )


if __name__ == "__main__":
    unittest.main()
