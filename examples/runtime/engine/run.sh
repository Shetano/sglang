python offline_batch_inference_vlm.py --model-path /home/xiaoxiang/pytorch-paligemma/paligemma-3b-pt-224 --chat-template=paligemma > pali.log 2>&1 