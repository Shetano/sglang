{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frontend: Structured Generation Language (SGLang)\n",
    "\n",
    "The frontend language can be used with local models or API models. It is an alternative to the OpenAI API. You may find it easier to use for complex prompting workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sglang.utils import (\n",
    "    execute_shell_command,\n",
    "    wait_for_server,\n",
    "    terminate_process,\n",
    "    print_highlight,\n",
    ")\n",
    "\n",
    "server_process = execute_shell_command(\n",
    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-1.5B-Instruct --mem-fraction-static 0.8 --port 30333 --host 0.0.0.0\"\n",
    ")\n",
    "wait_for_server(\"http://localhost:30333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup default backend for SGLang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sglang import set_default_backend, RuntimeEndpoint\n",
    "\n",
    "set_default_backend(RuntimeEndpoint(\"http://localhost:30333\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-turn conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGLang provides simple api to build multi-turn conversations. Prompt templates can be defined intuitively with the `function` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sglang import function, system, user, assistant, gen\n",
    "\n",
    "\n",
    "@function\n",
    "def multi_turn_conversation(s, country: str):\n",
    "    s += system(\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\")\n",
    "    s += user(f\"What is the capital of {country}?\")\n",
    "    s += assistant(gen(\"capital\", max_tokens=250))\n",
    "    s += user(\"Name an interesting building in this city.\")\n",
    "    s += assistant(gen(\"building\", max_tokens=250))\n",
    "\n",
    "\n",
    "state = multi_turn_conversation.run(\n",
    "    country=\"Germany\",\n",
    ")\n",
    "\n",
    "for m in state.messages():\n",
    "    print(m[\"role\"], \":\", m[\"content\"])\n",
    "\n",
    "print_highlight(\"#\" * 50)\n",
    "print_highlight(state[\"capital\"])\n",
    "print_highlight(\"#\" * 50)\n",
    "print_highlight(state[\"building\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use SGLang for OpenAI models as well.\n",
    "\n",
    "For that we only need to execute `export OPENAI_API_KEY=<your-openai-api-key>` and then `set_default_backend(OpenAI(<chosen-model>))`. Everything else stays exactly the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGLang's choices method is a powerful tool to control the flow of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function\n",
    "def control_flow(s, question: str):\n",
    "    s += user(question)\n",
    "    s += assistant(\n",
    "        \"Based on the question, this seems like \"\n",
    "        + gen(\"type\", choices=[\"a technical query\", \"a creative request\"])\n",
    "    )\n",
    "\n",
    "    if s[\"type\"] == \"a technical query\":\n",
    "        s += assistant(\n",
    "            \"Here's a technical explanation: \"\n",
    "            + gen(\"technical_response\", max_tokens=250)\n",
    "        )\n",
    "    else:\n",
    "        s += assistant(\n",
    "            \"Here's a creative response: \" + gen(\"creative_response\", max_tokens=250)\n",
    "        )\n",
    "\n",
    "\n",
    "state = control_flow.run(\n",
    "    question=\"What is the main difference between a CPU and a GPU?\"\n",
    ")\n",
    "print_highlight(state[\"technical_response\"])\n",
    "print_highlight(\"#\" * 50)\n",
    "state = control_flow.run(question=\"Can you help me write a story about time travel?\")\n",
    "print_highlight(state[\"creative_response\"])\n",
    "print_highlight(\"#\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGLang supports parallelism. `fork` can be used to launch multiple prompts in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function\n",
    "def parallel_sample(s, question, n):\n",
    "    s += user(question)\n",
    "    forks = s.fork(n)\n",
    "    forks += assistant(gen(\"answer\", temperature=0.7))\n",
    "    forks.join()\n",
    "\n",
    "\n",
    "states = parallel_sample.run(\n",
    "    question=\"What does the integral of sin(x) from 0 to 2pi evaluate to? Answer without calculation.\",\n",
    "    n=5,\n",
    ")\n",
    "for answer in states[\"answer\"]:\n",
    "    print_highlight(answer)\n",
    "    print_highlight(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGLang supports constrained decoding for structured outputs. The output format can be specified in form of a regular expression.\n",
    "\n",
    "*Note: This is only supported for local models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function\n",
    "def regular_expression_gen(s):\n",
    "    s += user(\"What is the birth date of Albert Einstein?\")\n",
    "    s += assistant(\n",
    "        gen(\n",
    "            \"answer\",\n",
    "            temperature=0,\n",
    "            regex=r\"\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "state = regular_expression_gen.run()\n",
    "print_highlight(state[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression can also be used for schema extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "character_regex = (\n",
    "    r\"\"\"\\{\\n\"\"\"\n",
    "    + r\"\"\"    \"name\": \"[\\w\\d\\s]{1,16}\",\\n\"\"\"\n",
    "    + r\"\"\"    \"house\": \"(Gryffindor|Slytherin|Ravenclaw|Hufflepuff)\",\\n\"\"\"\n",
    "    + r\"\"\"    \"blood status\": \"(Pure-blood|Half-blood|Muggle-born)\",\\n\"\"\"\n",
    "    + r\"\"\"    \"occupation\": \"(student|teacher|auror|ministry of magic|death eater|order of the phoenix)\",\\n\"\"\"\n",
    "    + r\"\"\"    \"wand\": \\{\\n\"\"\"\n",
    "    + r\"\"\"        \"wood\": \"[\\w\\d\\s]{1,16}\",\\n\"\"\"\n",
    "    + r\"\"\"        \"core\": \"[\\w\\d\\s]{1,16}\",\\n\"\"\"\n",
    "    + r\"\"\"        \"length\": [0-9]{1,2}\\.[0-9]{0,2}\\n\"\"\"\n",
    "    + r\"\"\"    \\},\\n\"\"\"\n",
    "    + r\"\"\"    \"alive\": \"(Alive|Deceased)\",\\n\"\"\"\n",
    "    + r\"\"\"    \"patronus\": \"[\\w\\d\\s]{1,16}\",\\n\"\"\"\n",
    "    + r\"\"\"    \"bogart\": \"[\\w\\d\\s]{1,16}\"\\n\"\"\"\n",
    "    + r\"\"\"\\}\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "@function\n",
    "def generate_character(s, name):\n",
    "    s += system(\n",
    "        \"You are a helpful assistant that extracts information about a character from a text.\"\n",
    "    )\n",
    "    s += user(f\"Extract the relevant information about {name}.\")\n",
    "    s += assistant(gen(\"character\", regex=character_regex, max_tokens=256))\n",
    "\n",
    "\n",
    "state = generate_character.run(name=\"Harry Potter\")\n",
    "print_highlight(state[\"character\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_batch` can be used to run prompts with continous batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function\n",
    "def simple_qa(s, question: str):\n",
    "    s += user(question)\n",
    "    s += assistant(gen(\"answer\", max_tokens=128, stop=[\"assistant\"], temperature=0))\n",
    "\n",
    "\n",
    "states = simple_qa.run_batch(\n",
    "    [\n",
    "        {\"question\": \"Who was the first man on the moon?\"},\n",
    "        {\"question\": \"Who was Lev Landau?\"},\n",
    "        {\"question\": \"Please tell me a joke about a chicken.\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "for state in states:\n",
    "    print_highlight(f\"Answer: {state['answer']}\")\n",
    "    print_highlight(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stream` can be used to stream the response from the model.\n",
    "\n",
    "*Note: We use* `print_highlight` *here to keep the color convention. In practice, we would use* `print(out, end=\"\", flush=True)` *to stream the response.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function\n",
    "def stream_qa(s, question: str):\n",
    "    s += user(question)\n",
    "    s += assistant(gen(\"answer\", max_tokens=128, stop=[\"assistant\"], temperature=0))\n",
    "\n",
    "\n",
    "state = stream_qa.run(question=\"Who was the first man on the moon?\", stream=True)\n",
    "for out in state.text_iter():\n",
    "    print_highlight(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[user|assistant|system]_[begin|end]` can be used to define more complex prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sglang import user_begin, user_end, assistant_begin, assistant_end\n",
    "\n",
    "\n",
    "@function\n",
    "def roles(s):\n",
    "    s += system(\n",
    "        \"You talk like a pirate and use frequently phrases like 'arrr' and 'yo-ho-ho'.\"\n",
    "    )\n",
    "    s += user_begin()\n",
    "    s += \"Hello, how do you like life as a pirate?\"\n",
    "    s += user_end()\n",
    "    s += assistant_begin()\n",
    "    s += \"There is much to tell about the life of a pirate.\" + gen(\n",
    "        \"story\", max_tokens=128\n",
    "    )\n",
    "    s += assistant_end()\n",
    "\n",
    "\n",
    "state = roles.run()\n",
    "print_highlight(state[\"story\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-modal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGLang supports a variety of [multi-modal models](https://docs.sglang.ai/backend/openai_api_vision.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_process = execute_shell_command(\n",
    "    \"python3 -m sglang.launch_server --model-path lmms-lab/llama3-llava-next-8b --mem-fraction-static 0.8 --port 30333 --host 0.0.0.0\"\n",
    ")\n",
    "wait_for_server(\"http://localhost:30333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_backend(RuntimeEndpoint(\"http://localhost:30333\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `image` to pass an image to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O example_image.png https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sglang import image\n",
    "\n",
    "\n",
    "@function\n",
    "def image_qa(s, image_file, question):\n",
    "    s += user(image(image_file) + question)\n",
    "    s += assistant(gen(\"answer\", max_tokens=128, stop=[\"assistant\"]))\n",
    "\n",
    "\n",
    "state = image_qa.run(\n",
    "    image_file=\"example_image.png\", question=\"Describe the image in one short sentence.\"\n",
    ")\n",
    "print_highlight(state[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm example_image.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more familar with SGLang we recommend to start studying the [benchmark scripts](https://github.com/sgl-project/sglang/tree/main/benchmark)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
