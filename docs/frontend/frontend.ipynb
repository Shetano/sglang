{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frontend: Structured Generation Language (SGLang)\n",
    "\n",
    "The frontend language can be used with local models or API models. It is an alternative to the OpenAI API. You may find it easier to use for complex prompting workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 21:02:52 __init__.py:190] Automatically detected platform cuda.\n",
      "[2025-02-13 21:02:57] server_args=ServerArgs(model_path='Qwen/Qwen2.5-1.5B-Instruct', tokenizer_path='Qwen/Qwen2.5-1.5B-Instruct', tokenizer_mode='auto', load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='Qwen/Qwen2.5-1.5B-Instruct', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=30333, mem_fraction_static=0.8, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, stream_output=False, random_seed=502020878, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False)\n",
      "INFO 02-13 21:03:00 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 02-13 21:03:00 __init__.py:190] Automatically detected platform cuda.\n",
      "[2025-02-13 21:03:06 TP0] Init torch distributed begin.\n",
      "[2025-02-13 21:03:06 TP0] Load weight begin. avail mem=78.84 GB\n",
      "[2025-02-13 21:03:07 TP0] Using model weights format ['*.safetensors']\n",
      "[2025-02-13 21:03:07 TP0] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.81it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.81it/s]\n",
      "\n",
      "[2025-02-13 21:03:08 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=75.74 GB\n",
      "[2025-02-13 21:03:08 TP0] KV Cache is allocated. K size: 29.99 GB, V size: 29.99 GB.\n",
      "[2025-02-13 21:03:08 TP0] Memory pool end. avail mem=15.19 GB\n",
      "[2025-02-13 21:03:08 TP0] Capture cuda graph begin. This can take up to several minutes.\n",
      "100%|██████████| 23/23 [00:05<00:00,  4.03it/s]\n",
      "[2025-02-13 21:03:14 TP0] Capture cuda graph end. Time elapsed: 5.71 s\n",
      "[2025-02-13 21:03:14 TP0] max_total_num_tokens=2245866, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4097, context_len=32768\n",
      "[2025-02-13 21:03:14] INFO:     Started server process [2688681]\n",
      "[2025-02-13 21:03:14] INFO:     Waiting for application startup.\n",
      "[2025-02-13 21:03:14] INFO:     Application startup complete.\n",
      "[2025-02-13 21:03:14] INFO:     Uvicorn running on http://0.0.0.0:30333 (Press CTRL+C to quit)\n",
      "[2025-02-13 21:03:14] INFO:     127.0.0.1:49834 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:15] INFO:     127.0.0.1:49846 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:15 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:16] INFO:     127.0.0.1:49862 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:16] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.utils import (\n",
    "    execute_shell_command,\n",
    "    wait_for_server,\n",
    "    terminate_process,\n",
    "    print_highlight,\n",
    ")\n",
    "\n",
    "server_process = execute_shell_command(\n",
    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-1.5B-Instruct --mem-fraction-static 0.8 --port 30333 --host 0.0.0.0\"\n",
    ")\n",
    "wait_for_server(\"http://localhost:30333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup default backend for SGLang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:19] INFO:     127.0.0.1:51850 - \"GET /get_model_info HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from sglang import set_default_backend, RuntimeEndpoint\n",
    "\n",
    "set_default_backend(RuntimeEndpoint(\"http://localhost:30333\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-turn conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGLang provides simple api to build multi-turn conversations. Prompt templates can be defined intuitively with the `function` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:19 TP0] Prefill batch. #new-seq: 1, #new-token: 36, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:20] INFO:     127.0.0.1:51860 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:20 TP0] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 43, cache hit rate: 41.75%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:20 TP0] Decode batch. #running-req: 1, #token: 86, token usage: 0.00, gen throughput (token/s): 7.04, #queue-req: 0\n",
      "[2025-02-13 21:03:20 TP0] Decode batch. #running-req: 1, #token: 126, token usage: 0.00, gen throughput (token/s): 244.17, #queue-req: 0\n",
      "[2025-02-13 21:03:20 TP0] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, gen throughput (token/s): 241.28, #queue-req: 0\n",
      "[2025-02-13 21:03:20 TP0] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, gen throughput (token/s): 241.34, #queue-req: 0\n",
      "[2025-02-13 21:03:20] INFO:     127.0.0.1:51866 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "system : You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user : What is the capital of Germany?\n",
      "assistant : The capital of Germany is Berlin.\n",
      "user : Name an interesting building in this city.\n",
      "assistant : Berlin, the capital of Germany, is home to many architectural wonders, but one particularly notable building is the Berlin Philharmonium, formerly known as the Berlin Topographie Museum. The Philharmonium was once a large concert hall and music school. It became famous for its installations with musical instruments ranging from flutes, trumpets, cymbals, xylophones and even animals. In 2015, it was closed to visitors due to safety concerns, and it was decided that the building would be adapted and transformed into a public museum. The museum self-organizes events, such as film screenings and music performances, and welcomes the public to experience modern art and historical narratives. Additionally, the site is still being considered for transformation into a cultural district which would provide room for hiking trails, food markets, and cultural heritage exhibitions, thereby enabling wider public accessibility and civic engagement.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>##################################################</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The capital of Germany is Berlin.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>##################################################</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Berlin, the capital of Germany, is home to many architectural wonders, but one particularly notable building is the Berlin Philharmonium, formerly known as the Berlin Topographie Museum. The Philharmonium was once a large concert hall and music school. It became famous for its installations with musical instruments ranging from flutes, trumpets, cymbals, xylophones and even animals. In 2015, it was closed to visitors due to safety concerns, and it was decided that the building would be adapted and transformed into a public museum. The museum self-organizes events, such as film screenings and music performances, and welcomes the public to experience modern art and historical narratives. Additionally, the site is still being considered for transformation into a cultural district which would provide room for hiking trails, food markets, and cultural heritage exhibitions, thereby enabling wider public accessibility and civic engagement.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang import function, system, user, assistant, gen\n",
    "\n",
    "\n",
    "@function\n",
    "def multi_turn_conversation(s, country: str):\n",
    "    s += system(\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\")\n",
    "    s += user(f\"What is the capital of {country}?\")\n",
    "    s += assistant(gen(\"capital\", max_tokens=250))\n",
    "    s += user(\"Name an interesting building in this city.\")\n",
    "    s += assistant(gen(\"building\", max_tokens=250))\n",
    "\n",
    "\n",
    "state = multi_turn_conversation.run(\n",
    "    country=\"Germany\",\n",
    ")\n",
    "\n",
    "for m in state.messages():\n",
    "    print(m[\"role\"], \":\", m[\"content\"])\n",
    "\n",
    "print_highlight(\"#\" * 50)\n",
    "print_highlight(state[\"capital\"])\n",
    "print_highlight(\"#\" * 50)\n",
    "print_highlight(state[\"building\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use SGLang for OpenAI models as well.\n",
    "\n",
    "For that we only need to execute `export OPENAI_API_KEY=<your-openai-api-key>` and then `set_default_backend(OpenAI(<chosen-model>))`. Everything else stays exactly the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGLang's choices method is a powerful tool to control the flow of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:20 TP0] Prefill batch. #new-seq: 1, #new-token: 35, #cached-token: 5, cache hit rate: 33.57%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:20] INFO:     127.0.0.1:51882 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:20 TP0] Prefill batch. #new-seq: 2, #new-token: 8, #cached-token: 76, cache hit rate: 54.63%, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:20] INFO:     127.0.0.1:51886 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:20 TP0] Decode batch. #running-req: 0, #token: 0, token usage: 0.00, gen throughput (token/s): 137.10, #queue-req: 0\n",
      "[2025-02-13 21:03:20 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 42, cache hit rate: 59.07%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:21 TP0] Decode batch. #running-req: 1, #token: 95, token usage: 0.00, gen throughput (token/s): 212.97, #queue-req: 0\n",
      "[2025-02-13 21:03:21 TP0] Decode batch. #running-req: 1, #token: 135, token usage: 0.00, gen throughput (token/s): 243.44, #queue-req: 0\n",
      "[2025-02-13 21:03:21 TP0] Decode batch. #running-req: 1, #token: 175, token usage: 0.00, gen throughput (token/s): 241.00, #queue-req: 0\n",
      "[2025-02-13 21:03:21] INFO:     127.0.0.1:51894 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>1) The core of a CPU is a Central Processing Unit. The purpose of a CPU is to process data and complete computation tasks in a computer. The core of a GPU  is a Graphics Processing Unit. The purpose of a GPU is to process graphics data for computers and displays. The function of a CPU is fundamentally computational, while the main function of a GPU is graphics rendering. So the main difference between a CPU and a GPU is that the CPR is primarily a CPU processor, while the GPU is primarily a GPU. In short, the CPU is designed to handle numbers and complete computations, while the GPU is designed to handle graphics and images.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>##################################################</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:21 TP0] Prefill batch. #new-seq: 1, #new-token: 25, #cached-token: 14, cache hit rate: 56.25%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:21] INFO:     127.0.0.1:51908 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:21 TP0] Prefill batch. #new-seq: 2, #new-token: 8, #cached-token: 74, cache hit rate: 63.18%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:21] INFO:     127.0.0.1:51924 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:21 TP0] Prefill batch. #new-seq: 1, #new-token: 12, #cached-token: 41, cache hit rate: 64.84%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:21 TP0] Decode batch. #running-req: 1, #token: 81, token usage: 0.00, gen throughput (token/s): 169.42, #queue-req: 0\n",
      "[2025-02-13 21:03:21 TP0] Decode batch. #running-req: 1, #token: 121, token usage: 0.00, gen throughput (token/s): 243.75, #queue-req: 0\n",
      "[2025-02-13 21:03:22 TP0] Decode batch. #running-req: 1, #token: 161, token usage: 0.00, gen throughput (token/s): 241.19, #queue-req: 0\n",
      "[2025-02-13 21:03:22 TP0] Decode batch. #running-req: 1, #token: 201, token usage: 0.00, gen throughput (token/s): 240.60, #queue-req: 0\n",
      "[2025-02-13 21:03:22] INFO:     127.0.0.1:51930 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> A group of archaeologists discovers the ruins of an ancient city on the edge of the ocean. As they dig deeper, they discover a time machine hidden beneath the sand. The archaeologists decide to embark on a mission, taking a year to experience the wonders of the ancient world. During their time in the past, they witness the pyramids being built, Alexander the Great conquering Babylon, and King Tut arriving in Upper Egypt. When they return to the present, they reflect on the richness of their experience and realize that life in the future could be just as exciting. The group decides to create a documentary to show future generations the path they took. As they embark on this adventure, their journey becomes a tale of hope, discovery, and the power of unique experiences to shape our lives.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>##################################################</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@function\n",
    "def control_flow(s, question: str):\n",
    "    s += user(question)\n",
    "    s += assistant(\n",
    "        \"Based on the question, this seems like \"\n",
    "        + gen(\"type\", choices=[\"a technical query\", \"a creative request\"])\n",
    "    )\n",
    "\n",
    "    if s[\"type\"] == \"a technical query\":\n",
    "        s += assistant(\n",
    "            \"Here's a technical explanation: \"\n",
    "            + gen(\"technical_response\", max_tokens=250)\n",
    "        )\n",
    "    else:\n",
    "        s += assistant(\n",
    "            \"Here's a creative response: \" + gen(\"creative_response\", max_tokens=250)\n",
    "        )\n",
    "\n",
    "\n",
    "state = control_flow.run(\n",
    "    question=\"What is the main difference between a CPU and a GPU?\"\n",
    ")\n",
    "print_highlight(state[\"technical_response\"])\n",
    "print_highlight(\"#\" * 50)\n",
    "state = control_flow.run(question=\"Can you help me write a story about time travel?\")\n",
    "print_highlight(state[\"creative_response\"])\n",
    "print_highlight(\"#\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGLang supports parallelism. `fork` can be used to launch multiple prompts in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:22 TP0] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 15, cache hit rate: 62.50%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:22 TP0] Prefill batch. #new-seq: 1, #new-token: 26, #cached-token: 15, cache hit rate: 60.52%, token usage: 0.00, #running-req: 1, #queue-req: 0\n",
      "[2025-02-13 21:03:22 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 120, cache hit rate: 67.42%, token usage: 0.00, #running-req: 2, #queue-req: 0\n",
      "[2025-02-13 21:03:22] INFO:     127.0.0.1:51968 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:22] INFO:     127.0.0.1:51960 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:22] INFO:     127.0.0.1:51946 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:22] INFO:     127.0.0.1:51970 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:22] INFO:     127.0.0.1:51986 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The integral of sin(x) from 0 to 2π evaluates to 0.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>--------------------------------------------------</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The integral of sin(x) from 0 to 2π evaluates to 0.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>--------------------------------------------------</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The integral of sin(x) from 0 to 2π is 0.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>--------------------------------------------------</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The integral of sin(x) from 0 to 2pi evaluates to 0.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>--------------------------------------------------</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The integral of sin(x) from 0 to 2π evaluates to 0.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>--------------------------------------------------</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@function\n",
    "def parallel_sample(s, question, n):\n",
    "    s += user(question)\n",
    "    forks = s.fork(n)\n",
    "    forks += assistant(gen(\"answer\", temperature=0.7))\n",
    "    forks.join()\n",
    "\n",
    "\n",
    "states = parallel_sample.run(\n",
    "    question=\"What does the integral of sin(x) from 0 to 2pi evaluate to? Answer without calculation.\",\n",
    "    n=5,\n",
    ")\n",
    "for answer in states[\"answer\"]:\n",
    "    print_highlight(answer)\n",
    "    print_highlight(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGLang supports constrained decoding for structured outputs. The output format can be specified in form of a regular expression.\n",
    "\n",
    "*Note: This is only supported for local models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:23 TP0] PyTorch version 2.5.1 available.\n",
      "[2025-02-13 21:03:24 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 17, cache hit rate: 67.15%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "/home/misc/simon/sglang/.venv/lib/python3.10/site-packages/sglang/srt/constrained/outlines_backend.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens = torch.tensor(\n",
      "[2025-02-13 21:03:24 TP0] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, gen throughput (token/s): 42.93, #queue-req: 0\n",
      "[2025-02-13 21:03:24] INFO:     127.0.0.1:51990 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>18/3/1879</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@function\n",
    "def regular_expression_gen(s):\n",
    "    s += user(\"What is the birth date of Albert Einstein?\")\n",
    "    s += assistant(\n",
    "        gen(\n",
    "            \"answer\",\n",
    "            temperature=0,\n",
    "            regex=r\"\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "state = regular_expression_gen.run()\n",
    "print_highlight(state[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression can also be used for schema extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:29 TP0] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 8, cache hit rate: 64.92%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:30 TP0] Decode batch. #running-req: 1, #token: 76, token usage: 0.00, gen throughput (token/s): 7.61, #queue-req: 0\n",
      "[2025-02-13 21:03:30 TP0] Decode batch. #running-req: 1, #token: 116, token usage: 0.00, gen throughput (token/s): 186.78, #queue-req: 0\n",
      "[2025-02-13 21:03:30 TP0] Decode batch. #running-req: 1, #token: 156, token usage: 0.00, gen throughput (token/s): 212.16, #queue-req: 0\n",
      "[2025-02-13 21:03:30] INFO:     127.0.0.1:52006 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>{<br>    \"name\": \"Harry Potter\",<br>    \"house\": \"Gryffindor\",<br>    \"blood status\": \"Muggle-born\",<br>    \"occupation\": \"student\",<br>    \"wand\": {<br>        \"wood\": \"Hawthorn\",<br>        \"core\": \"Phoenix feather\",<br>        \"length\": 12.5<br>    },<br>    \"alive\": \"Alive\",<br>    \"patronus\": \"Otter\",<br>    \"bogart\": \"No\"<br>}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "character_regex = (\n",
    "    r\"\"\"\\{\\n\"\"\"\n",
    "    + r\"\"\"    \"name\": \"[\\w\\d\\s]{1,16}\",\\n\"\"\"\n",
    "    + r\"\"\"    \"house\": \"(Gryffindor|Slytherin|Ravenclaw|Hufflepuff)\",\\n\"\"\"\n",
    "    + r\"\"\"    \"blood status\": \"(Pure-blood|Half-blood|Muggle-born)\",\\n\"\"\"\n",
    "    + r\"\"\"    \"occupation\": \"(student|teacher|auror|ministry of magic|death eater|order of the phoenix)\",\\n\"\"\"\n",
    "    + r\"\"\"    \"wand\": \\{\\n\"\"\"\n",
    "    + r\"\"\"        \"wood\": \"[\\w\\d\\s]{1,16}\",\\n\"\"\"\n",
    "    + r\"\"\"        \"core\": \"[\\w\\d\\s]{1,16}\",\\n\"\"\"\n",
    "    + r\"\"\"        \"length\": [0-9]{1,2}\\.[0-9]{0,2}\\n\"\"\"\n",
    "    + r\"\"\"    \\},\\n\"\"\"\n",
    "    + r\"\"\"    \"alive\": \"(Alive|Deceased)\",\\n\"\"\"\n",
    "    + r\"\"\"    \"patronus\": \"[\\w\\d\\s]{1,16}\",\\n\"\"\"\n",
    "    + r\"\"\"    \"bogart\": \"[\\w\\d\\s]{1,16}\"\\n\"\"\"\n",
    "    + r\"\"\"\\}\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "@function\n",
    "def generate_character(s, name):\n",
    "    s += system(\n",
    "        \"You are a helpful assistant that extracts information about a character from a text.\"\n",
    "    )\n",
    "    s += user(f\"Extract the relevant information about {name}.\")\n",
    "    s += assistant(gen(\"character\", regex=character_regex, max_tokens=256))\n",
    "\n",
    "\n",
    "state = generate_character.run(name=\"Harry Potter\")\n",
    "print_highlight(state[\"character\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_batch` can be used to run prompts with continous batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:30 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 13, cache hit rate: 65.45%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:30] INFO:     127.0.0.1:55672 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:30 TP0] Prefill batch. #new-seq: 2, #new-token: 25, #cached-token: 28, cache hit rate: 64.60%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:30 TP0] Prefill batch. #new-seq: 1, #new-token: 14, #cached-token: 14, cache hit rate: 64.10%, token usage: 0.00, #running-req: 2, #queue-req: 0\n",
      "[2025-02-13 21:03:30] INFO:     127.0.0.1:55704 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:30 TP0] Decode batch. #running-req: 2, #token: 89, token usage: 0.00, gen throughput (token/s): 325.52, #queue-req: 0\n",
      "[2025-02-13 21:03:30] INFO:     127.0.0.1:55684 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:30 TP0] Decode batch. #running-req: 1, #token: 91, token usage: 0.00, gen throughput (token/s): 321.15, #queue-req: 0\n",
      "[2025-02-13 21:03:31 TP0] Decode batch. #running-req: 1, #token: 131, token usage: 0.00, gen throughput (token/s): 250.02, #queue-req: 0\n",
      "[2025-02-13 21:03:31] INFO:     127.0.0.1:55688 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Answer: The first man on the moon was Neil Armstrong. He landed on the moon on July 20, 1969, during the Apollo 11 mission.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>--------------------------------------------------</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Answer: Lev Davidovich Landau (Russian: Ле́в Давидович Ландау, born Lev Davidovich Lifshitz; 7 March 1908 – 19 October 1968) was a Soviet physicist and a leading figure in the development of quantum theory. He was a professor at the Moscow State University and the director of the Institute of Theoretical Physics in Moscow. Landau was known for his work in quantum mechanics, statistical mechanics, and the theory of superfluidity. He was also a prolific author, having written over 1,000 scientific papers and several books</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>--------------------------------------------------</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>Answer: Why did the chicken cross the road? To get to the other side of the internet!</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>--------------------------------------------------</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@function\n",
    "def simple_qa(s, question: str):\n",
    "    s += user(question)\n",
    "    s += assistant(gen(\"answer\", max_tokens=128, stop=[\"assistant\"], temperature=0))\n",
    "\n",
    "\n",
    "states = simple_qa.run_batch(\n",
    "    [\n",
    "        {\"question\": \"Who was the first man on the moon?\"},\n",
    "        {\"question\": \"Who was Lev Landau?\"},\n",
    "        {\"question\": \"Please tell me a joke about a chicken.\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "for state in states:\n",
    "    print_highlight(f\"Answer: {state['answer']}\")\n",
    "    print_highlight(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stream` can be used to stream the response from the model.\n",
    "\n",
    "*Note: We use* `print_highlight` *here to keep the color convention. In practice, we would use* `print(out, end=\"\", flush=True)` *to stream the response.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><|im_start|>system<br>You are a helpful assistant.<|im_end|><br><|im_start|>user<br>Who was the first man on the moon?<|im_end|><br><|im_start|>assistant<br></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:31] INFO:     127.0.0.1:55714 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:03:31 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 27, cache hit rate: 65.17%, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>The</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> first</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> man</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> on</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> the</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> moon</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> was</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> Neil</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> Armstrong</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> He</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> landed</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> on</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> the</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> moon</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> on</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> July</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:31 TP0] Decode batch. #running-req: 1, #token: 46, token usage: 0.00, gen throughput (token/s): 201.85, #queue-req: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>2</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>0</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>,</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>1</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>9</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>6</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>9</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>,</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> during</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> the</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> Apollo</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>1</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>1</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> mission</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'>.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><|im_end|><br></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@function\n",
    "def stream_qa(s, question: str):\n",
    "    s += user(question)\n",
    "    s += assistant(gen(\"answer\", max_tokens=128, stop=[\"assistant\"], temperature=0))\n",
    "\n",
    "\n",
    "state = stream_qa.run(question=\"Who was the first man on the moon?\", stream=True)\n",
    "for out in state.text_iter():\n",
    "    print_highlight(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[user|assistant|system]_[begin|end]` can be used to define more complex prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:31 TP0] Prefill batch. #new-seq: 1, #new-token: 54, #cached-token: 4, cache hit rate: 61.44%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:03:31 TP0] Decode batch. #running-req: 1, #token: 80, token usage: 0.00, gen throughput (token/s): 209.65, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:03:31 TP0] Decode batch. #running-req: 1, #token: 120, token usage: 0.00, gen throughput (token/s): 244.86, #queue-req: 0\n",
      "[2025-02-13 21:03:31] INFO:     127.0.0.1:55718 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> Ar fella a poco, then, landlubbin' men lists to me; we talk of treasure and of havin' a good time at sea, with only sixteen fighting men at a time, of course. Yo-ho-ho, and a bottle of rum before tumbling sleepless to bed! Let me tell you, aye!</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang import user_begin, user_end, assistant_begin, assistant_end\n",
    "\n",
    "\n",
    "@function\n",
    "def roles(s):\n",
    "    s += system(\n",
    "        \"You talk like a pirate and use frequently phrases like 'arrr' and 'yo-ho-ho'.\"\n",
    "    )\n",
    "    s += user_begin()\n",
    "    s += \"Hello, how do you like life as a pirate?\"\n",
    "    s += user_end()\n",
    "    s += assistant_begin()\n",
    "    s += \"There is much to tell about the life of a pirate.\" + gen(\n",
    "        \"story\", max_tokens=128\n",
    "    )\n",
    "    s += assistant_end()\n",
    "\n",
    "\n",
    "state = roles.run()\n",
    "print_highlight(state[\"story\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-modal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGLang supports a variety of [multi-modal models](https://docs.sglang.ai/backend/openai_api_vision.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 21:03:37 __init__.py:190] Automatically detected platform cuda.\n",
      "[2025-02-13 21:03:42] server_args=ServerArgs(model_path='lmms-lab/llama3-llava-next-8b', tokenizer_path='lmms-lab/llama3-llava-next-8b', tokenizer_mode='auto', load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='lmms-lab/llama3-llava-next-8b', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=30333, mem_fraction_static=0.8, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, stream_output=False, random_seed=883450856, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "INFO 02-13 21:03:45 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 02-13 21:03:45 __init__.py:190] Automatically detected platform cuda.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[2025-02-13 21:03:53 TP0] Overlap scheduler is disabled for multimodal models.\n",
      "[2025-02-13 21:03:53 TP0] Automatically reduce --mem-fraction-static to 0.760 because this is a multimodal model.\n",
      "[2025-02-13 21:03:53 TP0] Init torch distributed begin.\n",
      "[2025-02-13 21:03:53 TP0] Load weight begin. avail mem=78.84 GB\n",
      "[2025-02-13 21:03:57 TP0] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.09it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.04s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.27it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.16it/s]\n",
      "\n",
      "[2025-02-13 21:04:00 TP0] Load weight end. type=LlavaLlamaForCausalLM, dtype=torch.float16, avail mem=63.19 GB\n",
      "[2025-02-13 21:04:00 TP0] KV Cache is allocated. K size: 22.13 GB, V size: 22.13 GB.\n",
      "[2025-02-13 21:04:00 TP0] Memory pool end. avail mem=18.69 GB\n",
      "[2025-02-13 21:04:01 TP0] Capture cuda graph begin. This can take up to several minutes.\n",
      "100%|██████████| 23/23 [00:05<00:00,  3.90it/s]\n",
      "[2025-02-13 21:04:06 TP0] Capture cuda graph end. Time elapsed: 5.90 s\n",
      "[2025-02-13 21:04:08 TP0] max_total_num_tokens=362648, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4097, context_len=8192\n",
      "[2025-02-13 21:04:08] INFO:     Started server process [2689758]\n",
      "[2025-02-13 21:04:08] INFO:     Waiting for application startup.\n",
      "[2025-02-13 21:04:08] INFO:     Application startup complete.\n",
      "[2025-02-13 21:04:08] INFO:     Uvicorn running on http://0.0.0.0:30333 (Press CTRL+C to quit)\n",
      "[2025-02-13 21:04:09] INFO:     127.0.0.1:57460 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:04:09] INFO:     127.0.0.1:57476 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:04:09 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:04:10] INFO:     127.0.0.1:57492 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-13 21:04:10] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server_process = execute_shell_command(\n",
    "    \"python3 -m sglang.launch_server --model-path lmms-lab/llama3-llava-next-8b --mem-fraction-static 0.8 --port 30333 --host 0.0.0.0\"\n",
    ")\n",
    "wait_for_server(\"http://localhost:30333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:04:14] INFO:     127.0.0.1:57504 - \"GET /get_model_info HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "set_default_backend(RuntimeEndpoint(\"http://localhost:30333\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `image` to pass an image to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-13 21:04:14--  https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\n",
      "Resolving github.com (github.com)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/sgl-project/sglang/raw/refs/heads/main/test/lang/example_image.png [following]\n",
      "--2025-02-13 21:04:14--  https://github.com/sgl-project/sglang/raw/refs/heads/main/test/lang/example_image.png\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/sgl-project/sglang/refs/heads/main/test/lang/example_image.png [following]\n",
      "--2025-02-13 21:04:15--  https://raw.githubusercontent.com/sgl-project/sglang/refs/heads/main/test/lang/example_image.png\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 57365 (56K) [image/png]\n",
      "Saving to: ‘example_image.png’\n",
      "\n",
      "example_image.png   100%[===================>]  56.02K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-02-13 21:04:15 (1.47 MB/s) - ‘example_image.png’ saved [57365/57365]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O example_image.png https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-13 21:04:18 TP0] Prefill batch. #new-seq: 1, #new-token: 2158, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-13 21:04:19] INFO:     127.0.0.1:57510 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'> \"The white shirts are being put in the back of the truck, along with his wares.\"</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang import image\n",
    "\n",
    "\n",
    "@function\n",
    "def image_qa(s, image_file, question):\n",
    "    s += user(image(image_file) + question)\n",
    "    s += assistant(gen(\"answer\", max_tokens=128, stop=[\"assistant\"]))\n",
    "\n",
    "\n",
    "state = image_qa.run(\n",
    "    image_file=\"example_image.png\", question=\"Describe the image in one short sentence.\"\n",
    ")\n",
    "print_highlight(state[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm example_image.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more familar with SGLang we recommend to start studying the [benchmark scripts](https://github.com/sgl-project/sglang/tree/main/benchmark)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
