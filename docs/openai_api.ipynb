{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Compatible API\n",
    "\n",
    "SGLang provides an OpenAI compatible API for smooth transition from OpenAI services.\n",
    "\n",
    "- `chat/completions`\n",
    "- `completions`\n",
    "- `batch`\n",
    "- `embeddings`(refer to [embedding_model.ipynb](embedding_model.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completions\n",
    "\n",
    "### Usage\n",
    "\n",
    "Similar to [send_request.ipynb](send_request.ipynb), we can send a chat completion request to SGLang server with OpenAI API format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is ready. Proceeding with the next steps.\n"
     ]
    }
   ],
   "source": [
    "from sglang.utils import execute_shell_command, wait_for_server, terminate_process\n",
    "\n",
    "\n",
    "server_process = execute_shell_command(\"\"\"\n",
    "python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "--port 30000 --host 0.0.0.0 --log-level warning\n",
    "\"\"\")\n",
    "\n",
    "wait_for_server(\"http://localhost:30000\")\n",
    "print(\"Server is ready. Proceeding with the next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Always assign an api_key, even if not specified during server initialization.\n",
    "# Setting an API key during server initialization is strongly recommended.\n",
    "\n",
    "client = openai.Client(\n",
    "    base_url=\"http://127.0.0.1:30000/v1\", api_key=\"None\"\n",
    ")\n",
    "\n",
    "# Chat completion example\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "The chat completions API accepts the following parameters:\n",
    "\n",
    "- `messages`: List of messages in the conversation, each containing `role` and `content`\n",
    "- `model`: The model identifier to use for completion\n",
    "- `max_tokens`: Maximum number of tokens to generate in the response\n",
    "- `temperature`: Controls randomness (0-2). Lower values make output more focused and deterministic\n",
    "- `top_p`: Alternative to temperature. Controls diversity via nucleus sampling\n",
    "- `n`: Number of chat completion choices to generate\n",
    "- `stream`: If true, partial message deltas will be sent as they become available\n",
    "- `stop`: Sequences where the API will stop generating further tokens\n",
    "- `presence_penalty`: Penalizes new tokens based on their presence in the text so far (-2.0 to 2.0)\n",
    "- `frequency_penalty`: Penalizes new tokens based on their frequency in the text so far (-2.0 to 2.0)\n",
    "- `logit_bias`: Modify the likelihood of specified tokens appearing in the completion\n",
    "- `logprobs`: Include log probabilities of tokens in the response\n",
    "- `top_logprobs`: Number of most likely tokens to return probabilities for\n",
    "- `seed`: Random seed for deterministic results\n",
    "- `response_format`: Specify the format of the response (e.g., JSON)\n",
    "- `stream_options`: Additional options for streaming responses\n",
    "- `user`: A unique identifier representing your end-user\n",
    "\n",
    "Here is an example of a detailed chat completion request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a knowledgeable historian who provides concise responses.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me about ancient Rome\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Ancient Rome was a civilization centered in Italy.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What were their major achievements?\"}\n",
    "    ],\n",
    "    temperature=0.3,                    # Lower temperature for more focused responses\n",
    "    max_tokens=100,                     # Reasonable length for a concise response\n",
    "    top_p=0.95,                        # Slightly higher for better fluency\n",
    "    stop=[\"\\n\\n\"],                     # Simple stop sequence\n",
    "    presence_penalty=0.2,               # Mild penalty to avoid repetition\n",
    "    frequency_penalty=0.2,              # Mild penalty for more natural language\n",
    "    n=1,                               # Single response is usually more stable\n",
    "    seed=42,                           # Keep for reproducibility\n",
    "    stream=True                        # Keep streaming for real-time output\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completions\n",
    "\n",
    "### Usage\n",
    "\n",
    "Completions API is similar to Chat Completions API, but without the `messages` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    prompt=\"List 3 countries and their capitals.\",\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    "    n=1,\n",
    "    stop=None\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "The completions API accepts the following parameters:\n",
    "\n",
    "- `model`: The model identifier to use for completion\n",
    "- `prompt`: Input text to generate completions for. Can be a string, array of strings, or token arrays\n",
    "- `best_of`: Number of completions to generate server-side and return the best one\n",
    "- `echo`: If true, the prompt will be included in the response\n",
    "- `frequency_penalty`: Penalizes new tokens based on their frequency in the text so far (-2.0 to 2.0)\n",
    "- `logit_bias`: Modify the likelihood of specified tokens appearing in the completion\n",
    "- `logprobs`: Include log probabilities of tokens in the response\n",
    "- `max_tokens`: Maximum number of tokens to generate in the response (default: 16)\n",
    "- `n`: Number of completion choices to generate\n",
    "- `presence_penalty`: Penalizes new tokens based on their presence in the text so far (-2.0 to 2.0)\n",
    "- `seed`: Random seed for deterministic results\n",
    "- `stop`: Sequences where the API will stop generating further tokens\n",
    "- `stream`: If true, partial completion deltas will be sent as they become available\n",
    "- `stream_options`: Additional options for streaming responses\n",
    "- `suffix`: Text to append to the completion\n",
    "- `temperature`: Controls randomness (0-2). Lower values make output more focused and deterministic\n",
    "- `top_p`: Alternative to temperature. Controls diversity via nucleus sampling\n",
    "- `user`: A unique identifier representing your end-user\n",
    "\n",
    "Here is an example of a detailed completions request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    prompt=\"Write a short story about a space explorer.\",\n",
    "    temperature=0.7,                    # Moderate temperature for creative writing\n",
    "    max_tokens=150,                     # Longer response for a story\n",
    "    top_p=0.9,                         # Balanced diversity in word choice\n",
    "    stop=[\"\\n\\n\", \"THE END\"],          # Multiple stop sequences\n",
    "    presence_penalty=0.3,               # Encourage novel elements\n",
    "    frequency_penalty=0.3,              # Reduce repetitive phrases\n",
    "    n=1,                               # Generate one completion\n",
    "    seed=123,                          # For reproducible results\n",
    "    stream=True                        # Stream the response\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].text or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch\n",
    "\n",
    "We have implemented the batch API for chat completions and completions. You can upload your requests in JSONL format, create a batch job, and retrieve the results when the batch job is completed (which takes longer but costs less).\n",
    "\n",
    "The batch APIs are:\n",
    "\n",
    "- `batches`\n",
    "- `batches/{batch_id}/cancel`\n",
    "- `batches/{batch_id}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch job created with ID: batch_02b2636f-06a7-4982-b18e-7f6c954cc5af\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:30000/v1\",\n",
    "    api_key=\"None\"\n",
    ")\n",
    "\n",
    "requests = [\n",
    "    {\n",
    "        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a joke about programming\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is Python?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "with open('batch_requests.jsonl', 'w') as f:\n",
    "    for req in requests:\n",
    "        f.write(json.dumps(req) + '\\n')\n",
    "\n",
    "with open('batch_requests.jsonl', 'rb') as f:\n",
    "    file_response = client.files.create(\n",
    "        file=f,\n",
    "        purpose='batch'\n",
    "    )\n",
    "\n",
    "batch_response = client.batches.create(\n",
    "    input_file_id=file_response.id,\n",
    "    endpoint='/chat/completions',\n",
    "    completion_window='24h'\n",
    ")\n",
    "\n",
    "print(f\"Batch job created with ID: {batch_response.id}\")\n",
    "batch_status = client.batches.retrieve(batch_id=batch_response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a while to complete the batch job. You can use these two APIs to cancel the batch job or retrieve the batch job status.\n",
    "\n",
    "1. `batches/{batch_id}`: Retrieve the batch job status.\n",
    "2. `batches/{batch_id}/cancel`: Cancel the batch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: failed\n"
     ]
    }
   ],
   "source": [
    "batch_status = client.batches.retrieve(batch_id=batch_response.id)\n",
    "print(f\"Current status: {batch_status.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_status.status == \"running\":\n",
    "    cancelled_response = client.batches.cancel(batch_id=batch_response.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlphaMeemory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
