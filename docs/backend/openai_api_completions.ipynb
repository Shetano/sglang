{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# OpenAI APIs - Completions\\n\",\n",
    "    \"\\n\",\n",
    "    \"SGLang provides OpenAI-compatible APIs to enable a smooth transition from OpenAI services to self-hosted local models.\\n\",\n",
    "    \"A complete reference for the API is available in the [OpenAI API Reference](https://platform.openai.com/docs/api-reference).\\n\",\n",
    "    \"\\n\",\n",
    "    \"This tutorial covers the following popular APIs:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- `chat/completions`\\n\",\n",
    "    \"- `completions`\\n\",\n",
    "    \"- `batches`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Check out other tutorials to learn about [vision APIs](https://docs.sglang.ai/backend/openai_api_vision.html) for vision-language models and [embedding APIs](https://docs.sglang.ai/backend/openai_api_embeddings.html) for embedding models.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Launch A Server\\n\",\n",
    "    \"\\n\",\n",
    "    \"Launch the server in your terminal and wait for it to initialize.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sglang.test.test_utils import is_in_ci\\n\",\n",
    "    \"\\n\",\n",
    "    \"if is_in_ci():\\n\",\n",
    "    \"    from patch import launch_server_cmd\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    from sglang.utils import launch_server_cmd\\n\",\n",
    "    \"\\n\",\n",
    "    \"from sglang.utils import wait_for_server, print_highlight, terminate_process\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"server_process, port = launch_server_cmd(\\n\",\n",
    "    \"    \\\"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --host 0.0.0.0\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"wait_for_server(f\\\"http://localhost:{port}\\\")\\n\",\n",
    "    \"print(f\\\"Server started on http://localhost:{port}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Chat Completions\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Usage\\n\",\n",
    "    \"\\n\",\n",
    "    \"The server fully implements the OpenAI API.\\n\",\n",
    "    \"It will automatically apply the chat template specified in the Hugging Face tokenizer, if one is available.\\n\",\n",
    "    \"You can also specify a custom chat template with `--chat-template` when launching the server.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import openai\\n\",\n",
    "    \"\\n\",\n",
    "    \"client = openai.Client(base_url=f\\\"http://127.0.0.1:{port}/v1\\\", api_key=\\\"None\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"response = client.chat.completions.create(\\n\",\n",
    "    \"    model=\\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\",\\n\",\n",
    "    \"    messages=[\\n\",\n",
    "    \"        {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"List 3 countries and their capitals.\\\"},\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    temperature=0,\\n\",\n",
    "    \"    max_completion_tokens=64,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print_highlight(f\\\"Response: {response}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Parameters\\n\",\n",
    "    \"\\n\",\n",
    "    \"The chat completions API accepts OpenAI Chat Completions API's parameters. Refer to [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat/create) for more details.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here is an example of a detailed chat completion request:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"response = client.chat.completions.create(\\n\",\n",
    "    \"    model=\\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\",\\n\",\n",
    "    \"    messages=[\\n\",\n",
    "    \"        {\\n\",\n",
    "    \"            \\\"role\\\": \\\"system\\\",\\n\",\n",
    "    \"            \\\"content\\\": \\\"You are a knowledgeable historian who provides concise responses.\\\",\\n\",\n",
    "    \"        },\\n\",\n",
    "    \"        {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Tell me about ancient Rome\\\"},\\n\",\n",
    "    \"        {\\n\",\n",
    "    \"            \\\"role\\\": \\\"assistant\\\",\\n\",\n",
    "    \"            \\\"content\\\": \\\"Ancient Rome was a civilization centered in Italy.\\\",\\n\",\n",
    "    \"        },\\n\",\n",
    "    \"        {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What were their major achievements?\\\"},\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    temperature=0.3,  # Lower temperature for more focused responses\\n\",\n",
    "    \"    max_completion_tokens=128,  # Reasonable length for a concise response\\n\",\n",
    "    \"    top_p=0.95,  # Slightly higher for better fluency\\n\",\n",
    "    \"    presence_penalty=0.2,  # Mild penalty to avoid repetition\\n\",\n",
    "    \"    frequency_penalty=0.2,  # Mild penalty for more natural language\\n\",\n",
    "    \"    n=1,  # Single response is usually more stable\\n\",\n",
    "    \"    seed=42,  # Keep for reproducibility\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print_highlight(response.choices[0].message.content)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Streaming mode is also supported.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"stream = client.chat.completions.create(\\n\",\n",
    "    \"    model=\\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\",\\n\",\n",
    "    \"    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Say this is a test\\\"}],\\n\",\n",
    "    \"    stream=True,\\n\",\n",
    "    \")\\n\",\n",
    "    \"for chunk in stream:\\n\",\n",
    "    \"    if chunk.choices[0].delta.content is not None:\\n\",\n",
    "    \"        print(chunk.choices[0].delta.content, end=\\\"\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Completions\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Usage\\n\",\n",
    "    \"Completions API is similar to Chat Completions API, but without the `messages` parameter or chat templates.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"response = client.completions.create(\\n\",\n",
    "    \"    model=\\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\",\\n\",\n",
    "    \"    prompt=\\\"List 3 countries and their capitals.\\\",\\n\",\n",
    "    \"    temperature=0,\\n\",\n",
    "    \"    max_tokens=64,\\n\",\n",
    "    \"    n=1,\\n\",\n",
    "    \"    stop=None,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print_highlight(f\\\"Response: {response}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Parameters\\n\",\n",
    "    \"\\n\",\n",
    "    \"The completions API accepts OpenAI Completions API's parameters.  Refer to [OpenAI Completions API](https://platform.openai.com/docs/api-reference/completions/create) for more details.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here is an example of a detailed completions request:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"response = client.completions.create(\\n\",\n",
    "    \"    model=\\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\",\\n\",\n",
    "    \"    prompt=\\\"Write a short story about a space explorer.\\\",\\n\",\n",
    "    \"    temperature=0.7,  # Moderate temperature for creative writing\\n\",\n",
    "    \"    max_tokens=150,  # Longer response for a story\\n\",\n",
    "    \"    top_p=0.9,  # Balanced diversity in word choice\\n\",\n",
    "    \"    stop=[\\\"\\\\n\\\\n\\\", \\\"THE END\\\"],  # Multiple stop sequences\\n\",\n",
    "    \"    presence_penalty=0.3,  # Encourage novel elements\\n\",\n",
    "    \"    frequency_penalty=0.3,  # Reduce repetitive phrases\\n\",\n",
    "    \"    n=1,  # Generate one completion\\n\",\n",
    "    \"    seed=123,  # For reproducible results\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print_highlight(f\\\"Response: {response}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Structured Outputs (JSON, Regex, EBNF)\\n\",\n",
    "    \"\\n\",\n",
    "    \"For OpenAI compatible structed outputs API, refer to [Structured Outputs](https://docs.sglang.ai/backend/structured_outputs.html#OpenAI-Compatible-API) for more details.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Batches\\n\",\n",
    "    \"\\n\",\n",
    "    \"Batches API for chat completions and completions are also supported. You can upload your requests in `jsonl` files, create a batch job, and retrieve the results when the batch job is completed (which takes longer but costs less).\\n\",\n",
    "    \"\\n\",\n",
    "    \"The batches APIs are:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- `batches`\\n\",\n",
    "    \"- `batches/{batch_id}/cancel`\\n\",\n",
    "    \"- `batches/{batch_id}`\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here is an example of a batch job for chat completions, completions are similar.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import json\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"from openai import OpenAI\\n\",\n",
    "    \"\\n\",\n",
    "    \"client = OpenAI(base_url=f\\\"http://127.0.0.1:{port}/v1\\\", api_key=\\\"None\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"requests = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"custom_id\\\": \\\"request-1\\\",\\n\",\n",
    "    \"        \\\"method\\\": \\\"POST\\\",\\n\",\n",
    "    \"        \\\"url\\\": \\\"/chat/completions\\\",\\n\",\n",
    "    \"        \\\"body\\\": {\\n\",\n",
    "    \"            \\\"model\\\": \\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\",\\n\",\n",
    "    \"            \\\"messages\\\": [\\n\",\n",
    "    \"                {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Tell me a joke about programming\\\"}\\n\",\n",
    "    \"            ],\\n\",\n",
    "    \"            \\\"max_completion_tokens\\\": 50,\\n\",\n",
    "    \"        },\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        \\\"custom_id\\\": \\\"request-2\\\",\\n\",\n",
    "    \"        \\\"method\\\": \\\"POST\\\",\\n\",\n",
    "    \"        \\\"url\\\": \\\"/chat/completions\\\",\\n\",\n",
    "    \"        \\\"body\\\": {\\n\",\n",
    "    \"            \\\"model\\\": \\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\",\\n\",\n",
    "    \"            \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What is Python?\\\"}],\\n\",\n",
    "    \"            \\\"max_completion_tokens\\\": 50,\\n\",\n",
    "    \"        },\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"input_file_path = \\\"batch_requests.jsonl\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(input_file_path, \\\"w\\\") as f:\\n\",\n",
    "    \"    for req in requests:\\n\",\n",
    "    \"        f.write(json.dumps(req) + \\\"\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(input_file_path, \\\"rb\\\") as f:\\n\",\n",
    "    \"    file_response = client.files.create(file=f, purpose=\\\"batch\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"batch_response = client.batches.create(\\n\",\n",
    "    \"    input_file_id=file_response.id,\\n\",\n",
    "    \"    endpoint=\\\"/v1/chat/completions\\\",\\n\",\n",
    "    \"    completion_window=\\\"24h\\\",\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print_highlight(f\\\"Batch job created with ID: {batch_response.id}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"while batch_response.status not in [\\\"completed\\\", \\\"failed\\\", \\\"cancelled\\\"]:\\n\",\n",
    "    \"    time.sleep(3)\\n\",\n",
    "    \"    print(f\\\"Batch job status: {batch_response.status}...trying again in 3 seconds...\\\")\\n\",\n",
    "    \"    batch_response = client.batches.retrieve(batch_response.id)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if batch_response.status == \\\"completed\\\":\\n\",\n",
    "    \"    print(\\\"Batch job completed successfully!\\\")\\n\",\n",
    "    \"    print(f\\\"Request counts: {batch_response.request_counts}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    result_file_id = batch_response.output_file_id\\n\",\n",
    "    \"    file_response = client.files.content(result_file_id)\\n\",\n",
    "    \"    result_content = file_response.read().decode(\\\"utf-8\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    results = [\\n\",\n",
    "    \"        json.loads(line) for line in result_content.split(\\\"\\\\n\\\") if line.strip() != \\\"\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for result in results:\\n\",\n",
    "    \"        print_highlight(f\\\"Request {result['custom_id']}:\\\")\\n\",\n",
    "    \"        print_highlight(f\\\"Response: {result['response']}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print_highlight(\\\"Cleaning up files...\\\")\\n\",\n",
    "    \"    # Only delete the result file ID since file_response is just content\\n\",\n",
    "    \"    client.files.delete(result_file_id)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print_highlight(f\\\"Batch job failed with status: {batch_response.status}\\\")\\n\",\n",
    "    \"    if hasattr(batch_response, \\\"errors\\\"):\\n\",\n",
    "    \"        print_highlight(f\\\"Errors: {batch_response.errors}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"It takes a while to complete the batch job. You can use these two APIs to retrieve the batch job status or cancel the batch job.\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. `batches/{batch_id}`: Retrieve the batch job status.\\n\",\n",
    "    \"2. `batches/{batch_id}/cancel`: Cancel the batch job.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here is an example to check the batch job status.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import json\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"from openai import OpenAI\\n\",\n",
    "    \"\\n\",\n",
    "    \"client = OpenAI(base_url=f\\\"http://127.0.0.1:{port}/v1\\\", api_key=\\\"None\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"requests = []\\n\",\n",
    "    \"for i in range(20):\\n\",\n",
    "    \"    requests.append(\\n\",\n",
    "    \"        {\\n\",\n",
    "    \"            \\\"custom_id\\\": f\\\"request-{i}\\\",\\n\",\n",
    "    \"            \\\"method\\\": \\\"POST\\\",\\n\",\n",
    "    \"            \\\"url\\\": \\\"/chat/completions\\\",\\n\",\n",
    "    \"            \\\"body\\\": {\\n\",\n",
    "    \"                \\\"model\\\": \\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\",\\n\",\n",
    "    \"                \\\"messages\\\": [\\n\",\n",
    "    \"                    {\\n\",\n",
    "    \"                        \\\"role\\\": \\\"system\\\",\\n\",\n",
    "    \"                        \\\"content\\\": f\\\"{i}: You are a helpful AI assistant\\\",\\n\",\n",
    "    \"                    },\\n\",\n",
    "    \"                    {\\n\",\n",
    "    \"                        \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"                        \\\"content\\\": \\\"Write a detailed story about topic. Make it very long.\\\",\\n\",\n",
    "    \"                    },\\n\",\n",
    "    \"                ],\\n\",\n",
    "    \"                \\\"max_completion_tokens\\\": 64,\\n\",\n",
    "    \"            },\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"input_file_path = \\\"batch_requests.jsonl\\\"\\n\",\n",
    "    \"with open(input_file_path, \\\"w\\\") as f:\\n\",\n",
    "    \"    for req in requests:\\n\",\n",
    "    \"        f.write(json.dumps(req) + \\\"\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(input_file_path, \\\"rb\\\") as f:\\n\",\n",
    "    \"    uploaded_file = client.files.create(file=f, purpose=\\\"batch\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"batch_job = client.batches.create(\\n\",\n",
    "    \"    input_file_id=uploaded_file.id,\\n\",\n",
    "    \"    endpoint=\\\"/v1/chat/completions\\\",\\n\",\n",
    "    \"    completion_window=\\\"24h\\\",\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print_highlight(f\\\"Created batch job with ID: {batch_job.id}\\\")\\n\",\n",
    "    \"print_highlight(f\\\"Initial status: {batch_job.status}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"time.sleep(10)\\n\",\n",
    "    \"\\n\",\n",
    "    \"max_checks = 5\\n\",\n",
    "    \"for i in range(max_checks):\\n\",\n",
    "    \"    batch_details = client.batches.retrieve(batch_id=batch_job.id)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print_highlight(\\n\",\n",
    "    \"        f\\\"Batch job details (check {i + 1} / {max_checks}) // ID: {batch_details.id} // Status: {batch_details.status} // Created at: {batch_details.created_at} // Input file ID: {batch_details.input_file_id} // Output file ID: {batch_details.output_file_id}\\\"\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    if batch_details.status == \\\"completed\\\":\\n\",\n",
    "    \"        request_counts = batch_details.request_counts\\n\",\n",
    "    \"        print_highlight(\\n\",\n",
    "    \"            f\\\"<strong>Request counts: Total: {batch_details.request_counts.total} // Completed: {batch_details.request_counts.completed} // Failed: {batch_details.request_counts.failed}</strong>\\\"\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    time.sleep(3)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Here is an example to cancel a batch job.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import json\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"from openai import OpenAI\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"client = OpenAI(base_url=f\\\"http://127.0.0.1:{port}/v1\\\", api_key=\\\"None\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"requests = []\\n\",\n",
    "    \"for i in range(5000):\\n\",\n",
    "    \"    requests.append(\\n\",\n",
    "    \"        {\\n\",\n",
    "    \"            \\\"custom_id\\\": f\\\"request-{i}\\\",\\n\",\n",
    "    \"            \\\"method\\\": \\\"POST\\\",\\n\",\n",
    "    \"            \\\"url\\\": \\\"/chat/completions\\\",\\n\",\n",
    "    \"            \\\"body\\\": {\\n\",\n",
    "    \"                \\\"model\\\": \\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\",\\n\",\n",
    "    \"                \\\"messages\\\": [\\n\",\n",
    "    \"                    {\\n\",\n",
    "    \"                        \\\"role\\\": \\\"system\\\",\\n\",\n",
    "    \"                        \\\"content\\\": f\\\"{i}: You are a helpful AI assistant\\\",\\n\",\n",
    "    \"                    },\\n\",\n",
    "    \"                    {\\n\",\n",
    "    \"                        \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"                        \\\"content\\\": \\\"Write a detailed story about topic. Make it very long.\\\",\\n\",\n",
    "    \"                    },\\n\",\n",
    "    \"                ],\\n\",\n",
    "    \"                \\\"max_completion_tokens\\\": 128,\\n\",\n",
    "    \"            },\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"input_file_path = \\\"batch_requests.jsonl\\\"\\n\",\n",
    "    \"with open(input_file_path, \\\"w\\\") as f:\\n\",\n",
    "    \"    for req in requests:\\n\",\n",
    "    \"        f.write(json.dumps(req) + \\\"\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open(input_file_path, \\\"rb\\\") as f:\\n\",\n",
    "    \"    uploaded_file = client.files.create(file=f, purpose=\\\"batch\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"batch_job = client.batches.create(\\n\",\n",
    "    \"    input_file_id=uploaded_file.id,\\n\",\n",
    "    \"    endpoint=\\\"/v1/chat/completions\\\",\\n\",\n",
    "    \"    completion_window=\\\"24h\\\",\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print_highlight(f\\\"Created batch job with ID: {batch_job.id}\\\")\\n\",\n",
    "    \"print_highlight(f\\\"Initial status: {batch_job.status}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"time.sleep(10)\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    cancelled_job = client.batches.cancel(batch_id=batch_job.id)\\n\",\n",
    "    \"    print_highlight(f\\\"Cancellation initiated. Status: {cancelled_job.status}\\\")\\n\",\n",
    "    \"    assert cancelled_job.status == \\\"cancelling\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Monitor the cancellation process\\n\",\n",
    "    \"    while cancelled_job.status not in [\\\"failed\\\", \\\"cancelled\\\"]:\\n\",\n",
    "    \"        time.sleep(3)\\n\",\n",
    "    \"        cancelled_job = client.batches.retrieve(batch_job.id)\\n\",\n",
    "    \"        print_highlight(f\\\"Current status: {cancelled_job.status}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Verify final status\\n\",\n",
    "    \"    assert cancelled_job.status == \\\"cancelled\\\"\\n\",\n",
    "    \"    print_highlight(\\\"Batch job successfully cancelled\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print_highlight(f\\\"Error during cancellation: {e}\\\")\\n\",\n",
    "    \"    raise e\\n\",\n",
    "    \"\\n\",\n",
    "    \"finally:\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        del_response = client.files.delete(uploaded_file.id)\\n\",\n",
    "    \"        if del_response.deleted:\\n\",\n",
    "    \"            print_highlight(\\\"Successfully cleaned up input file\\\")\\n\",\n",
    "    \"        if os.path.exists(input_file_path):\\n\",\n",
    "    \"            os.remove(input_file_path)\\n\",\n",
    "    \"            print_highlight(\\\"Successfully deleted local batch_requests.jsonl file\\\")\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print_highlight(f\\\"Error cleaning up: {e}\\\")\\n\",\n",
    "    \"        raise e\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"terminate_process(server_process)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
