{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Completion Parser\n",
    "\n",
    "SGLang support code fim completion in completion interface for code model. Code fim completion is feature that predicts and inserts missing code snippets within the context of existing code, boosting productivity and reducing errors. \n",
    "\n",
    "## Supported Models\n",
    "\n",
    "Currently, SGLang supports the following code models:\n",
    "- DeepSeek Coder\n",
    "- Qwen Coder\n",
    "- Star Coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "### Launching the Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the `--completion-template` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from sglang.test.test_utils import is_in_ci\n",
    "\n",
    "if is_in_ci():\n",
    "    from patch import launch_server_cmd\n",
    "else:\n",
    "    from sglang.utils import launch_server_cmd\n",
    "\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python -m sglang.launch_server --model-path deepseek-ai/deepseek-coder-1.3b-base --completion-template deepseek_coder --port 30020 --host 0.0.0.0\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Compatible API\n",
    "\n",
    "completion template currently have three options: deepseek_coder, qwen_coder, star_ coder\n",
    "\n",
    "- `completion-template`: the completion template you want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI-like client\n",
    "client = OpenAI(api_key=\"None\", base_url=f\"http://0.0.0.0:{port}/v1\")\n",
    "model_name = client.models.list().data[0].id\n",
    "\n",
    "prompt = \"function sum(a: number, b: number): number {\"\n",
    "suffix = \"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request\n",
    "it's the same for Stream request and Non-Stream request, so we take non-stream as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_non_stream = client.completions.create(\n",
    "    model=model_name,\n",
    "    prompt=prompt,\n",
    "    suffix=suffix,\n",
    "    temperature=0.3,\n",
    "    top_p=0.95,\n",
    "    stream=False,  # Non-streaming\n",
    ")\n",
    "\n",
    "print_highlight(\"==== [FIM] ====\")\n",
    "print_highlight(response_non_stream.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Engine API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sglang as sgl\n",
    "from sglang.srt.code_completion_parser import generate_completion_prompt\n",
    "\n",
    "llm = sgl.Engine(model_path=\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
    "input = generate_completion_prompt(prompt, suffix, \"deepseek_coder\")\n",
    "\n",
    "\n",
    "sampling_params = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"skip_special_tokens\": False,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "result = llm.generate(prompt=input, sampling_params=sampling_params)\n",
    "\n",
    "fim_code = result[\"text\"]  # Assume there is only one prompt\n",
    "\n",
    "print_highlight(fim_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
