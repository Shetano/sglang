{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Completion Parser\n",
    "\n",
    "SGLang support code fim completion in completion interface for code model. Codefim completion is feature that predicts and inserts missing code snippets within the context of existing code, boosting productivity and reducing errors. \n",
    "\n",
    "## Supported Models\n",
    "\n",
    "Currently, SGLang supports the following reasoning models:\n",
    "- DeepSeek Coder\n",
    "- Qwen Coder\n",
    "- Star Coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "### Launching the Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the `--completion-template` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "from sglang.test.test_utils import is_in_ci\n",
    "\n",
    "if is_in_ci():\n",
    "    from patch import launch_server_cmd\n",
    "else:\n",
    "    from sglang.utils import launch_server_cmd\n",
    "\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python -m sglang.launch_server --model-path deepseek-ai/deepseek-coder-1.3b-base --completion-template deepseek_coder --port 30020 --host 0.0.0.0\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Compatible API\n",
    "\n",
    "completion template currently have three options: deepseek_coder, qwen_coder\n",
    "\n",
    "- `completion-template`: the completion template you want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI-like client\n",
    "client = OpenAI(api_key=\"None\", base_url=f\"http://0.0.0.0:{port}/v1\")\n",
    "model_name = client.models.list().data[0].id\n",
    "\n",
    "prompt = \"function sum(a: number, b: number): number {\"\n",
    "suffix = \"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request\n",
    "it's the same for Stream request and Non-Stream request, so we take non-stream as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_non_stream = client.completions.create(\n",
    "    model=model_name,\n",
    "    prompt=prompt,\n",
    "    suffix=suffix,\n",
    "    temperature=0.3,\n",
    "    top_p=0.95,\n",
    "    stream=False,  # Non-streaming\n",
    ")\n",
    "\n",
    "print_highlight(\"==== [FIM] ====\")\n",
    "print_highlight(response_non_stream.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Engine API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woodx/anaconda3/envs/sgl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-08 21:47:34 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 21:47:34,939\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-08 21:47:41 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 03-08 21:47:41 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "/home/woodx/workspace/sglang/python/sglang/srt/model_loader/weight_utils.py:458: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.31s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.31s/it]\n",
      "\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  return a + b;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sglang as sgl\n",
    "from sglang.srt.code_completion_parser import generate_completion_prompt\n",
    "from sglang.utils import print_highlight\n",
    "\n",
    "\n",
    "llm = sgl.Engine(model_path=\"deepseek-ai/deepseek-coder-1.3b-base\")\n",
    "input = generate_completion_prompt(prompt, suffix, \"deepseek_coder\")\n",
    "\n",
    "\n",
    "sampling_params = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"skip_special_tokens\": False,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "result = llm.generate(prompt=input, sampling_params=sampling_params)\n",
    "\n",
    "fim_code = result[\"text\"]  # Assume there is only one prompt\n",
    "\n",
    "print_highlight(fim_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
