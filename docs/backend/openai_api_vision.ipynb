{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# OpenAI APIs - Vision\\n\",\n",
    "    \"\\n\",\n",
    "    \"SGLang provides OpenAI-compatible APIs to enable a smooth transition from OpenAI services to self-hosted local models.\\n\",\n",
    "    \"A complete reference for the API is available in the [OpenAI API Reference](https://platform.openai.com/docs/guides/vision).\\n\",\n",
    "    \"This tutorial covers the vision APIs for vision language models.\\n\",\n",
    "    \"\\n\",\n",
    "    \"SGLang supports vision language models such as Llama 3.2, LLaVA-OneVision, and QWen-VL2\\n\",\n",
    "    \"- [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)\\n\",\n",
    "    \"- [lmms-lab/llava-onevision-qwen2-72b-ov-chat](https://huggingface.co/lmms-lab/llava-onevision-qwen2-72b-ov-chat)\\n\",\n",
    "    \"- [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)\\n\",\n",
    "    \"\\n\",\n",
    "    \"As an alternative to the OpenAI API, you can also use the [SGLang offline engine](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/offline_batch_inference_vlm.py).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Launch A Server\\n\",\n",
    "    \"\\n\",\n",
    "    \"Launch the server in your terminal and wait for it to initialize.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Remember to add** `--chat-template llama_3_vision` **to specify the vision chat template, otherwise the server only supports text, and performance degradation may occur.**\\n\",\n",
    "    \"\\n\",\n",
    "    \"We need to specify `--chat-template` for vision language models because the chat template provided in Hugging Face tokenizer only supports text.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sglang.test.test_utils import is_in_ci\\n\",\n",
    "    \"\\n\",\n",
    "    \"if is_in_ci():\\n\",\n",
    "    \"    from patch import launch_server_cmd\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    from sglang.utils import launch_server_cmd\\n\",\n",
    "    \"\\n\",\n",
    "    \"from sglang.utils import wait_for_server, print_highlight, terminate_process\\n\",\n",
    "    \"\\n\",\n",
    "    \"vision_process, port = launch_server_cmd(\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-11B-Vision-Instruct \\\\\\n\",\n",
    "    \"    --chat-template=llama_3_vision\\n\",\n",
    "    \"\\\"\\\"\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"wait_for_server(f\\\"http://localhost:{port}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Using cURL\\n\",\n",
    "    \"\\n\",\n",
    "    \"Once the server is up, you can send test requests using curl or requests.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import subprocess\\n\",\n",
    "    \"\\n\",\n",
    "    \"curl_command = f\\\"\\\"\\\"\\n\",\n",
    "    \"curl -s http://localhost:{port}/v1/chat/completions \\\\\\\\\\n\",\n",
    "    \"  -d '{{\\n\",\n",
    "    \"    \\\"model\\\": \\\"meta-llama/Llama-3.2-11B-Vision-Instruct\\\",\\n\",\n",
    "    \"    \\\"messages\\\": [\\n\",\n",
    "    \"      {{\\n\",\n",
    "    \"        \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"        \\\"content\\\": [\\n\",\n",
    "    \"          {{\\n\",\n",
    "    \"            \\\"type\\\": \\\"text\\\",\\n\",\n",
    "    \"            \\\"text\\\": \\\"What’s in this image?\\\"\\n\",\n",
    "    \"          }},\\n\",\n",
    "    \"          {{\\n\",\n",
    "    \"            \\\"type\\\": \\\"image_url\\\",\\n\",\n",
    "    \"            \\\"image_url\\\": {{\\n\",\n",
    "    \"              \\\"url\\\": \\\"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\\\"\\n\",\n",
    "    \"            }}\\n\",\n",
    "    \"          }}\\n\",\n",
    "    \"        ]\\n\",\n",
    "    \"      }}\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    \\\"max_completion_tokens\\\": 300\\n\",\n",
    "    \"  }}'\\n\",\n",
    "    \"\\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"response = subprocess.check_output(curl_command, shell=True).decode()\\n\",\n",
    "    \"print_highlight(response)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"response = subprocess.check_output(curl_command, shell=True).decode()\\n\",\n",
    "    \"print_highlight(response)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Using Python Requests\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import requests\\n\",\n",
    "    \"\\n\",\n",
    "    \"url = f\\\"http://localhost:{port}/v1/chat/completions\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"data = {\\n\",\n",
    "    \"    \\\"model\\\": \\\"meta-llama/Llama-3.2-11B-Vision-Instruct\\\",\\n\",\n",
    "    \"    \\\"messages\\\": [\\n\",\n",
    "    \"        {\\n\",\n",
    "    \"            \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"            \\\"content\\\": [\\n\",\n",
    "    \"                {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"What’s in this image?\\\"},\\n\",\n",
    "    \"                {\\n\",\n",
    "    \"                    \\\"type\\\": \\\"image_url\\\",\\n\",\n",
    "    \"                    \\\"image_url\\\": {\\n\",\n",
    "    \"                        \\\"url\\\": \\\"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\\\"\\n\",\n",
    "    \"                    },\\n\",\n",
    "    \"                },\\n\",\n",
    "    \"            ],\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    \\\"max_completion_tokens\\\": 300,\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"response = requests.post(url, json=data)\\n\",\n",
    "    \"print_highlight(response.text)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Using OpenAI Python Client\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from openai import OpenAI\\n\",\n",
    "    \"\\n\",\n",
    "    \"client = OpenAI(base_url=f\\\"http://localhost:{port}/v1\\\", api_key=\\\"None\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"response = client.chat.completions.create(\\n\",\n",
    "    \"    model=\\\"meta-llama/Llama-3.2-11B-Vision-Instruct\\\",\\n\",\n",
    "    \"    messages=[\\n\",\n",
    "    \"        {\\n\",\n",
    "    \"            \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"            \\\"content\\\": [\\n\",\n",
    "    \"                {\\n\",\n",
    "    \"                    \\\"type\\\": \\\"text\\\",\\n\",\n",
    "    \"                    \\\"text\\\": \\\"What is in this image?\\\",\\n\",\n",
    "    \"                },\\n\",\n",
    "    \"                {\\n\",\n",
    "    \"                    \\\"type\\\": \\\"image_url\\\",\\n\",\n",
    "    \"                    \\\"image_url\\\": {\\n\",\n",
    "    \"                        \\\"url\\\": \\\"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\\\"\\n\",\n",
    "    \"                    },\\n\",\n",
    "    \"                },\\n\",\n",
    "    \"            ],\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    max_completion_tokens=300,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print_highlight(response.choices[0].message.content)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Multiple-Image Inputs\\n\",\n",
    "    \"\\n\",\n",
    "    \"The server also supports multiple images and interleaved text and images if the model supports it.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from openai import OpenAI\\n\",\n",
    "    \"\\n\",\n",
    "    \"client = OpenAI(base_url=f\\\"http://localhost:{port}/v1\\\", api_key=\\\"None\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"response = client.chat.completions.create(\\n\",\n",
    "    \"    model=\\\"meta-llama/Llama-3.2-11B-Vision-Instruct\\\",\\n\",\n",
    "    \"    messages=[\\n\",\n",
    "    \"        {\\n\",\n",
    "    \"            \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"            \\\"content\\\": [\\n\",\n",
    "    \"                {\\n\",\n",
    "    \"                    \\\"type\\\": \\\"image_url\\\",\\n\",\n",
    "    \"                    \\\"image_url\\\": {\\n\",\n",
    "    \"                        \\\"url\\\": \\\"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\\\",\\n\",\n",
    "    \"                    },\\n\",\n",
    "    \"                },\\n\",\n",
    "    \"                {\\n\",\n",
    "    \"                    \\\"type\\\": \\\"image_url\\\",\\n\",\n",
    "    \"                    \\\"image_url\\\": {\\n\",\n",
    "    \"                        \\\"url\\\": \\\"https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png\\\",\\n\",\n",
    "    \"                    },\\n\",\n",
    "    \"                },\\n\",\n",
    "    \"                {\\n\",\n",
    "    \"                    \\\"type\\\": \\\"text\\\",\\n\",\n",
    "    \"                    \\\"text\\\": \\\"I have two very different images. They are not related at all. \\\"\\n\",\n",
    "    \"                    \\\"Please describe the first image in one sentence, and then describe the second image in another sentence.\\\",\\n\",\n",
    "    \"                },\\n\",\n",
    "    \"            ],\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    temperature=0,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print_highlight(response.choices[0].message.content)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"terminate_process(vision_process)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Chat Template\\n\",\n",
    "    \"\\n\",\n",
    "    \"As mentioned before, if you do not specify a vision model's `--chat-template`, the server uses Hugging Face's default template, which only supports text.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We list popular vision models with their chat templates:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- [meta-llama/Llama-3.2-Vision](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct) uses `llama_3_vision`.\\n\",\n",
    "    \"- [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) uses `qwen2-vl`.\\n\",\n",
    "    \"- [LlaVA-OneVision](https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov) uses `chatml-llava`.\\n\",\n",
    "    \"- [LLaVA-NeXT](https://huggingface.co/collections/lmms-lab/llava-next-6623288e2d61edba3ddbf5ff) uses `chatml-llava`.\\n\",\n",
    "    \"- [Llama3-LLaVA-NeXT](https://huggingface.co/lmms-lab/llama3-llava-next-8b) uses `llava_llama_3`.\\n\",\n",
    "    \"- [LLaVA-v1.5 / 1.6](https://huggingface.co/liuhaotian/llava-v1.6-34b) uses `vicuna_v1.1`.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
