{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI APIs - Embedding\n",
    "\n",
    "SGLang provides OpenAI-compatible APIs to enable a smooth transition from OpenAI services to self-hosted local models.\n",
    "A complete reference for the API is available in the [OpenAI API Reference](https://platform.openai.com/docs/guides/embeddings).\n",
    "\n",
    "This tutorial covers the embedding APIs for embedding models, such as  \n",
    "- [intfloat/e5-mistral-7b-instruct](https://huggingface.co/intfloat/e5-mistral-7b-instruct)  \n",
    "- [Alibaba-NLP/gte-Qwen2-7B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server\n",
    "\n",
    "Launch the server in your terminal and wait for it to initialize. Remember to add `--is-embedding` to the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:30:14.844955Z",
     "iopub.status.busy": "2025-02-19T22:30:14.844579Z",
     "iopub.status.idle": "2025-02-19T22:30:52.358569Z",
     "shell.execute_reply": "2025-02-19T22:30:52.357909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/chayenne/.python/sglang/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "INFO 02-19 22:30:22 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-19 22:30:23,901 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:24] server_args=ServerArgs(model_path='Alibaba-NLP/gte-Qwen2-7B-instruct', tokenizer_path='Alibaba-NLP/gte-Qwen2-7B-instruct', tokenizer_mode='auto', load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='Alibaba-NLP/gte-Qwen2-7B-instruct', chat_template=None, is_embedding=True, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=30062, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, stream_output=False, random_seed=239964993, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, return_hidden_states=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False)\n",
      "[2025-02-19 22:30:24] Downcasting torch.float32 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/chayenne/.python/sglang/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "/data/chayenne/.python/sglang/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "INFO 02-19 22:30:28 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 02-19 22:30:28 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-19 22:30:30,418 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
      "2025-02-19 22:30:30,430 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:31 TP0] Downcasting torch.float32 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:31 TP0] Overlap scheduler is disabled for embedding models.\n",
      "[2025-02-19 22:30:31 TP0] Downcasting torch.float32 to torch.float16.\n",
      "[2025-02-19 22:30:31 TP0] Init torch distributed begin.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:32 TP0] Load weight begin. avail mem=75.98 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:32 TP0] Using model weights format ['*.safetensors']\n",
      "\r",
      "Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:05,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:02<00:06,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:04<00:06,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:06<00:05,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:08<00:03,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:10<00:02,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:12<00:00,  2.00s/it]\n",
      "\r",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:12<00:00,  1.85s/it]\n",
      "\n",
      "[2025-02-19 22:30:45 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.float16, avail mem=20.85 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:46 TP0] KV Cache is allocated. K size: 5.87 GB, V size: 5.87 GB.\n",
      "[2025-02-19 22:30:46 TP0] Memory pool end. avail mem=8.04 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:46 TP0] max_total_num_tokens=219696, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=131072\n",
      "[2025-02-19 22:30:46] INFO:     Started server process [1903080]\n",
      "[2025-02-19 22:30:46] INFO:     Waiting for application startup.\n",
      "[2025-02-19 22:30:46] INFO:     Application startup complete.\n",
      "[2025-02-19 22:30:46] INFO:     Uvicorn running on http://0.0.0.0:30062 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:47] INFO:     127.0.0.1:55888 - \"GET /v1/models HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:47] INFO:     127.0.0.1:55904 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-02-19 22:30:47 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-19 22:30:48,286 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "2025-02-19 22:30:48,309 - INFO - flashinfer.jit: Finished loading JIT ops: batch_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_0_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "[2025-02-19 22:30:48] INFO:     127.0.0.1:55908 - \"POST /encode HTTP/1.1\" 200 OK\n",
      "[2025-02-19 22:30:48] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                    NOTE: Typically, the server runs in a separate terminal.\n",
      "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
      "                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.\n",
      "                    We are running those notebooks in a CI parallel environment, so the throughput is not representative of the actual performance.\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "from sglang.test.test_utils import is_in_ci\n",
    "\n",
    "if is_in_ci():\n",
    "    from patch import launch_server_cmd\n",
    "else:\n",
    "    from sglang.utils import launch_server_cmd\n",
    "\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "embedding_process, port = launch_server_cmd(\n",
    "    \"\"\"\n",
    "python -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-7B-instruct \\\n",
    "    --host 0.0.0.0 --is-embedding\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:30:52.362763Z",
     "iopub.status.busy": "2025-02-19T22:30:52.362216Z",
     "iopub.status.idle": "2025-02-19T22:30:52.406576Z",
     "shell.execute_reply": "2025-02-19T22:30:52.406076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:52 TP0] Prefill batch. #new-seq: 1, #new-token: 4, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-19 22:30:52] INFO:     127.0.0.1:34678 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n",
      "Text embedding (first 10): [0.00830841064453125, 0.000698089599609375, -0.008087158203125, -0.0006761550903320312, 0.01438140869140625, -0.00899505615234375, 0.012359619140625, 0.00208282470703125, 0.00620269775390625, -0.003021240234375]\n"
     ]
    }
   ],
   "source": [
    "import subprocess, json\n",
    "\n",
    "text = \"Once upon a time\"\n",
    "\n",
    "curl_text = f\"\"\"curl -s http://localhost:{port}/v1/embeddings \\\n",
    "  -d '{{\"model\": \"Alibaba-NLP/gte-Qwen2-7B-instruct\", \"input\": \"{text}\"}}'\"\"\"\n",
    "\n",
    "text_embedding = json.loads(subprocess.check_output(curl_text, shell=True))[\"data\"][0][\n",
    "    \"embedding\"\n",
    "]\n",
    "\n",
    "print_highlight(f\"Text embedding (first 10): {text_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:30:52.408998Z",
     "iopub.status.busy": "2025-02-19T22:30:52.408529Z",
     "iopub.status.idle": "2025-02-19T22:30:52.461565Z",
     "shell.execute_reply": "2025-02-19T22:30:52.461155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:52 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3, cache hit rate: 21.43%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "2025-02-19 22:30:52,416 - INFO - flashinfer.jit: Loading JIT ops: cascade\n",
      "2025-02-19 22:30:52,428 - INFO - flashinfer.jit: Finished loading JIT ops: cascade\n",
      "[2025-02-19 22:30:52] INFO:     127.0.0.1:34690 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n",
      "Text embedding (first 10): [0.0083160400390625, 0.0006837844848632812, -0.00809478759765625, -0.0006804466247558594, 0.0143585205078125, -0.0089874267578125, 0.0123748779296875, 0.002101898193359375, 0.006221771240234375, -0.003025054931640625]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "text = \"Once upon a time\"\n",
    "\n",
    "response = requests.post(\n",
    "    f\"http://localhost:{port}/v1/embeddings\",\n",
    "    json={\"model\": \"Alibaba-NLP/gte-Qwen2-7B-instruct\", \"input\": text},\n",
    ")\n",
    "\n",
    "text_embedding = response.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "print_highlight(f\"Text embedding (first 10): {text_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:30:52.463641Z",
     "iopub.status.busy": "2025-02-19T22:30:52.463400Z",
     "iopub.status.idle": "2025-02-19T22:30:52.538440Z",
     "shell.execute_reply": "2025-02-19T22:30:52.538001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:52 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3, cache hit rate: 33.33%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-19 22:30:52] INFO:     127.0.0.1:34692 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n",
      "Text embedding (first 10): [0.0083160400390625, 0.0006837844848632812, -0.00809478759765625, -0.0006804466247558594, 0.0143585205078125, -0.0089874267578125, 0.0123748779296875, 0.002101898193359375, 0.006221771240234375, -0.003025054931640625]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", api_key=\"None\")\n",
    "\n",
    "# Text embedding example\n",
    "response = client.embeddings.create(\n",
    "    model=\"Alibaba-NLP/gte-Qwen2-7B-instruct\",\n",
    "    input=text,\n",
    ")\n",
    "\n",
    "embedding = response.data[0].embedding[:10]\n",
    "print_highlight(f\"Text embedding (first 10): {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Input IDs\n",
    "\n",
    "SGLang also supports `input_ids` as input to get the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:30:52.540772Z",
     "iopub.status.busy": "2025-02-19T22:30:52.540352Z",
     "iopub.status.idle": "2025-02-19T22:30:53.997670Z",
     "shell.execute_reply": "2025-02-19T22:30:53.997200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-19 22:30:53 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 3, cache hit rate: 40.91%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-19 22:30:53] INFO:     127.0.0.1:34702 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n",
      "Input IDs embedding (first 10): [0.0083160400390625, 0.0006837844848632812, -0.00809478759765625, -0.0006804466247558594, 0.0143585205078125, -0.0089874267578125, 0.0123748779296875, 0.002101898193359375, 0.006221771240234375, -0.003025054931640625]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-Qwen2-7B-instruct\")\n",
    "input_ids = tokenizer.encode(text)\n",
    "\n",
    "curl_ids = f\"\"\"curl -s http://localhost:{port}/v1/embeddings \\\n",
    "  -d '{{\"model\": \"Alibaba-NLP/gte-Qwen2-7B-instruct\", \"input\": {json.dumps(input_ids)}}}'\"\"\"\n",
    "\n",
    "input_ids_embedding = json.loads(subprocess.check_output(curl_ids, shell=True))[\"data\"][\n",
    "    0\n",
    "][\"embedding\"]\n",
    "\n",
    "print_highlight(f\"Input IDs embedding (first 10): {input_ids_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T22:30:53.999598Z",
     "iopub.status.busy": "2025-02-19T22:30:53.999439Z",
     "iopub.status.idle": "2025-02-19T22:30:54.047491Z",
     "shell.execute_reply": "2025-02-19T22:30:54.046478Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(embedding_process)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
