{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native APIs\n",
    "\n",
    "Apart from the OpenAI compatible APIs, the SGLang Runtime also provides its native server APIs. We introduce these following APIs:\n",
    "\n",
    "- `/generate` (text generation model)\n",
    "- `/get_model_info`\n",
    "- `/get_server_info`\n",
    "- `/health`\n",
    "- `/health_generate`\n",
    "- `/flush_cache`\n",
    "- `/update_weights`\n",
    "- `/encode`(embedding model)\n",
    "- `/classify`(reward model)\n",
    "\n",
    "We mainly use `requests` to test these APIs in the following examples. You can also use `curl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch A Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:48:18.728819Z",
     "iopub.status.busy": "2024-12-02T22:48:18.728690Z",
     "iopub.status.idle": "2024-12-02T22:48:47.958696Z",
     "shell.execute_reply": "2024-12-02T22:48:47.958226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-02 22:53:31] server_args=ServerArgs(model_path='meta-llama/Llama-3.2-1B-Instruct', tokenizer_path='meta-llama/Llama-3.2-1B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, context_length=None, device='cuda', served_model_name='meta-llama/Llama-3.2-1B-Instruct', chat_template=None, is_embedding=False, revision=None, host='127.0.0.1', port=30010, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, tp_size=1, stream_interval=1, random_seed=412569283, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='SGLang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, num_continuous_decode_steps=1, delete_ckpt_after_loading=False)\n",
      "/opt/dlami/nvme/chenyang/miniconda3/envs/sglang/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/opt/dlami/nvme/chenyang/miniconda3/envs/sglang/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[2024-12-02 22:53:38 TP0] Init torch distributed begin.\n",
      "[2024-12-02 22:53:39 TP0] Load weight begin. avail mem=4.98 GB\n",
      "[2024-12-02 22:53:39 TP0] lm_eval is not installed, GPTQ may not be usable\n",
      "[2024-12-02 22:53:39 TP0] Using model weights format ['*.safetensors']\n",
      "[2024-12-02 22:53:39 TP0] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.22it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.22it/s]\n",
      "\n",
      "[2024-12-02 22:53:40 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=2.34 GB\n",
      "[2024-12-02 22:53:40 TP0] Scheduler hit an exception: Traceback (most recent call last):\n",
      "  File \"/opt/dlami/nvme/chenyang/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py\", line 1489, in run_scheduler_process\n",
      "    scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/dlami/nvme/chenyang/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py\", line 194, in __init__\n",
      "    self.tp_worker = TpWorkerClass(\n",
      "                     ^^^^^^^^^^^^^^\n",
      "  File \"/opt/dlami/nvme/chenyang/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 61, in __init__\n",
      "    self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/dlami/nvme/chenyang/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/managers/tp_worker.py\", line 62, in __init__\n",
      "    self.model_runner = ModelRunner(\n",
      "                        ^^^^^^^^^^^^\n",
      "  File \"/opt/dlami/nvme/chenyang/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 164, in __init__\n",
      "    self.init_memory_pool(\n",
      "  File \"/opt/dlami/nvme/chenyang/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/model_executor/model_runner.py\", line 544, in init_memory_pool\n",
      "    self.token_to_kv_pool = MHATokenToKVPool(\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/dlami/nvme/chenyang/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/mem_cache/memory_pool.py\", line 194, in __init__\n",
      "    self.v_buffer = [\n",
      "                    ^\n",
      "  File \"/opt/dlami/nvme/chenyang/miniconda3/envs/sglang/lib/python3.11/site-packages/sglang/srt/mem_cache/memory_pool.py\", line 195, in <listcomp>\n",
      "    torch.empty(\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 27.50 MiB is free. Process 3843098 has 32.29 GiB memory in use. Process 3908588 has 41.52 GiB memory in use. Including non-PyTorch memory, this process has 5.24 GiB memory in use. Of the allocated memory 4.63 GiB is allocated by PyTorch, and 19.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sglang.utils import (\n",
    "    execute_shell_command,\n",
    "    wait_for_server,\n",
    "    terminate_process,\n",
    "    print_highlight,\n",
    ")\n",
    "\n",
    "import requests\n",
    "\n",
    "server_process = execute_shell_command(\n",
    "    \"\"\"\n",
    "python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-1B-Instruct --port=30010\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(\"http://localhost:30010\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate (text generation model)\n",
    "Generate completions. This is similar to the `/v1/completions` in OpenAI API. Detailed parameters can be found in the [sampling parameters](../references/sampling_params.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:48:47.960280Z",
     "iopub.status.busy": "2024-12-02T22:48:47.960068Z",
     "iopub.status.idle": "2024-12-02T22:48:48.227582Z",
     "shell.execute_reply": "2024-12-02T22:48:48.227156Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://localhost:30010/generate\"\n",
    "data = {\"text\": \"What is the capital of France?\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Info\n",
    "\n",
    "Get the information of the model.\n",
    "\n",
    "- `model_path`: The path/name of the model.\n",
    "- `is_generation`: Whether the model is used as generation model or embedding model.\n",
    "- `tokenizer_path`: The path/name of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:48:48.228900Z",
     "iopub.status.busy": "2024-12-02T22:48:48.228763Z",
     "iopub.status.idle": "2024-12-02T22:48:48.234059Z",
     "shell.execute_reply": "2024-12-02T22:48:48.233689Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://localhost:30010/get_model_info\"\n",
    "\n",
    "response = requests.get(url)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"model_path\"] == \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "assert response_json[\"is_generation\"] is True\n",
    "assert response_json[\"tokenizer_path\"] == \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "assert response_json.keys() == {\"model_path\", \"is_generation\", \"tokenizer_path\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Server Info\n",
    "Gets the server information including CLI arguments, token limits, and memory pool sizes.\n",
    "- Note: `get_server_info` merges the following deprecated endpoints:\n",
    "  - `get_server_args`\n",
    "  - `get_memory_pool_size` \n",
    "  - `get_max_total_num_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:48:48.235454Z",
     "iopub.status.busy": "2024-12-02T22:48:48.235125Z",
     "iopub.status.idle": "2024-12-02T22:48:48.239787Z",
     "shell.execute_reply": "2024-12-02T22:48:48.239433Z"
    }
   },
   "outputs": [],
   "source": [
    "# get_server_info\n",
    "\n",
    "url = \"http://localhost:30010/get_server_info\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check\n",
    "- `/health`: Check the health of the server.\n",
    "- `/health_generate`: Check the health of the server by generating one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:48:48.240966Z",
     "iopub.status.busy": "2024-12-02T22:48:48.240838Z",
     "iopub.status.idle": "2024-12-02T22:48:48.254049Z",
     "shell.execute_reply": "2024-12-02T22:48:48.253663Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://localhost:30010/health_generate\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:48:48.255429Z",
     "iopub.status.busy": "2024-12-02T22:48:48.255085Z",
     "iopub.status.idle": "2024-12-02T22:48:48.259344Z",
     "shell.execute_reply": "2024-12-02T22:48:48.258964Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://localhost:30010/health\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flush Cache\n",
    "\n",
    "Flush the radix cache. It will be automatically triggered when the model weights are updated by the `/update_weights` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:48:48.260464Z",
     "iopub.status.busy": "2024-12-02T22:48:48.260335Z",
     "iopub.status.idle": "2024-12-02T22:48:48.265409Z",
     "shell.execute_reply": "2024-12-02T22:48:48.264993Z"
    }
   },
   "outputs": [],
   "source": [
    "# flush cache\n",
    "\n",
    "url = \"http://localhost:30010/flush_cache\"\n",
    "\n",
    "response = requests.post(url)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Weights From Disk\n",
    "\n",
    "Update model weights from disk without restarting the server. Only applicable for models with the same architecture and parameter size.\n",
    "\n",
    "SGLang support `update_weights_from_disk` API for continuous evaluation during training (save checkpoint to disk and update weights from disk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:48:48.266586Z",
     "iopub.status.busy": "2024-12-02T22:48:48.266452Z",
     "iopub.status.idle": "2024-12-02T22:48:48.946566Z",
     "shell.execute_reply": "2024-12-02T22:48:48.946081Z"
    }
   },
   "outputs": [],
   "source": [
    "# successful update with same architecture and size\n",
    "\n",
    "url = \"http://localhost:30010/update_weights_from_disk\"\n",
    "data = {\"model_path\": \"meta-llama/Llama-3.2-1B\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print_highlight(response.text)\n",
    "assert response.json()[\"success\"] is True\n",
    "assert response.json()[\"message\"] == \"Succeeded to update model weights.\"\n",
    "assert response.json().keys() == {\"success\", \"message\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:48:48.948180Z",
     "iopub.status.busy": "2024-12-02T22:48:48.947760Z",
     "iopub.status.idle": "2024-12-02T22:48:49.075751Z",
     "shell.execute_reply": "2024-12-02T22:48:49.075374Z"
    }
   },
   "outputs": [],
   "source": [
    "# failed update with different parameter size or wrong name\n",
    "\n",
    "url = \"http://localhost:30010/update_weights_from_disk\"\n",
    "data = {\"model_path\": \"meta-llama/Llama-3.2-1B-wrong\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"success\"] is False\n",
    "assert response_json[\"message\"] == (\n",
    "    \"Failed to get weights iterator: \"\n",
    "    \"meta-llama/Llama-3.2-1B-wrong\"\n",
    "    \" (repository not found).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode (embedding model)\n",
    "\n",
    "Encode text into embeddings. Note that this API is only available for [embedding models](openai_api_embeddings.html#openai-apis-embedding) and will raise an error for generation models.\n",
    "Therefore, we launch a new server to server an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:48:49.076953Z",
     "iopub.status.busy": "2024-12-02T22:48:49.076828Z",
     "iopub.status.idle": "2024-12-02T22:49:23.694647Z",
     "shell.execute_reply": "2024-12-02T22:49:23.694216Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(server_process)\n",
    "\n",
    "embedding_process = execute_shell_command(\n",
    "    \"\"\"\n",
    "python -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-7B-instruct \\\n",
    "    --port 30020 --host 0.0.0.0 --is-embedding\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(\"http://localhost:30020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:49:23.696104Z",
     "iopub.status.busy": "2024-12-02T22:49:23.695882Z",
     "iopub.status.idle": "2024-12-02T22:49:23.723827Z",
     "shell.execute_reply": "2024-12-02T22:49:23.723526Z"
    }
   },
   "outputs": [],
   "source": [
    "# successful encode for embedding model\n",
    "\n",
    "url = \"http://localhost:30020/encode\"\n",
    "data = {\"model\": \"Alibaba-NLP/gte-Qwen2-7B-instruct\", \"text\": \"Once upon a time\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "response_json = response.json()\n",
    "print_highlight(f\"Text embedding (first 10): {response_json['embedding'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify (reward model)\n",
    "\n",
    "SGLang Runtime also supports reward models. Here we use a reward model to classify the quality of pairwise generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:49:23.724947Z",
     "iopub.status.busy": "2024-12-02T22:49:23.724827Z",
     "iopub.status.idle": "2024-12-02T22:49:47.825508Z",
     "shell.execute_reply": "2024-12-02T22:49:47.825004Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(embedding_process)\n",
    "\n",
    "# Note that SGLang now treats embedding models and reward models as the same type of models.\n",
    "# This will be updated in the future.\n",
    "\n",
    "reward_process = execute_shell_command(\n",
    "    \"\"\"\n",
    "python -m sglang.launch_server --model-path Skywork/Skywork-Reward-Llama-3.1-8B-v0.2 --port 30030 --host 0.0.0.0 --is-embedding\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(\"http://localhost:30030\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:49:47.826961Z",
     "iopub.status.busy": "2024-12-02T22:49:47.826823Z",
     "iopub.status.idle": "2024-12-02T22:49:48.853347Z",
     "shell.execute_reply": "2024-12-02T22:49:48.852882Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "PROMPT = (\n",
    "    \"What is the range of the numeric output of a sigmoid node in a neural network?\"\n",
    ")\n",
    "\n",
    "RESPONSE1 = \"The output of a sigmoid node is bounded between -1 and 1.\"\n",
    "RESPONSE2 = \"The output of a sigmoid node is bounded between 0 and 1.\"\n",
    "\n",
    "CONVS = [\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE1}],\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", \"content\": RESPONSE2}],\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\")\n",
    "prompts = tokenizer.apply_chat_template(CONVS, tokenize=False)\n",
    "\n",
    "url = \"http://localhost:30030/encode\"\n",
    "data = {\"model\": \"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\", \"text\": prompts}\n",
    "\n",
    "responses = requests.post(url, json=data).json()\n",
    "for response in responses:\n",
    "    print_highlight(f\"reward: {response['embedding'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:49:48.854711Z",
     "iopub.status.busy": "2024-12-02T22:49:48.854489Z",
     "iopub.status.idle": "2024-12-02T22:49:48.919126Z",
     "shell.execute_reply": "2024-12-02T22:49:48.918561Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_process(reward_process)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
