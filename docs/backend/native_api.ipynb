{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native Server API\n",
    "\n",
    "Apart from the OpenAI compatible API, the SGLang Runtime also provides its native server API. We introduce these following API:\n",
    "\n",
    "- `/generate`\n",
    "- `/update_weights`\n",
    "- `/get_server_args`\n",
    "- `/get_model_info`\n",
    "- `/health`\n",
    "- `/health_generate`\n",
    "- `/flush_cache`\n",
    "- `/get_memory_pool_size`\n",
    "\n",
    "We mainly use `requests` to test these APIs in the following examples. You can also use `curl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch The Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[2024-11-01 21:15:39] server_args=ServerArgs(model_path='meta-llama/Llama-3.2-1B-Instruct', tokenizer_path='meta-llama/Llama-3.2-1B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, context_length=None, device='cuda', served_model_name='meta-llama/Llama-3.2-1B-Instruct', chat_template=None, is_embedding=False, host='127.0.0.1', port=30010, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, tp_size=1, stream_interval=1, random_seed=68951524, constrained_json_whitespace_pattern=None, decode_log_interval=40, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, api_key=None, file_storage_pth='SGLang_storage', enable_cache_report=False, watchdog_timeout=600, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', disable_flashinfer=False, disable_flashinfer_sampling=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_penalizer=False, disable_nan_detection=False, enable_overlap_schedule=False, enable_mixed_chunk=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, torchao_config='', enable_p2p_check=False, triton_attention_reduce_in_fp32=False, num_continuous_decode_steps=1)\n",
      "/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/chenyang/miniconda3/envs/AlphaMeemory/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[2024-11-01 21:15:45 TP0] Init torch distributed begin.\n",
      "[2024-11-01 21:15:45 TP0] Load weight begin. avail mem=47.27 GB\n",
      "[2024-11-01 21:15:46 TP0] lm_eval is not installed, GPTQ may not be usable\n",
      "INFO 11-01 21:15:46 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-01 21:15:46 weight_utils.py:288] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.72it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.71it/s]\n",
      "\n",
      "[2024-11-01 21:15:47 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=44.40 GB\n",
      "[2024-11-01 21:15:47 TP0] Memory pool end. avail mem=3.65 GB\n",
      "[2024-11-01 21:15:47 TP0] Capture cuda graph begin. This can take up to several minutes.\n",
      "[2024-11-01 21:15:54 TP0] max_total_num_tokens=1269153, max_prefill_tokens=16384, max_running_requests=4097, context_len=131072\n",
      "[2024-11-01 21:15:54] INFO:     Started server process [3282187]\n",
      "[2024-11-01 21:15:54] INFO:     Waiting for application startup.\n",
      "[2024-11-01 21:15:54] INFO:     Application startup complete.\n",
      "[2024-11-01 21:15:54] INFO:     Uvicorn running on http://127.0.0.1:30010 (Press CTRL+C to quit)\n",
      "[2024-11-01 21:15:55] INFO:     127.0.0.1:43900 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2024-11-01 21:15:55] INFO:     127.0.0.1:43914 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2024-11-01 21:15:55 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2024-11-01 21:15:55] INFO:     127.0.0.1:43926 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2024-11-01 21:15:55] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.utils import (\n",
    "    execute_shell_command,\n",
    "    wait_for_server,\n",
    "    terminate_process,\n",
    "    print_highlight,\n",
    ")\n",
    "import subprocess, json\n",
    "\n",
    "server_process = execute_shell_command(\n",
    "    \"\"\"\n",
    "    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-1B-Instruct --port=30010\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(\"http://localhost:30010\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate\n",
    "\n",
    "Used to generate completion from the model, similar to the `/v1/chat/completions` API in OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 21:16:16 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 8, cache hit rate: 36.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2024-11-01 21:16:16 TP0] Decode batch. #running-req: 1, #token: 35, token usage: 0.00, gen throughput (token/s): 8.13, #queue-req: 0\n",
      "[2024-11-01 21:16:16 TP0] Decode batch. #running-req: 1, #token: 75, token usage: 0.00, gen throughput (token/s): 213.64, #queue-req: 0\n",
      "[2024-11-01 21:16:17 TP0] Decode batch. #running-req: 1, #token: 115, token usage: 0.00, gen throughput (token/s): 212.85, #queue-req: 0\n",
      "[2024-11-01 21:16:17] INFO:     127.0.0.1:49030 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'>{\"text\":\" \\n\\nI. Gambia\\nII. Kenya\\nIII. Mexico City\\n\\n- List the key facts about each. \\n\\n- Gambia said goodbye to the UK and Canada no longer represents the Gambia in international sports.\\n- IPMA ( pov-ed) publishes educational informations on real motives about crop loans grace elimination of gender diservidence.\\n- The Gambia gained independence from colonial powers on 18 September 1965.\\n\\n- IPMA (unofficially) says Gambia should be at the top of the UNG global governance sector ranking. To accomplish this goal, the Gambia has to take til an active still greatest role in\",\"meta_info\":{\"prompt_tokens\":9,\"completion_tokens\":128,\"completion_tokens_wo_jump_forward\":128,\"cached_tokens\":8,\"finish_reason\":{\"type\":\"length\",\"length\":128},\"id\":\"4077c0bf806e49c5add2da62e53bda19\"},\"index\":0}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"http://localhost:30010/generate\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer None\"\n",
    "}\n",
    "data = {\n",
    "    \"text\": \"List 3 countries and their capitals.\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Server Args\n",
    "\n",
    "Used to get the serving args when the server is launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 21:16:46] INFO:     127.0.0.1:56770 - \"GET /get_server_args HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'>{'model_path': 'meta-llama/Llama-3.2-1B-Instruct', 'tokenizer_path': 'meta-llama/Llama-3.2-1B-Instruct', 'tokenizer_mode': 'auto', 'skip_tokenizer_init': False, 'load_format': 'auto', 'trust_remote_code': False, 'dtype': 'auto', 'kv_cache_dtype': 'auto', 'quantization': None, 'context_length': None, 'device': 'cuda', 'served_model_name': 'meta-llama/Llama-3.2-1B-Instruct', 'chat_template': None, 'is_embedding': False, 'host': '127.0.0.1', 'port': 30010, 'mem_fraction_static': 0.88, 'max_running_requests': None, 'max_total_tokens': None, 'chunked_prefill_size': 8192, 'max_prefill_tokens': 16384, 'schedule_policy': 'lpm', 'schedule_conservativeness': 1.0, 'tp_size': 1, 'stream_interval': 1, 'random_seed': 68951524, 'constrained_json_whitespace_pattern': None, 'decode_log_interval': 40, 'log_level': 'info', 'log_level_http': None, 'log_requests': False, 'show_time_cost': False, 'api_key': None, 'file_storage_pth': 'SGLang_storage', 'enable_cache_report': False, 'watchdog_timeout': 600, 'dp_size': 1, 'load_balance_method': 'round_robin', 'dist_init_addr': None, 'nnodes': 1, 'node_rank': 0, 'json_model_override_args': '{}', 'enable_double_sparsity': False, 'ds_channel_config_path': None, 'ds_heavy_channel_num': 32, 'ds_heavy_token_num': 256, 'ds_heavy_channel_type': 'qk', 'ds_sparse_decode_threshold': 4096, 'lora_paths': None, 'max_loras_per_batch': 8, 'attention_backend': 'flashinfer', 'sampling_backend': 'flashinfer', 'grammar_backend': 'outlines', 'disable_flashinfer': False, 'disable_flashinfer_sampling': False, 'disable_radix_cache': False, 'disable_regex_jump_forward': False, 'disable_cuda_graph': False, 'disable_cuda_graph_padding': False, 'disable_disk_cache': False, 'disable_custom_all_reduce': False, 'disable_mla': False, 'disable_penalizer': False, 'disable_nan_detection': False, 'enable_overlap_schedule': False, 'enable_mixed_chunk': False, 'enable_torch_compile': False, 'torch_compile_max_bs': 32, 'cuda_graph_max_bs': 160, 'torchao_config': '', 'enable_p2p_check': False, 'triton_attention_reduce_in_fp32': False, 'num_continuous_decode_steps': 1}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get server args\n",
    "\n",
    "url = \"http://localhost:30010/get_server_args\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer None\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "print_highlight(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Info\n",
    "\n",
    "Used to get the model info.\n",
    "\n",
    "- `model_path`: The path/name of the model.\n",
    "- `is_generation`: Whether the model is used as generation model or embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 21:17:54] INFO:     127.0.0.1:49774 - \"GET /get_model_info HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'>{'model_path': 'meta-llama/Llama-3.2-1B-Instruct', 'is_generation': True}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"http://localhost:30010/get_model_info\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer None\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"model_path\"] == \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "assert response_json[\"is_generation\"] == True\n",
    "assert response_json.keys() == {\"model_path\", \"is_generation\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health and Health Generate\n",
    "\n",
    "- `/health`: Check the health of the server.\n",
    "- `/health_generate`: Check the health of the server by generating one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 21:18:13 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1, cache hit rate: 37.04%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2024-11-01 21:18:13] INFO:     127.0.0.1:60162 - \"GET /health_generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 21:18:13] INFO:     127.0.0.1:60172 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# health generate\n",
    "\n",
    "url = \"http://localhost:30010/health_generate\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer None\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "print_highlight(response.text)\n",
    "\n",
    "# health\n",
    "\n",
    "url = \"http://localhost:30010/health\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flush Cache\n",
    "\n",
    "Used to flush the radix cache. It will be automatically triggered when the model weights are updated by the `/update_weights` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 21:18:28] INFO:     127.0.0.1:48360 - \"POST /flush_cache HTTP/1.1\" 200 OK\n",
      "[2024-11-01 21:18:28 TP0] Cache flushed successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'>Cache flushed.<br>Please check backend logs for more details. (When there are running or waiting requests, the operation will not be performed.)<br></strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# flush cache\n",
    "\n",
    "url = \"http://localhost:30010/flush_cache\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer None\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Memory Pool Size\n",
    "\n",
    "Get the memory pool size in number of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 21:18:42] INFO:     127.0.0.1:37912 - \"GET /get_memory_pool_size HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'>1269153</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get_memory_pool_size\n",
    "\n",
    "url = \"http://localhost:30010/get_memory_pool_size\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer None\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "print_highlight(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Weights\n",
    "\n",
    "Update model weights without restarting the server. Use for continuous evaluation during training. Only applicable for models with the same architecture and parameter size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 21:19:26 TP0] Update weights begin. avail mem=1.35 GB\n",
      "INFO 11-01 21:19:27 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-01 21:19:27 weight_utils.py:288] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.18it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.18it/s]\n",
      "\n",
      "[2024-11-01 21:19:27 TP0] Update weights end.\n",
      "[2024-11-01 21:19:27 TP0] Cache flushed successfully!\n",
      "[2024-11-01 21:19:27] INFO:     127.0.0.1:60486 - \"POST /update_weights HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'>{\"success\":true,\"message\":\"Succeeded to update model weights.\"}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# update weights\n",
    "\n",
    "# successful update with same architecture and size\n",
    "\n",
    "url = \"http://localhost:30010/update_weights\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer None\"\n",
    "}\n",
    "data = {\n",
    "    \"model_path\": \"meta-llama/Llama-3.2-1B\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print_highlight(response.text)\n",
    "assert response.json()[\"success\"] == True\n",
    "assert response.json()[\"message\"] == \"Succeeded to update model weights.\"\n",
    "assert response.json().keys() == {\"success\", \"message\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-01 19:53:00 TP0] Update weights begin. avail mem=1.35 GB\n",
      "INFO 11-01 19:53:01 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "INFO 11-01 19:53:01 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-01 19:53:02 weight_utils.py:288] No model.safetensors.index.json found in remote.\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.65it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.65it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "\n",
      "[2024-11-01 19:53:02 TP0] Failed to update weights: The size of tensor a (2048) must match the size of tensor b (3072) at non-singleton dimension 1.\n",
      "Rolling back to original weights.\n",
      "[2024-11-01 19:53:02] INFO:     127.0.0.1:59090 - \"POST /update_weights HTTP/1.1\" 400 Bad Request\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #FF0000;'>{'success': False, 'message': 'Failed to update weights: The size of tensor a (2048) must match the size of tensor b (3072) at non-singleton dimension 1.\\nRolling back to original weights.'}</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# failed update with different parameter size\n",
    "\n",
    "url = \"http://localhost:30010/update_weights\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer None\"\n",
    "}\n",
    "data = {\n",
    "    \"model_path\": \"meta-llama/Llama-3.2-3B\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "response_json = response.json()\n",
    "print_highlight(response_json)\n",
    "assert response_json[\"success\"] == False\n",
    "assert response_json[\"message\"] == (\n",
    "    \"Failed to update weights: The size of tensor a (2048) must match \"\n",
    "    \"the size of tensor b (3072) at non-singleton dimension 1.\\n\"\n",
    "    \"Rolling back to original weights.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlphaMeemory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
