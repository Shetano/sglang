{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Function Calling Streaming Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the Server\n",
    "To begin, you need to launch a server that will handle the requests. The following code block is used to launch the server.\n",
    "After the server is running, you can interact with it using the client to get the weather information.\n",
    "\n",
    "```bash\n",
    "python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --port 30000 --host 0.0.0.0\n",
    "```\n",
    "This starts the server on port `30000`. Once running, you can make requests to it from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 03:56:53] server_args=ServerArgs(model_path='meta-llama/Meta-Llama-3.1-8B-Instruct', tokenizer_path='meta-llama/Meta-Llama-3.1-8B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, context_length=None, device='cuda', served_model_name='meta-llama/Meta-Llama-3.1-8B-Instruct', chat_template=None, is_embedding=False, revision=None, return_token_ids=False, host='0.0.0.0', port=30111, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, random_seed=465810866, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='SGLang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False)\n",
      "[2025-01-03 03:57:01 TP0] Init torch distributed begin.\n",
      "[2025-01-03 03:57:01 TP0] Load weight begin. avail mem=78.84 GB\n",
      "[2025-01-03 03:57:02 TP0] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.24it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.24it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.81it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.60it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.54it/s]\n",
      "\n",
      "[2025-01-03 03:57:05 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=63.76 GB\n",
      "[2025-01-03 03:57:06 TP0] Memory pool end. avail mem=8.38 GB\n",
      "[2025-01-03 03:57:06 TP0] Capture cuda graph begin. This can take up to several minutes.\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.50it/s]\n",
      "[2025-01-03 03:57:12 TP0] Capture cuda graph end. Time elapsed: 6.58 s\n",
      "[2025-01-03 03:57:13 TP0] max_total_num_tokens=444824, max_prefill_tokens=16384, max_running_requests=2049, context_len=131072\n",
      "[2025-01-03 03:57:13] INFO:     Started server process [3297308]\n",
      "[2025-01-03 03:57:13] INFO:     Waiting for application startup.\n",
      "[2025-01-03 03:57:13] INFO:     Application startup complete.\n",
      "[2025-01-03 03:57:13] INFO:     Uvicorn running on http://0.0.0.0:30111 (Press CTRL+C to quit)\n",
      "[2025-01-03 03:57:13] INFO:     127.0.0.1:47024 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-01-03 03:57:14] INFO:     127.0.0.1:47040 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-01-03 03:57:14 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-01-03 03:57:14] INFO:     127.0.0.1:47054 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-01-03 03:57:14] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.utils import execute_shell_command, wait_for_server, terminate_process\n",
    "\n",
    "server_process = execute_shell_command(\n",
    "    \"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --port 30111 --host 0.0.0.0\"\n",
    ")\n",
    "wait_for_server(\"http://localhost:30111\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools for Function Call\n",
    "Next, we'll define the tools for our function call. In this example, we define a function for getting the current weather in a specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 03:57:19] INFO:     127.0.0.1:35030 - \"GET /v1/models HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston today? Please respond with the format: Today's weather is :{function call result}\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize OpenAI-like client\n",
    "client = OpenAI(api_key=\"YOUR_API_KEY\", base_url=\"http://0.0.0.0:30111/v1\")\n",
    "model_name = client.models.list().data[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Non-Streaming Request\n",
    "We'll now test the non-streaming function call and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 03:57:19 TP0] Prefill batch. #new-seq: 1, #new-token: 235, #cached-token: 1, cache hit rate: 0.41%, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 03:57:19] INFO:     127.0.0.1:35030 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "Non-stream response:\n",
      "ChatCompletion(id='9663d03c6f7048539cdef8e2bf4bbc59', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='0', function=Function(arguments='{\"location\": \"Boston, MA\", \"unit\": \"fahrenheit\"}', name='get_current_weather'), type='function')]), matched_stop=128008)], created=1735876639, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=28, prompt_tokens=236, total_tokens=264, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "# Non-streaming mode test\n",
    "response_non_stream = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=0.8,\n",
    "    top_p=0.8,\n",
    "    stream=False,  # Non-streaming\n",
    "    tools=tools,\n",
    ")\n",
    "print(\"Non-stream response:\")\n",
    "print(response_non_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Streaming Request\n",
    "Next, we will test the streaming function call. The response will come in chunks, and we will handle the stream and process the function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "[2025-01-03 03:57:19] INFO:     127.0.0.1:35030 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-01-03 03:57:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 235, cache hit rate: 49.27%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "ChatCompletionChunk(id='3e30674d309747eb9fc9a7e4c177c843', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason='', index=0, logprobs=None, matched_stop=None)], created=1735876639, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "starting on new tool %d 0\n",
      "[2025-01-03 03:57:19 TP0] Decode batch. #running-req: 1, #token: 241, token usage: 0.00, gen throughput (token/s): 6.42, #queue-req: 0\n",
      "ChatCompletionChunk(id='3e30674d309747eb9fc9a7e4c177c843', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_weather'), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1735876639, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='3e30674d309747eb9fc9a7e4c177c843', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='{\"unit\": \"', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1735876639, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3e30674d309747eb9fc9a7e4c177c843', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='f', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1735876639, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3e30674d309747eb9fc9a7e4c177c843', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='ahrenheit\"', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1735876639, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3e30674d309747eb9fc9a7e4c177c843', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments=', \"location\": \"', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1735876639, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3e30674d309747eb9fc9a7e4c177c843', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='Boston', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1735876639, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3e30674d309747eb9fc9a7e4c177c843', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments=',', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1735876639, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3e30674d309747eb9fc9a7e4c177c843', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments=' MA\"}', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1735876639, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3e30674d309747eb9fc9a7e4c177c843', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1735876639, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "Streamed function call name: get_current_weather\n",
      "Streamed function call arguments: {\"unit\": \"\n",
      "Streamed function call arguments: f\n",
      "Streamed function call arguments: ahrenheit\"\n",
      "Streamed function call arguments: , \"location\": \"\n",
      "Streamed function call arguments: Boston\n",
      "Streamed function call arguments: ,\n",
      "Streamed function call arguments:  MA\"}\n",
      "Final streamed function call arguments: {\"unit\": \"fahrenheit\", \"location\": \"Boston, MA\"}\n"
     ]
    }
   ],
   "source": [
    "# Streaming mode test\n",
    "print(\"Streaming response:\")\n",
    "response_stream = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=0.8,\n",
    "    top_p=0.8,\n",
    "    stream=True,  # Enable streaming\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "# Handle streaming responses, combine different chunks\n",
    "chunks = []\n",
    "for chunk in response_stream:\n",
    "    chunks.append(chunk)\n",
    "    print(chunk)  # Optionally print each chunk to observe its content\n",
    "\n",
    "# Parse and combine function call arguments\n",
    "arguments = []\n",
    "for chunk in chunks:\n",
    "    choice = chunk.choices[0]\n",
    "    delta = choice.delta\n",
    "    if delta.tool_calls:\n",
    "        tool_call = delta.tool_calls[0]\n",
    "        if tool_call.function.name:\n",
    "            print(f\"Streamed function call name: {tool_call.function.name}\")\n",
    "\n",
    "        if tool_call.function.arguments:\n",
    "            arguments.append(tool_call.function.arguments)\n",
    "            print(f\"Streamed function call arguments: {tool_call.function.arguments}\")\n",
    "\n",
    "# Combine all argument fragments\n",
    "full_arguments = \"\".join(arguments)\n",
    "print(f\"Final streamed function call arguments: {full_arguments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Tool Call\n",
    "In this section, we'll simulate a call to the weather function and handle the arguments passed from the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call result: The weather in Boston, MA is 85 degrees fahrenheit. It is partly cloudy, with highs in the 90's.\n"
     ]
    }
   ],
   "source": [
    "# Add user message and function call result to the message list\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\",\n",
    "        \"tool_calls\": {\"name\": \"get_current_weather\", \"arguments\": full_arguments},\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Define the actual function for getting current weather\n",
    "def get_current_weather(location: str, unit: str):\n",
    "    # Here you can integrate an actual weather API\n",
    "    return f\"The weather in {location} is 85 degrees {unit}. It is partly cloudy, with highs in the 90's.\"\n",
    "\n",
    "\n",
    "# Simulate tool call\n",
    "available_tools = {\"get_current_weather\": get_current_weather}\n",
    "\n",
    "# Parse JSON arguments\n",
    "try:\n",
    "    call_data = json.loads(full_arguments)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON decoding error: {e}\")\n",
    "    call_data = {}\n",
    "\n",
    "# Call the corresponding tool function\n",
    "if \"tool_calls\" in messages[-1] and \"name\" in messages[-1][\"tool_calls\"]:\n",
    "    tool_name = messages[-1][\"tool_calls\"][\"name\"]\n",
    "    if tool_name in available_tools:\n",
    "        tool_to_call = available_tools[tool_name]\n",
    "        result = tool_to_call(**call_data)\n",
    "        print(f\"Function call result: {result}\")\n",
    "        messages.append({\"role\": \"tool\", \"content\": result, \"name\": tool_name})\n",
    "    else:\n",
    "        print(f\"Unknown tool name: {tool_name}\")\n",
    "else:\n",
    "    print(\"Function call name not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Chat Completion\n",
    "We will now perform the final chat completion using the simulated function call result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 03:57:19 TP0] Prefill batch. #new-seq: 1, #new-token: 40, #cached-token: 233, cache hit rate: 62.37%, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-03 03:57:19 TP0] Decode batch. #running-req: 1, #token: 290, token usage: 0.00, gen throughput (token/s): 81.38, #queue-req: 0\n",
      "[2025-01-03 03:57:20] INFO:     127.0.0.1:35030 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\n",
      "Final Chat Completion:\n",
      "ChatCompletion(id='dbc7c72d3cc54dcc807683232c4ec487', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='0', function=Function(arguments='{\"unit\": \"fahrenheit\", \"location\": \"Boston, MA\"}', name='get_current_weather'), type='function')]), matched_stop=128008)], created=1735876640, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=28, prompt_tokens=273, total_tokens=301, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "chat_completion_final = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=0.8,\n",
    "    top_p=0.8,\n",
    "    stream=False,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Chat Completion:\")\n",
    "print(chat_completion_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate Server\n",
    "Once you are done, you can terminate the server process using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
