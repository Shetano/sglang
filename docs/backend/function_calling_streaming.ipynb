{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGLang – Tool Calling Feature Documentation\n",
    "\n",
    "This guide demonstrates how to use SGLang’s **ToolCalling** functionality—from starting the server to invoking custom tool functions. The main steps include:\n",
    "\n",
    "1.  Launching the server\n",
    "2.  Defining tools (Tools)\n",
    "3.  Defining messages (Messages)\n",
    "4.  Initializing the client\n",
    "5.  Sending requests (non-streaming and streaming)\n",
    "6.  Handling tool calls from the server\n",
    "7.  Simulating and executing tool functions\n",
    "\n",
    "> **Note**: This example uses a `get_current_weather` function to fetch weather information, serving as a demonstration. You can replace or add any tool function depending on your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the Server\n",
    "First, you need to launch the SGLang server so it can handle incoming requests. The server is started with the `sglang.launch_server` command.\n",
    "\n",
    "**Example command:**\n",
    "\n",
    "```bash\n",
    "python -m sglang.launch_server \\\n",
    "  --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "  --tool-call-parser llama3 \\\n",
    "  --port 30000 \\\n",
    "  --host 0.0.0.0\n",
    "\n",
    "```\n",
    "\n",
    "-   **`--model-path`**: Specifies the path of the model to be used, e.g., `meta-llama/Meta-Llama-3.1-8B-Instruct`.\n",
    "-   **`--tool-call-parser`**: Defines the parser used to interpret responses. Currently supported parsers include:\n",
    "    -   llama3: Llama 3.1 / 3.2 (e.g. `meta-llama/Llama-3.1-8B-Instruct`, `meta-llama/Llama-3.2-1B-Instruct`)\n",
    "    -   mistral: Mistral (e.g. `mistralai/Mistral-7B-Instruct-v0.3`, `mistralai/Mistral-Nemo-Instruct-2407`, `mistralai/Mistral-Nemo-Instruct-2407`, `mistralai/Mistral-7B-v0.3`)\n",
    "    -   qwen25: Qwen 2.5 (e.g. `Qwen/Qwen2.5-1.5B-Instruct`, `Qwen/Qwen2.5-7B-Instruct`)\n",
    "-   **`--port`**: Sets the port number for the server (e.g., 30000).\n",
    "-   **`--host`**: Sets the hostname or IP address (using `0.0.0.0` allows connections from any network).\n",
    "\n",
    "The following code block is used to launch the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-08 05:42:08] server_args=ServerArgs(model_path='meta-llama/Meta-Llama-3.1-8B-Instruct', tokenizer_path='meta-llama/Meta-Llama-3.1-8B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, context_length=None, device='cuda', served_model_name='meta-llama/Meta-Llama-3.1-8B-Instruct', chat_template=None, is_embedding=False, revision=None, return_token_ids=False, host='0.0.0.0', port=30222, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, random_seed=944500726, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='SGLang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, tool_call_parser='llama3')\n",
      "[2025-01-08 05:42:17 TP0] Init torch distributed begin.\n",
      "[2025-01-08 05:42:18 TP0] Load weight begin. avail mem=78.84 GB\n",
      "[2025-01-08 05:42:19 TP0] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.17it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.12it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.64it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.41it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.37it/s]\n",
      "\n",
      "[2025-01-08 05:42:22 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=63.76 GB\n",
      "[2025-01-08 05:42:22 TP0] Memory pool end. avail mem=8.38 GB\n",
      "[2025-01-08 05:42:23 TP0] Capture cuda graph begin. This can take up to several minutes.\n",
      "100%|██████████| 23/23 [00:07<00:00,  3.18it/s]\n",
      "[2025-01-08 05:42:30 TP0] Capture cuda graph end. Time elapsed: 7.23 s\n",
      "[2025-01-08 05:42:30 TP0] max_total_num_tokens=444824, max_prefill_tokens=16384, max_running_requests=2049, context_len=131072\n",
      "[2025-01-08 05:42:30] INFO:     Started server process [4019092]\n",
      "[2025-01-08 05:42:30] INFO:     Waiting for application startup.\n",
      "[2025-01-08 05:42:30] INFO:     Application startup complete.\n",
      "[2025-01-08 05:42:30] INFO:     Uvicorn running on http://0.0.0.0:30222 (Press CTRL+C to quit)\n",
      "[2025-01-08 05:42:31] INFO:     127.0.0.1:52162 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-01-08 05:42:31] INFO:     127.0.0.1:52174 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-01-08 05:42:31 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-01-08 05:42:32] INFO:     127.0.0.1:52188 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-01-08 05:42:32] The server is fired up and ready to roll!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong style='color: #00008B;'><br><br>                    NOTE: Typically, the server runs in a separate terminal.<br>                    In this notebook, we run the server and notebook code together, so their outputs are combined.<br>                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.<br>                    </strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sglang.utils import execute_shell_command, wait_for_server, terminate_process\n",
    "\n",
    "server_process = execute_shell_command(\n",
    "    \"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --tool-call-parser llama3 --port 30222 --host 0.0.0.0\"  # llama3\n",
    ")\n",
    "wait_for_server(\"http://localhost:30222\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the server is running, you’ll be ready to interact with it using a client, as described in the subsequent steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools for Function Call\n",
    "This example introduces a `get_current_weather` tool to fetch weather information. You can define any tool in a similar structure, according to your requirements.\n",
    "\n",
    "Below is a Python snippet that shows how to define a tool as a dictionary. The dictionary includes:\n",
    "\n",
    "-   **Tool name**\n",
    "-   **Description**\n",
    "-   **Parameters** (with property definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Messages\n",
    "\n",
    "Set up an initial conversation history to simulate how a user interacts with the assistant. This helps the model understand the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston today? Please respond with the format: Today's weather is :{function call result}\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Client\n",
    "\n",
    "Use an OpenAI-like client to communicate with the SGLang server. Replace `YOUR_API_KEY` with the appropriate key, or leave it as a dummy value if you’re testing locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-08 05:42:36] INFO:     127.0.0.1:52192 - \"GET /v1/models HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI-like client\n",
    "client = OpenAI(api_key=\"YOUR_API_KEY\", base_url=\"http://0.0.0.0:30222/v1\")\n",
    "model_name = client.models.list().data[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Non-Streaming Request\n",
    "By default, you can send requests to the server without streaming the results back. This returns a single, complete response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-08 05:42:36 TP0] Prefill batch. #new-seq: 1, #new-token: 235, #cached-token: 1, cache hit rate: 0.41%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-01-08 05:42:36] INFO:     127.0.0.1:52192 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "Non-stream response:\n",
      "ChatCompletion(id='6606168ebd2b4a8da599ba84577b196f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='0', function=Function(arguments='{\"location\": \"Boston, MA\", \"unit\": \"fahrenheit\"}', name='get_current_weather'), type='function')]), matched_stop=128008)], created=1736314956, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=28, prompt_tokens=236, total_tokens=264, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "# Non-streaming mode test\n",
    "response_non_stream = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=0.8,\n",
    "    top_p=0.8,\n",
    "    stream=False,  # Non-streaming\n",
    "    tools=tools,\n",
    ")\n",
    "print(\"Non-stream response:\")\n",
    "print(response_non_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Streaming Request\n",
    "If you prefer receiving real-time responses or incremental updates, you can use streaming mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "[2025-01-08 05:42:36] INFO:     127.0.0.1:52192 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-01-08 05:42:36 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 235, cache hit rate: 49.27%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "ChatCompletionChunk(id='3494adc4dafe4c8ea00d1bf87ec12d83', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason='', index=0, logprobs=None, matched_stop=None)], created=1736314956, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "starting on new tool %d 0\n",
      "[2025-01-08 05:42:36 TP0] Decode batch. #running-req: 1, #token: 241, token usage: 0.00, gen throughput (token/s): 6.94, #queue-req: 0\n",
      "ChatCompletionChunk(id='3494adc4dafe4c8ea00d1bf87ec12d83', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_weather'), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1736314956, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3494adc4dafe4c8ea00d1bf87ec12d83', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='{\"unit\": \"', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1736314956, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3494adc4dafe4c8ea00d1bf87ec12d83', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='f', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1736314956, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3494adc4dafe4c8ea00d1bf87ec12d83', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='ahrenheit\"', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1736314956, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3494adc4dafe4c8ea00d1bf87ec12d83', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments=', \"location\": \"', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1736314956, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3494adc4dafe4c8ea00d1bf87ec12d83', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='Boston', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1736314956, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3494adc4dafe4c8ea00d1bf87ec12d83', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments=',', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1736314956, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3494adc4dafe4c8ea00d1bf87ec12d83', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments=' MA\"}', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1736314956, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "ChatCompletionChunk(id='3494adc4dafe4c8ea00d1bf87ec12d83', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=None, id='0', function=ChoiceDeltaToolCallFunction(arguments='', name=''), type='function')]), finish_reason='tool_call', index=0, logprobs=None, matched_stop=None)], created=1736314956, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n"
     ]
    }
   ],
   "source": [
    "# Streaming mode test\n",
    "print(\"Streaming response:\")\n",
    "response_stream = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=0.8,\n",
    "    top_p=0.8,\n",
    "    stream=True,  # Enable streaming\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for chunk in response_stream:\n",
    "    chunks.append(chunk)\n",
    "    print(chunk)  # Observe streaming output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `chunks` list accumulates all response segments, which might be needed later to handle tool calls.\n",
    "\n",
    "----------\n",
    "\n",
    "## Handle Tool Calls\n",
    "\n",
    "When the server determines it should call a particular tool, it will return arguments or partial arguments through the streaming response. You can parse these arguments and later invoke the tool accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamed function call name: get_current_weather\n",
      "Streamed function call arguments: {\"unit\": \"\n",
      "Streamed function call arguments: f\n",
      "Streamed function call arguments: ahrenheit\"\n",
      "Streamed function call arguments: , \"location\": \"\n",
      "Streamed function call arguments: Boston\n",
      "Streamed function call arguments: ,\n",
      "Streamed function call arguments:  MA\"}\n",
      "Final streamed function call arguments: {\"unit\": \"fahrenheit\", \"location\": \"Boston, MA\"}\n"
     ]
    }
   ],
   "source": [
    "# Parse and combine function call arguments\n",
    "arguments = []\n",
    "for chunk in chunks:\n",
    "    choice = chunk.choices[0]\n",
    "    delta = choice.delta\n",
    "    if delta.tool_calls:\n",
    "        tool_call = delta.tool_calls[0]\n",
    "        if tool_call.function.name:\n",
    "            print(f\"Streamed function call name: {tool_call.function.name}\")\n",
    "\n",
    "        if tool_call.function.arguments:\n",
    "            arguments.append(tool_call.function.arguments)\n",
    "            print(f\"Streamed function call arguments: {tool_call.function.arguments}\")\n",
    "\n",
    "# Combine all fragments into a single JSON string\n",
    "full_arguments = \"\".join(arguments)\n",
    "print(f\"Final streamed function call arguments: {full_arguments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a Tool Function\n",
    "\n",
    "Next, define the actual function that implements the tool’s logic. In this example, `get_current_weather` simply returns a hard-coded weather description for Dallas, Texas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actual function for getting current weather\n",
    "def get_current_weather(location: str, unit: str):\n",
    "    # Here you can integrate an actual weather API\n",
    "    return f\"The weather in {location} is 85 degrees {unit}. It is partly cloudy, with highs in the 90's.\"\n",
    "\n",
    "\n",
    "# Simulate tool call\n",
    "available_tools = {\"get_current_weather\": get_current_weather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Execute the Tool\n",
    "\n",
    "Finally, parse the JSON arguments and call the relevant tool if it exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call result: The weather in Boston, MA is 85 degrees fahrenheit. It is partly cloudy, with highs in the 90's.\n",
      "Updated message history: [{'role': 'user', 'content': \"What's the weather like in Boston today? Please respond with the format: Today's weather is :{function call result}\"}, {'role': 'user', 'content': '', 'tool_calls': {'name': 'get_current_weather', 'arguments': '{\"unit\": \"fahrenheit\", \"location\": \"Boston, MA\"}'}}, {'role': 'tool', 'content': \"The weather in Boston, MA is 85 degrees fahrenheit. It is partly cloudy, with highs in the 90's.\", 'name': 'get_current_weather'}]\n"
     ]
    }
   ],
   "source": [
    "# Parse JSON arguments\n",
    "try:\n",
    "    call_data = json.loads(full_arguments)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON decoding error: {e}\")\n",
    "    call_data = {}\n",
    "\n",
    "# Add user message and function call result to the message list\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\",\n",
    "        \"tool_calls\": {\"name\": \"get_current_weather\", \"arguments\": full_arguments},\n",
    "    }\n",
    ")\n",
    "\n",
    "# Call the corresponding tool function\n",
    "if \"tool_calls\" in messages[-1] and \"name\" in messages[-1][\"tool_calls\"]:\n",
    "    tool_name = messages[-1][\"tool_calls\"][\"name\"]\n",
    "    if tool_name in available_tools:\n",
    "        tool_to_call = available_tools[tool_name]\n",
    "        result = tool_to_call(**call_data)\n",
    "        print(f\"Function call result: {result}\")\n",
    "        messages.append({\"role\": \"tool\", \"content\": result, \"name\": tool_name})\n",
    "    else:\n",
    "        print(f\"Unknown tool name: {tool_name}\")\n",
    "else:\n",
    "    print(\"Function call name not found.\")\n",
    "\n",
    "print(f\"Updated message history: {messages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate Server\n",
    "Once you are done, you can terminate the server process using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
