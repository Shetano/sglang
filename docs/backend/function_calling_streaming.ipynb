{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Function Calling Streaming Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the Server\n",
    "To begin, you need to launch a server that will handle the requests. The following code block is used to launch the server.\n",
    "After the server is running, you can interact with it using the client to get the weather information.\n",
    "\n",
    "```bash\n",
    "python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --port 30000 --host 0.0.0.0\n",
    "```\n",
    "This starts the server on port `30000`. Once running, you can make requests to it from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/misc/yangmin/miniconda3/envs/sglang_fork/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/misc/yangmin/miniconda3/envs/sglang_fork/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/misc/yangmin/fork/sglang/python/sglang/launch_server.py\", line 6, in <module>\n",
      "    from sglang.srt.server import launch_server\n",
      "  File \"/home/misc/yangmin/fork/sglang/python/sglang/srt/server.py\", line 65, in <module>\n",
      "    from sglang.srt.openai_api.adapter import (\n",
      "  File \"/home/misc/yangmin/fork/sglang/python/sglang/srt/openai_api/adapter.py\", line 76, in <module>\n",
      "    from sglang.srt.function_call_parser import FunctionCallParser\n",
      "ModuleNotFoundError: No module named 'sglang.srt.function_call_parser'\n"
     ]
    }
   ],
   "source": [
    "from sglang.utils import execute_shell_command, wait_for_server\n",
    "\n",
    "server_process = execute_shell_command(\n",
    "    \"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --port 30000 --host 0.0.0.0\"\n",
    ")\n",
    "wait_for_server(\"http://localhost:30000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools for Function Call\n",
    "Next, we'll define the tools for our function call. In this example, we define a function for getting the current weather in a specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston today? Please respond with the format: Today's weather is :{function call result}\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize OpenAI-like client\n",
    "client = OpenAI(api_key=\"YOUR_API_KEY\", base_url=\"http://0.0.0.0:30000/v1\")\n",
    "model_name = client.models.list().data[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Non-Streaming Request\n",
    "We'll now test the non-streaming function call and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-streaming mode test\n",
    "response_non_stream = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=0.8,\n",
    "    top_p=0.8,\n",
    "    stream=False,  # Non-streaming\n",
    "    tools=tools,\n",
    ")\n",
    "print(\"Non-stream response:\")\n",
    "print(response_non_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Streaming Request\n",
    "Next, we will test the streaming function call. The response will come in chunks, and we will handle the stream and process the function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming mode test\n",
    "print(\"Streaming response:\")\n",
    "response_stream = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=0.8,\n",
    "    top_p=0.8,\n",
    "    stream=True,  # Enable streaming\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "# Handle streaming responses, combine different chunks\n",
    "chunks = []\n",
    "for chunk in response_stream:\n",
    "    chunks.append(chunk)\n",
    "    print(chunk)  # Optionally print each chunk to observe its content\n",
    "\n",
    "# Parse and combine function call arguments\n",
    "arguments = []\n",
    "for chunk in chunks:\n",
    "    choice = chunk.choices[0]\n",
    "    delta = choice.delta\n",
    "    if delta.tool_calls:\n",
    "        tool_call = delta.tool_calls[0]\n",
    "        if tool_call.function.name:\n",
    "            print(f\"Streamed function call name: {tool_call.function.name}\")\n",
    "\n",
    "        if tool_call.function.arguments:\n",
    "            arguments.append(tool_call.function.arguments)\n",
    "            print(f\"Streamed function call arguments: {tool_call.function.arguments}\")\n",
    "\n",
    "# Combine all argument fragments\n",
    "full_arguments = \"\".join(arguments)\n",
    "print(f\"Final streamed function call arguments: {full_arguments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Tool Call\n",
    "In this section, we'll simulate a call to the weather function and handle the arguments passed from the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actual function for getting current weather\n",
    "def get_current_weather(location: str, unit: str):\n",
    "    # Here you can integrate an actual weather API\n",
    "    return f\"The weather in {location} is 85 degrees {unit}. It is partly cloudy, with highs in the 90's.\"\n",
    "\n",
    "\n",
    "# Simulate tool call\n",
    "available_tools = {\"get_current_weather\": get_current_weather}\n",
    "\n",
    "# Parse JSON arguments\n",
    "try:\n",
    "    call_data = json.loads(full_arguments)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON decoding error: {e}\")\n",
    "    call_data = {}\n",
    "\n",
    "# Call the corresponding tool function\n",
    "if \"name\" in messages[-1][\"tool_calls\"]:\n",
    "    tool_name = messages[-1][\"tool_calls\"][\"name\"]\n",
    "    if tool_name in available_tools:\n",
    "        tool_to_call = available_tools[tool_name]\n",
    "        result = tool_to_call(**call_data)\n",
    "        print(f\"Function call result: {result}\")\n",
    "        messages.append({\"role\": \"tool\", \"content\": result, \"name\": tool_name})\n",
    "    else:\n",
    "        print(f\"Unknown tool name: {tool_name}\")\n",
    "else:\n",
    "    print(\"Function call name not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Chat Completion\n",
    "We will now perform the final chat completion using the simulated function call result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion_final = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=0.8,\n",
    "    top_p=0.8,\n",
    "    stream=False,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Chat Completion:\")\n",
    "print(chat_completion_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate Server\n",
    "Once you are done, you can terminate the server process using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
