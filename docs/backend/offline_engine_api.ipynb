{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Engine API\n",
    "\n",
    "SGLang provides a direct inference engine without the need for an HTTP server, especially for use cases where additional HTTP server adds unnecessary complexity or overhead. Here are two general use cases:\n",
    "\n",
    "- Offline Batch Inference\n",
    "- Custom Server on Top of the Engine\n",
    "\n",
    "This document focuses on the offline batch inference, demonstrating four different inference modes:\n",
    "\n",
    "- Non-streaming synchronous generation\n",
    "- Streaming synchronous generation\n",
    "- Non-streaming asynchronous generation\n",
    "- Streaming asynchronous generation\n",
    "\n",
    "Additionally, you can easily build a custom server on top of the SGLang offline engine. A detailed example working in a python script can be found in [custom_server](https://github.com/sgl-project/sglang/blob/main/examples/runtime/engine/custom_server.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Batch Inference\n",
    "\n",
    "SGLang offline engine supports batch inference with efficient scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenyang/miniconda3/envs/sgl-zbz/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-26 19:57:24,783\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.73it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.87it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.94it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.67it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.31it/s]\n",
      "\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# launch the offline engine\n",
    "\n",
    "import sglang as sgl\n",
    "import asyncio\n",
    "\n",
    "llm = sgl.Engine(model_path=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Prompt: Hello, my name is\n",
      "Generated text:  Dr. Jeannette Gaudry Haynie, and I am a psychologist who has dedicated my professional life to helping individuals achieve greater self-awareness and personal growth.\n",
      "As a licensed psychologist with over 20 years of experience, I have worked with individuals from all walks of life, including those struggling with anxiety, depression, trauma, relationship issues, and career dissatisfaction.\n",
      "My approach is grounded in evidence-based practices, including cognitive-behavioral therapy (CBT), psychodynamic therapy, and mindfulness-based interventions. I also incorporate elements of existential and humanistic psychology, recognizing that each individual's unique experiences, values, and perspectives shape their\n",
      "===============================\n",
      "Prompt: The president of the United States is\n",
      "Generated text:  not just the leader of the country, but also the symbolic head of the military. The president is responsible for the defense of the nation and is the commander-in-chief of the armed forces. The president plays a crucial role in determining the country's foreign policy, including its military actions, defense strategies, and international relations.\n",
      "The president's powers regarding the military are granted by the U.S. Constitution, which states that the president \"shall be Commander in Chief of the Army and Navy of the United States, and of the Militia of the several States, when called into the actual Service of the United States.\" This means that the president has\n",
      "===============================\n",
      "Prompt: The capital of France is\n",
      "Generated text:  a city of romance and beauty, with landmarks like the Eiffel Tower, Notre-Dame Cathedral and the Louvre Museum. Paris is also famous for its fashion, cuisine and art, and is home to many world-class museums and galleries. Visitors can stroll along the Seine River, explore the charming streets of Montmartre or visit the Palace of Versailles, a former royal residence with stunning gardens and fountains.\n",
      "From romantic river cruises to lively street performers and historic museums, Paris has something for everyone. Whether you're interested in history, art, fashion, food or simply soaking up the city's charming atmosphere, Paris is\n",
      "===============================\n",
      "Prompt: The future of AI is\n",
      "Generated text:  here and now, and it's transforming the way businesses operate, interact with customers, and create value. Artificial Intelligence (AI) is a broad field of study that involves creating intelligent machines capable of performing tasks that typically require human intelligence, such as:\n",
      "Understanding natural language\n",
      "Learning from data\n",
      "Making decisions\n",
      "Recognizing patterns\n",
      "AI has many applications, including:\n",
      "Virtual assistants (e.g., Siri, Alexa, Google Assistant)\n",
      "Chatbots and customer service automation\n",
      "Predictive maintenance and quality control\n",
      "Image and speech recognition\n",
      "Recommendation systems (e.g., Netflix, Amazon)\n",
      "Healthcare and medical diagnosis\n",
      "Financial analysis and portfolio management\n",
      "Transport\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(\"===============================\")\n",
    "    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Synchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing synchronous streaming generation with overlap removal ===\n",
      "\n",
      "Prompt: Write a short, neutral self-introduction for a fictional character. Hello, my name is\n",
      "Generated text:  Kaida. I'm a 17-year-old high school student. I live in a small town in the Pacific Northwest with my family. I enjoy hiking and reading in my free time. I'm a bit of a introvert, but I'm working on being more outgoing. That's me in a nutshell. What do you think? Is it a good self-introduction?\n",
      "This is a good self-introduction because it:\n",
      "* Introduces the character's name and age\n",
      "* Provides some background information about the character's life\n",
      "* Reveals some of the character's interests and personality traits\n",
      "* Is concise and easy to read\n",
      "\n",
      "Prompt: Provide a concise factual statement about France’s capital city. The capital of France is\n",
      "Generated text:  Paris. Paris is located in the northern part of the country, near the Seine River. It is the largest city in France and is known for its rich history, art, fashion, and cuisine. Paris is home to many famous landmarks such as the Eiffel Tower, Notre Dame Cathedral, and the Louvre Museum. The city is also a major center for business, education, and culture. Paris is a popular tourist destination and is known for its romantic atmosphere and beautiful architecture. The city has a population of over 2.1 million people and is a major hub for transportation, with two international airports and a comprehensive public transportation\n",
      "\n",
      "Prompt: Explain possible future trends in artificial intelligence. The future of AI is\n",
      "Generated text:  likely to be shaped by several factors, including advancements in machine learning, natural language processing, and computer vision. Here are some possible future trends in AI:\n",
      "1. Increased use of AI in healthcare: AI is likely to play a larger role in healthcare, including diagnosis, treatment, and patient care. AI-powered systems may be able to analyze medical images, identify patterns in patient data, and provide personalized treatment recommendations.\n",
      "2. Widespread adoption of AI in customer service: AI-powered chatbots and virtual assistants are likely to become more common in customer service, helping to answer customer queries, resolve issues, and provide personalized support.\n",
      "3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_overlap(existing_text, new_chunk):\n",
    "    \"\"\"\n",
    "    Finds the largest suffix of 'existing_text' that is a prefix of 'new_chunk'\n",
    "    and removes that overlap from the start of 'new_chunk'.\n",
    "    \"\"\"\n",
    "    max_overlap = 0\n",
    "    max_possible = min(len(existing_text), len(new_chunk))\n",
    "\n",
    "    for i in range(max_possible, 0, -1):\n",
    "        if existing_text.endswith(new_chunk[:i]):\n",
    "            max_overlap = i\n",
    "            break\n",
    "\n",
    "    return new_chunk[max_overlap:]\n",
    "\n",
    "def generate_text_no_repeats(llm, prompt, sampling_params):\n",
    "    \"\"\"\n",
    "    Example function that:\n",
    "    1) Streams the text,\n",
    "    2) Removes chunk overlaps,\n",
    "    3) Returns the merged text.\n",
    "    \"\"\"\n",
    "    final_text = \"\"\n",
    "    for chunk in llm.generate(prompt, sampling_params, stream=True):\n",
    "        chunk_text = chunk[\"text\"]\n",
    "\n",
    "        cleaned_chunk = remove_overlap(final_text, chunk_text)\n",
    "\n",
    "        final_text += cleaned_chunk\n",
    "\n",
    "    return final_text\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Write a short, neutral self-introduction for a fictional character. Hello, my name is\",\n",
    "    \"Provide a concise factual statement about France’s capital city. The capital of France is\",\n",
    "    \"Explain possible future trends in artificial intelligence. The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "print(\"\\n=== Testing synchronous streaming generation with overlap removal ===\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    merged_output = generate_text_no_repeats(llm, prompt, sampling_params)\n",
    "    print(\"Generated text:\", merged_output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous batch generation ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    outputs = await llm.async_generate(prompts, sampling_params)\n",
    "\n",
    "    for prompt, output in zip(prompts, outputs):\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated text: {output['text']}\")\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Asynchronous Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n",
    "\n",
    "print(\"\\n=== Testing asynchronous streaming generation ===\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(\"Generated text: \", end=\"\", flush=True)\n",
    "\n",
    "        generator = await llm.async_generate(prompt, sampling_params, stream=True)\n",
    "        async for chunk in generator:\n",
    "            print(chunk[\"text\"], end=\"\", flush=True)\n",
    "        print()\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgl-zbz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
