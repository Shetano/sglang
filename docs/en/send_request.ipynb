{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch a server\n",
    "\n",
    "This code uses `subprocess.Popen` to start an SGLang server process, equivalent to executing \n",
    "\n",
    "```bash\n",
    "python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "--port 30000 --host 0.0.0.0 --log-level warning\n",
    "```\n",
    "in your command line and wait for the server to be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is ready. Proceeding with the next steps.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "server_process = subprocess.Popen(\n",
    "    [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"sglang.launch_server\",\n",
    "        \"--model-path\",\n",
    "        \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        \"--port\",\n",
    "        \"30000\",\n",
    "        \"--host\",\n",
    "        \"0.0.0.0\",\n",
    "        \"--log-level\",\n",
    "        \"error\",\n",
    "    ],\n",
    "    text=True,\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            \"http://localhost:30000/v1/models\",\n",
    "            headers={\"Authorization\": \"Bearer None\"},\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            break\n",
    "    except requests.exceptions.RequestException:\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"Server is ready. Proceeding with the next steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a Request\n",
    "\n",
    "Once the server is running, you can send test requests using curl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"d85b69cf1f274f628c3288478ff6d5cf\",\"object\":\"chat.completion\",\"created\":1729811858,\"model\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"LLM stands for Large Language Model. It's a type of artificial intelligence (AI) designed to process and generate human-like language. LLMs are trained on vast amounts of text data, which allows them to learn patterns, relationships, and structures of language.\\n\\nLarge language models are typically characterized by:\\n\\n1. **Training data:** They're trained on massive datasets, often billions of words or more, which enables them to understand a wide range of language styles, genres, and domains.\\n2. **Model size:** They consist of complex neural networks with millions or even billions of parameters, allowing them to capture subtle nuances of language.\\n3. **Generative capabilities:** LLMs can generate text based on input prompts, which can range from simple sentences to longer passages or even entire articles.\\n\\nSome common applications of LLMs include:\\n\\n1. **Language translation:** LLMs can translate text from one language to another with remarkable accuracy.\\n2. **Text summarization:** They can condense long pieces of text into concise summaries.\\n3. **Content generation:** LLMs can create original content, such as articles, stories, or even entire books.\\n4. **Chatbots and virtual assistants:** They power conversational interfaces, like myself!\\n5. **Text analysis:** LLMs can analyze text to extract insights, sentiments, and emotions.\\n\\nSome popular examples of LLMs include:\\n\\n1. **BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google, it's a pioneering LLM that has been widely used in natural language processing tasks.\\n2. **RoBERTa (Robustly Optimized BERT Pretraining Approach):** It's an improved version of BERT, also developed by Google.\\n3. **Chatbots like myself:** I'm a large language model, trained to provide helpful and informative responses to a wide range of questions and topics!\\n\\nKeep in mind that LLMs are not perfect and can be limited by their training data, biases, and lack of common sense. However, they have the potential to revolutionize the way we interact with language and information.\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":128009}],\"usage\":{\"prompt_tokens\":47,\"total_tokens\":473,\"completion_tokens\":426,\"prompt_tokens_details\":null}}"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:30000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer None\" \\\n",
    "  -d '{\"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is a LLM?\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Compatible API\n",
    "\n",
    "SGLang supports OpenAI-compatible APIs. Here are Python examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='53a386a5b50942f58637488a6f965d79', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are 3 countries and their capitals:\\n\\n1. **Country:** Japan\\n**Capital:** Tokyo\\n\\n2. **Country:** Australia\\n**Capital:** Canberra\\n\\n3. **Country:** Brazil\\n**Capital:** Bras√≠lia', refusal=None, role='assistant', function_call=None, tool_calls=None), matched_stop=128009)], created=1729811859, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=46, prompt_tokens=49, total_tokens=95, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Always assign an api_key, even if not specified during server initialization.\n",
    "# Setting an API key during server initialization is strongly recommended.\n",
    "\n",
    "client = openai.Client(\n",
    "    base_url=\"http://127.0.0.1:30000/v1\", api_key=\"None\"\n",
    ")\n",
    "\n",
    "# Chat completion example\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def terminate_process(process):\n",
    "    try:\n",
    "        process.terminate()\n",
    "        try:\n",
    "            process.wait(timeout=5)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            if os.name != 'nt':\n",
    "                try:\n",
    "                    pgid = os.getpgid(process.pid)\n",
    "                    os.killpg(pgid, signal.SIGTERM)\n",
    "                    time.sleep(1)\n",
    "                    if process.poll() is None:\n",
    "                        os.killpg(pgid, signal.SIGKILL)\n",
    "                except ProcessLookupError:\n",
    "                    pass\n",
    "            else:\n",
    "                process.kill()\n",
    "            process.wait()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {e}\")\n",
    "    finally:\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "terminate_process(server_process)\n",
    "time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlphaMeemory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
