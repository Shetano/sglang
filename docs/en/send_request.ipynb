{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch a server\n",
    "\n",
    "This code uses `subprocess.Popen` to start an SGLang server process, equivalent to executing \n",
    "\n",
    "```bash\n",
    "python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "--port 30000 --host 0.0.0.0 --log-level warning\n",
    "```\n",
    "in your command line and wait for the server to be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is ready. Proceeding with the next steps.\n"
     ]
    }
   ],
   "source": [
    "# import subprocess\n",
    "# import sys\n",
    "# import time\n",
    "# import requests\n",
    "\n",
    "# server_process = subprocess.Popen(\n",
    "#     [\n",
    "#         \"python\",\n",
    "#         \"-m\",\n",
    "#         \"sglang.launch_server\",\n",
    "#         \"--model-path\",\n",
    "#         \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "#         \"--port\",\n",
    "#         \"30000\",\n",
    "#         \"--host\",\n",
    "#         \"0.0.0.0\",\n",
    "#         \"--log-level\",\n",
    "#         \"error\",\n",
    "#     ],\n",
    "#     text=True,\n",
    "#     stdout=subprocess.DEVNULL,\n",
    "#     stderr=subprocess.DEVNULL,\n",
    "# )\n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         response = requests.get(\n",
    "#             \"http://localhost:30000/v1/models\",\n",
    "#             headers={\"Authorization\": \"Bearer None\"},\n",
    "#         )\n",
    "#         if response.status_code == 200:\n",
    "#             break\n",
    "#     except requests.exceptions.RequestException:\n",
    "#         time.sleep(1)\n",
    "\n",
    "# print(\"Server is ready. Proceeding with the next steps.\")\n",
    "print(\"test config notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a Request\n",
    "\n",
    "Once the server is running, you can send test requests using curl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"05076bfa83a24c4eae680762b947cf07\",\"object\":\"chat.completion\",\"created\":1729793995,\"model\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"LLM stands for Large Language Model. It's a type of artificial intelligence (AI) model that's designed to process and generate human-like language. These models are trained on massive amounts of text data, which allows them to learn patterns, relationships, and structures of language.\\n\\nA Large Language Model like myself uses this training to generate text based on a given prompt or input. I can respond to questions, engage in conversations, and even create content such as stories, poems, or articles.\\n\\nSome of the key features of LLMs like myself include:\\n\\n1. **Language understanding**: I can comprehend natural language input and respond accordingly.\\n2. **Language generation**: I can create text based on a given prompt or topic.\\n3. **Contextual understanding**: I can understand the context of a conversation and respond accordingly.\\n4. **Knowledge retrieval**: I can retrieve information from my training data and provide it in response to a question.\\n\\nLLMs like myself are being used in a wide range of applications, including:\\n\\n1. **Virtual assistants**: I can assist with tasks, answer questions, and provide information.\\n2. **Content generation**: I can create content such as articles, stories, and social media posts.\\n3. **Language translation**: I can translate text from one language to another.\\n4. **Chatbots**: I can engage in conversations with users and provide customer support.\\n\\nOverall, Large Language Models like myself are powerful tools that can help with a wide range of tasks and applications.\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"matched_stop\":128009}],\"usage\":{\"prompt_tokens\":47,\"total_tokens\":347,\"completion_tokens\":300,\"prompt_tokens_details\":null}}"
     ]
    }
   ],
   "source": [
    "# !curl http://localhost:30000/v1/chat/completions \\\n",
    "#   -H \"Content-Type: application/json\" \\\n",
    "#   -H \"Authorization: Bearer None\" \\\n",
    "#   -d '{\"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is a LLM?\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Compatible API\n",
    "\n",
    "SGLang supports OpenAI-compatible APIs. Here are Python examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='f2436ca47f404a2499a69a64115a8801', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are 3 countries and their capitals:\\n\\n1. **Country:** Japan\\n**Capital:** Tokyo\\n\\n2. **Country:** Australia\\n**Capital:** Canberra\\n\\n3. **Country:** Brazil\\n**Capital:** Bras√≠lia', refusal=None, role='assistant', function_call=None, tool_calls=None), matched_stop=128009)], created=1729793996, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=46, prompt_tokens=49, total_tokens=95, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "# import openai\n",
    "\n",
    "# # Always assign an api_key, even if not specified during server initialization.\n",
    "# # Setting an API key during server initialization is strongly recommended.\n",
    "\n",
    "# client = openai.Client(\n",
    "#     base_url=\"http://127.0.0.1:30000/v1\", api_key=\"None\"\n",
    "# )\n",
    "\n",
    "# # Chat completion example\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "#         {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "#     ],\n",
    "#     temperature=0,\n",
    "#     max_tokens=64,\n",
    "# )\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# server_process.terminate()\n",
    "# server_process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Embedding Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding server is ready. Proceeding with the next steps.\n"
     ]
    }
   ],
   "source": [
    "# # Start a new server process for embedding models\n",
    "# # Equivalent to running this in the shell:\n",
    "# # python -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-7B-instruct --port 30010 --host 0.0.0.0 --is-embedding --log-level error\n",
    "# embedding_process = subprocess.Popen(\n",
    "#     [\n",
    "#         \"python\",\n",
    "#         \"-m\",\n",
    "#         \"sglang.launch_server\",\n",
    "#         \"--model-path\",\n",
    "#         \"Alibaba-NLP/gte-Qwen2-7B-instruct\",\n",
    "#         \"--port\",\n",
    "#         \"30010\",\n",
    "#         \"--host\",\n",
    "#         \"0.0.0.0\",\n",
    "#         \"--is-embedding\",\n",
    "#         \"--log-level\",\n",
    "#         \"error\",\n",
    "#     ],\n",
    "#     text=True,\n",
    "#     stdout=subprocess.DEVNULL,\n",
    "#     stderr=subprocess.DEVNULL,\n",
    "# )\n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         response = requests.get(\n",
    "#             \"http://localhost:30010/v1/models\",\n",
    "#             headers={\"Authorization\": \"Bearer None\"},\n",
    "#         )\n",
    "#         if response.status_code == 200:\n",
    "#             break\n",
    "#     except requests.exceptions.RequestException:\n",
    "#         time.sleep(1)\n",
    "\n",
    "# print(\"Embedding server is ready. Proceeding with the next steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0083160400390625, 0.0006804466247558594, -0.00809478759765625, -0.0006995201110839844, 0.0143890380859375, -0.0090179443359375, 0.01238250732421875, 0.00209808349609375, 0.0062103271484375, -0.003047943115234375]\n"
     ]
    }
   ],
   "source": [
    "# # Get the first 10 elements of the embedding\n",
    "\n",
    "# ! curl -s http://localhost:30010/v1/embeddings \\\n",
    "#   -H \"Content-Type: application/json\" \\\n",
    "#   -H \"Authorization: Bearer None\" \\\n",
    "#   -d '{\"model\": \"Alibaba-NLP/gte-Qwen2-7B-instruct\", \"input\": \"Once upon a time\"}' \\\n",
    "#   | python3 -c \"import sys, json; print(json.load(sys.stdin)['data'][0]['embedding'][:10])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI Compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00603485107421875, -0.0190582275390625, -0.01273345947265625, 0.01552581787109375, 0.0066680908203125, -0.0135955810546875, 0.01131439208984375, 0.0013713836669921875, -0.0089874267578125, 0.021759033203125]\n"
     ]
    }
   ],
   "source": [
    "# import openai\n",
    "\n",
    "# client = openai.Client(\n",
    "#     base_url=\"http://127.0.0.1:30010/v1\", api_key=\"None\"\n",
    "# )\n",
    "\n",
    "# # Text embedding example\n",
    "# response = client.embeddings.create(\n",
    "#     model=\"Alibaba-NLP/gte-Qwen2-7B-instruct\",\n",
    "#     input=\"How are you today\",\n",
    "# )\n",
    "\n",
    "# embedding = response.data[0].embedding[:10]\n",
    "# print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_process.terminate()\n",
    "# embedding_process.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlphaMeemory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
