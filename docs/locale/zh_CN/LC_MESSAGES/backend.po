# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2024, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.3.1.post3\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-09-25 22:11+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"Generated-By: Babel 2.16.0\n"

#: ../../backend.md:1
msgid "Backend: SGLang Runtime (SRT)"
msgstr "后端：SGLang 运行时 (SRT)"

#: ../../backend.md:2
msgid "The SGLang Runtime (SRT) is an efficient serving engine."
msgstr "SGLang 运行时 (SRT) 是一款高效的服务引擎。"

#: ../../backend.md:4
msgid "Quick Start"
msgstr "快速入门"

#: ../../backend.md:5
msgid "Launch a server"
msgstr "启动服务器"

#: ../../backend.md:10
msgid "Send a request"
msgstr "发送请求"

#: ../../backend.md:22
msgid ""
"Learn more about the argument format "
"[here](https://sglang.readthedocs.io/en/latest/sampling_params.html)."
msgstr ""
"了解更多关于参数格式的信息 "
"[此处](https://sglang.readthedocs.io/en/latest/sampling_params.html)。"

#: ../../backend.md:24
msgid "OpenAI Compatible API"
msgstr "与 OpenAI 兼容的 API"

#: ../../backend.md:25
msgid "In addition, the server supports OpenAI-compatible APIs."
msgstr "此外，服务器支持与 OpenAI 兼容的 API。"

#: ../../backend.md:61
msgid ""
"It supports streaming, vision, and most features of the "
"Chat/Completions/Models/Batch endpoints specified by the [OpenAI API "
"Reference](https://platform.openai.com/docs/api-reference/)."
msgstr ""
"它支持流式传输、视觉以及 [OpenAI API 参考](https://platform.openai.com/docs/api-"
"reference/) 中指定的 Chat/Completions/Models/Batch 端点的多数功能。"

#: ../../backend.md:63
msgid "Additional Server Arguments"
msgstr "其他服务器参数"

#: ../../backend.md:64
msgid ""
"To enable multi-GPU tensor parallelism, add `--tp 2`. If it reports the "
"error \"peer access is not supported between these two devices\", add "
"`--enable-p2p-check` to the server launch command."
msgstr ""
"要启用多 GPU 张量并行，请添加 `--tp 2`。如果它报告错误“这些设备之间不支持对等访问”，请在服务器启动命令中添加 "
"`--enable-p2p-check`。"

#: ../../backend.md:68
msgid ""
"To enable multi-GPU data parallelism, add `--dp 2`. Data parallelism is "
"better for throughput if there is enough memory. It can also be used "
"together with tensor parallelism. The following command uses 4 GPUs in "
"total."
msgstr ""
"要启用多 GPU 数据并行，请添加 `--dp 2`。如果内存充足，数据并行更适合吞吐量。它也可以与张量并行一起使用。以下命令总共使用 4 个 GPU。"

#: ../../backend.md:72
msgid ""
"If you see out-of-memory errors during serving, try to reduce the memory "
"usage of the KV cache pool by setting a smaller value of `--mem-fraction-"
"static`. The default value is `0.9`."
msgstr ""
"如果在服务期间看到内存不足错误，请尝试通过设置较小的 `--mem-fraction-static` 值来减少 KV 缓存池的内存使用量。默认值为 "
"`0.9`。"

#: ../../backend.md:76
msgid ""
"See [hyperparameter "
"tuning](https://sglang.readthedocs.io/en/latest/hyperparameter_tuning.html) "
"on tuning hyperparameters for better performance."
msgstr ""
"有关调整超参数以获得更好性能的信息，请参阅 "
"[超参数调整](https://sglang.readthedocs.io/en/latest/hyperparameter_tuning.html)。"

#: ../../backend.md:77
msgid ""
"If you see out-of-memory errors during prefill for long prompts, try to set "
"a smaller chunked prefill size."
msgstr "如果在为长提示进行预填充时看到内存不足错误，请尝试设置较小的分块预填充大小。"

#: ../../backend.md:81
msgid ""
"To enable torch.compile acceleration, add `--enable-torch-compile`. It "
"accelerates small models on small batch sizes."
msgstr "要启用 torch.compile 加速，请添加 `--enable-torch-compile`。它加速了小批量大小的小模型。"

#: ../../backend.md:82
msgid ""
"To enable fp8 weight quantization, add `--quantization fp8` on a fp16 "
"checkpoint or directly load a fp8 checkpoint without specifying any "
"arguments."
msgstr ""
"要启用 fp8 权重量化，请在 fp16 检查点上添加 `--quantization fp8` 或直接加载 fp8 检查点，无需指定任何参数。"

#: ../../backend.md:83
msgid "To enable fp8 kv cache quantization, add `--kv-cache-dtype fp8_e5m2`."
msgstr "要启用 fp8 kv 缓存量化，请添加 `--kv-cache-dtype fp8_e5m2`。"

#: ../../backend.md:84
msgid ""
"If the model does not have a chat template in the Hugging Face tokenizer, "
"you can specify a [custom chat "
"template](https://sglang.readthedocs.io/en/latest/custom_chat_template.html)."
msgstr ""
"如果模型在 Hugging Face 分词器中没有聊天模板，你可以指定一个 "
"[自定义聊天模板](https://sglang.readthedocs.io/en/latest/custom_chat_template.html)。"

#: ../../backend.md:85
msgid ""
"To run tensor parallelism on multiple nodes, add `--nnodes 2`. If you have "
"two nodes with two GPUs on each node and want to run TP=4, let `sgl-dev-0` "
"be the hostname of the first node and `50000` be an available port."
msgstr ""
"在多个节点上运行张量并行，添加 `--nnodes 2`。如果你有两个节点，每个节点有两个 GPU，并且想要运行 TP=4，让 `sgl-dev-0` "
"成为第一个节点的主机名，`50000` 成为一个可用的端口。"

#: ../../backend.md:94
msgid "Supported Models"
msgstr "支持的模型"

#: ../../backend.md:96
msgid "**Generative Models**"
msgstr "**生成模型**"

#: ../../backend.md:97
msgid "Llama / Llama 2 / Llama 3 / Llama 3.1"
msgstr "Llama / Llama 2 / Llama 3 / Llama 3.1"

#: ../../backend.md:98
msgid "Mistral / Mixtral / Mistral NeMo"
msgstr "Mistral / Mixtral / Mistral NeMo"

#: ../../backend.md:99
msgid "Gemma / Gemma 2"
msgstr "Gemma / Gemma 2"

#: ../../backend.md:100
msgid "Qwen / Qwen 2 / Qwen 2 MoE"
msgstr "Qwen / Qwen 2 / Qwen 2 MoE"

#: ../../backend.md:101
msgid "DeepSeek / DeepSeek 2"
msgstr "DeepSeek / DeepSeek 2"

#: ../../backend.md:102
msgid ""
"[LLaVA-OneVision](https://llava-vl.github.io/blog/2024-08-05-llava-"
"onevision/)"
msgstr ""
"[LLaVA-OneVision](https://llava-vl.github.io/blog/2024-08-05-llava-"
"onevision/)"

#: ../../backend.md:103
msgid ""
"`python3 -m sglang.launch_server --model-path lmms-lab/llava-onevision-"
"qwen2-7b-ov --port=30000 --chat-template=chatml-llava`"
msgstr ""
"`python3 -m sglang.launch_server --model-path lmms-lab/llava-onevision-"
"qwen2-7b-ov --port=30000 --chat-template=chatml-llava`"

#: ../../backend.md:104
msgid ""
"`python3 -m sglang.launch_server --model-path lmms-lab/llava-onevision-"
"qwen2-72b-ov --port=30000 --tp-size=8 --chat-template=chatml-llava`"
msgstr ""
"`python3 -m sglang.launch_server --model-path lmms-lab/llava-onevision-"
"qwen2-72b-ov --port=30000 --tp-size=8 --chat-template=chatml-llava`"

#: ../../backend.md:105 ../../backend.md:109
msgid ""
"Query the server with the [OpenAI Vision "
"API](https://platform.openai.com/docs/guides/vision). See examples at "
"[test/srt/test_vision_openai_server.py](https://github.com/sgl-"
"project/sglang/blob/main/test/srt/test_vision_openai_server.py)"
msgstr ""
"使用 [OpenAI Vision API](https://platform.openai.com/docs/guides/vision) "
"查询服务器。示例请参考 [test/srt/test_vision_openai_server.py](https://github.com/sgl-"
"project/sglang/blob/main/test/srt/test_vision_openai_server.py)"

#: ../../backend.md:106
msgid "LLaVA 1.5 / 1.6 / NeXT"
msgstr "LLaVA 1.5 / 1.6 / NeXT"

#: ../../backend.md:107
msgid ""
"`python -m sglang.launch_server --model-path lmms-lab/llama3-llava-next-8b "
"--port=30000 --tp-size=1 --chat-template=llava_llama_3`"
msgstr ""
"`python -m sglang.launch_server --model-path lmms-lab/llama3-llava-next-8b "
"--port=30000 --tp-size=1 --chat-template=llava_llama_3`"

#: ../../backend.md:108
msgid ""
"`python -m sglang.launch_server --model-path lmms-lab/llava-next-72b "
"--port=30000 --tp-size=8 --chat-template=chatml-llava`"
msgstr ""
"`python -m sglang.launch_server --model-path lmms-lab/llava-next-72b "
"--port=30000 --tp-size=8 --chat-template=chatml-llava`"

#: ../../backend.md:110
msgid "Yi-VL"
msgstr "Yi-VL"

#: ../../backend.md:111
msgid "StableLM"
msgstr "StableLM"

#: ../../backend.md:112
msgid "Command-R"
msgstr "Command-R"

#: ../../backend.md:113
msgid "DBRX"
msgstr "DBRX"

#: ../../backend.md:114
msgid "Grok"
msgstr "Grok"

#: ../../backend.md:115
msgid "ChatGLM"
msgstr "ChatGLM"

#: ../../backend.md:116
msgid "InternLM 2"
msgstr "InternLM 2"

#: ../../backend.md:117
msgid "Exaone 3"
msgstr "Exaone 3"

#: ../../backend.md:119
msgid "**Embedding Models**"
msgstr "**Embedding Models**"

#: ../../backend.md:121
msgid "e5-mistral"
msgstr "e5-mistral"

#: ../../backend.md:122
msgid "gte-Qwen2"
msgstr "gte-Qwen2"

#: ../../backend.md:123
msgid ""
"`python -m sglang.launch_server --model-path Alibaba-NLP/gte-"
"Qwen2-7B-instruct --is-embedding`"
msgstr ""
"`python -m sglang.launch_server --model-path Alibaba-NLP/gte-"
"Qwen2-7B-instruct --is-embedding`"

#: ../../backend.md:125
msgid ""
"Instructions for supporting a new model are "
"[here](https://sglang.readthedocs.io/en/latest/model_support.html)."
msgstr ""
"支持新模型的说明请参考 "
"[此处](https://sglang.readthedocs.io/en/latest/model_support.html)。"

#: ../../backend.md:127
msgid "Use Models From ModelScope"
msgstr "使用 ModelScope 中的模型"

#: ../../backend.md:131
msgid ""
"To use a model from [ModelScope](https://www.modelscope.cn), set the "
"environment variable SGLANG_USE_MODELSCOPE."
msgstr ""
"要使用 [ModelScope](https://www.modelscope.cn) 中的模型，请设置环境变量 "
"SGLANG_USE_MODELSCOPE。"

#: ../../backend.md:135
msgid ""
"Launch "
"[Qwen2-7B-Instruct](https://www.modelscope.cn/models/qwen/qwen2-7b-instruct)"
" Server"
msgstr ""
"启动 "
"[Qwen2-7B-Instruct](https://www.modelscope.cn/models/qwen/qwen2-7b-instruct)"
" 服务器"

#: ../../backend.md:142
msgid "Run Llama 3.1 405B"
msgstr "运行 Llama 3.1 405B"

#: ../../backend.md:160
msgid "Benchmark Performance"
msgstr "基准性能"

#: ../../backend.md:162
msgid ""
"Benchmark a single static batch by running the following command without "
"launching a server. The arguments are the same as for `launch_server.py`. "
"Note that this is not a dynamic batching server, so it may run out of memory"
" for a batch size that a real server can handle. A real server truncates the"
" prefill into several batches, while this unit test does not. For accurate "
"large batch testing, please use `sglang.bench_serving` instead."
msgstr ""
"通过运行以下命令，在不启动服务器的情况下，对单个静态批次进行基准测试。参数与 `launch_server.py` "
"相同。请注意，这不是动态批处理服务器，因此它可能无法处理真实服务器可以处理的批次大小的内存。真实服务器会将预填充截断为多个批次，而此单元测试不会。为了进行准确的大批次测试，请使用"
" `sglang.bench_serving`。"

#: ../../backend.md:168
msgid ""
"Benchmark online serving. Launch a server first and run the following "
"command."
msgstr "基准测试在线服务。首先启动服务器，然后运行以下命令。"
