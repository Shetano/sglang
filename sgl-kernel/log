pip install scikit-build-core isort black
Requirement already satisfied: scikit-build-core in /usr/local/lib/python3.10/dist-packages (0.11.1)
Requirement already satisfied: isort in /usr/local/lib/python3.10/dist-packages (6.0.1)
Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (25.1.0)
Requirement already satisfied: exceptiongroup>=1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-build-core) (1.2.2)
Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.10/dist-packages (from scikit-build-core) (24.2)
Requirement already satisfied: pathspec>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from scikit-build-core) (0.12.1)
Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-build-core) (2.2.1)
Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.8)
Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black) (1.0.0)
Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.3.7)
Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.13.1)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.

[notice] A new release of pip is available: 25.0.1 -> 25.1
[notice] To update, run: python3 -m pip install --upgrade pip
[34mDEBUG[39m uv 0.6.14
[34mDEBUG[39m Found workspace root: `/sgl-workspace/sglang/sgl-kernel`
[34mDEBUG[39m Adding root workspace member: `/sgl-workspace/sglang/sgl-kernel`
[34mDEBUG[39m Searching for Python >=3.9 in virtual environments, managed installations, or search path
[34mDEBUG[39m Searching for managed installations at `/root/.local/share/uv/python`
[34mDEBUG[39m Found `cpython-3.10.12-linux-x86_64-gnu` at `/usr/bin/python3` (first executable in the search path)
[34mDEBUG[39m Using request timeout of 30s
[1mBuilding wheel...[0m
[34mDEBUG[39m No workspace root found, using project root
[34mDEBUG[39m Proceeding without build isolation
[34mDEBUG[39m Calling `scikit_build_core.build.build_wheel("/sgl-workspace/sglang/sgl-kernel/dist", {"build-dir":"build"}, None)`
*** scikit-build-core 0.11.1 using CMake 3.31.1 (wheel)
*** Configuring CMake...
loading initial cache file build/CMakeInit.txt
-- The CXX compiler identification is GNU 11.4.0
-- The CUDA compiler identification is NVIDIA 12.4.131 with host compiler GNU 11.4.0
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/x86_64-linux-gnu-g++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Found Python: /usr/bin/python3 (found version "3.10.12") found components: Interpreter Development.Module Development.SABIModule
-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version "12.4.131")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- Detected CUDA_VERSION=
-- Found CUDA: /usr/local/cuda (found version "12.4")
-- Found CUDAToolkit: /usr/local/cuda/include (found version "12.4.131")
-- PyTorch: CUDA detected: 12.4
-- PyTorch: CUDA nvcc is: /usr/local/cuda/bin/nvcc
-- PyTorch: CUDA toolkit directory: /usr/local/cuda
-- PyTorch: Header version is: 12.4
-- Found Python: /usr/bin/python3 (found version "3.10.12") found components: Interpreter
[33mCMake Warning at /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:140 (message):
  Failed to compute shorthash for libnvrtc.so
Call Stack (most recent call first):
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)
  CMakeLists.txt:36 (find_package)

[0m
[33mCMake Warning (dev) at /usr/local/share/cmake-3.31/Modules/FindPackageHandleStandardArgs.cmake:441 (message):
  The package name passed to `find_package_handle_standard_args` (nvtx3) does
  not match the name of the calling package (Caffe2).  This can lead to
  problems in calling code that expects `find_package` result variables
  (e.g., `_FOUND`) to follow a certain pattern.
Call Stack (most recent call first):
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:178 (find_package_handle_standard_args)
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)
  CMakeLists.txt:36 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.
[0m
-- Could NOT find nvtx3 (missing: nvtx3_dir)
[33mCMake Warning at /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:184 (message):
  Cannot find NVTX3, find old NVTX instead
Call Stack (most recent call first):
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)
  CMakeLists.txt:36 (find_package)

[0m
-- USE_CUDNN is set to 0. Compiling without cuDNN support
-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support
-- USE_CUDSS is set to 0. Compiling without cuDSS support
-- USE_CUFILE is set to 0. Compiling without cuFile support
-- Autodetected CUDA architecture(s):  9.0 9.0 9.0 9.0 9.0 9.0 9.0 9.0
-- Added CUDA NVCC flags for: -gencode;arch=compute_90,code=sm_90
[33mCMake Warning at /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):
  static library kineto_LIBRARY-NOTFOUND not found.
Call Stack (most recent call first):
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:121 (append_torchlib_if_found)
  CMakeLists.txt:36 (find_package)

[0m
-- Found Torch: /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so
-- Configuring done (117.2s)
[33mCMake Warning (dev) at /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:186 (set_property):
  The library that is being linked to, CUDA::nvToolsExt, is marked as being
  deprecated by the owner.  The message provided by the developer is:

  nvToolsExt has been superseded by nvtx3 since CUDA 10.0 and CMake 3.25.
  Use CUDA::nvtx3 and include <nvtx3/nvToolsExt.h> instead.

Call Stack (most recent call first):
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)
  CMakeLists.txt:36 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.
[0m
[33mCMake Warning (dev) at /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:186 (set_property):
  The library that is being linked to, CUDA::nvToolsExt, is marked as being
  deprecated by the owner.  The message provided by the developer is:

  nvToolsExt has been superseded by nvtx3 since CUDA 10.0 and CMake 3.25.
  Use CUDA::nvtx3 and include <nvtx3/nvToolsExt.h> instead.

Call Stack (most recent call first):
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)
  /usr/local/lib/python3.10/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)
  CMakeLists.txt:36 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.
[0m
-- Generating done (0.0s)
-- Build files have been written to: /sgl-workspace/sglang/sgl-kernel/build
*** Building project with Ninja...
[1/104] Building CXX object CMakeFiles/common_ops.dir/_deps/repo-flash-attention-src/csrc/flash_attn/flash_sparse_api.cpp.o
[2/104] Building CXX object CMakeFiles/flash_ops.dir/csrc/flash_extension.cc.o
[3/104] Building CXX object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/flash_api.cpp.o
[4/104] Building CXX object CMakeFiles/common_ops.dir/csrc/common_extension.cc.o
[5/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/attention/cascade.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cascade.cu: In function â€˜void merge_state(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cascade.cu:31:28: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   31 |   unsigned int seq_len = v_a.size(0);
      |                    ~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cascade.cu:32:30: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   32 |   unsigned int num_heads = v_a.size(1);
      |                      ~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cascade.cu:33:29: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   33 |   unsigned int head_dim = v_a.size(2);
      |                     ~~~~~~~~^~~
[6/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/elementwise/activation.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/activation.cu: In function â€˜void silu_and_mul(at::Tensor&, at::Tensor&, int64_t)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/activation.cu:37:24: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
   37 |   int d = input.size(-1) / 2;
      |         ~~~~~~~~~~~~~~~^~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/activation.cu:39:11: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   39 |   dim3 grid(num_tokens);
      |           ^~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/activation.cu: In function â€˜void gelu_tanh_and_mul(at::Tensor&, at::Tensor&, int64_t)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/activation.cu:53:24: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
   53 |   int d = input.size(-1) / 2;
      |         ~~~~~~~~~~~~~~~^~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/activation.cu:55:11: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   55 |   dim3 grid(num_tokens);
      |           ^~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/activation.cu: In function â€˜void gelu_and_mul(at::Tensor&, at::Tensor&, int64_t)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/activation.cu:69:24: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
   69 |   int d = input.size(-1) / 2;
      |         ~~~~~~~~~~~~~~~^~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/activation.cu:71:11: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   71 |   dim3 grid(num_tokens);
      |           ^~~~~~~~~~
[7/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/elementwise/rope.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/rope.cu: In function â€˜void apply_rope_pos_ids_cos_sin_cache(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, bool, int64_t)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/rope.cu:46:41: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   46 |   unsigned int rotary_dim = cos_sin_cache.size(1);
      |                       ~~~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/rope.cu:47:31: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   47 |   unsigned int num_qo_heads = q.size(1);
      |                         ~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/rope.cu:48:31: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   48 |   unsigned int num_kv_heads = k.size(1);
      |                         ~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/rope.cu:49:27: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   49 |   unsigned int head_dim = q.size(2);
      |                     ~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/rope.cu:50:22: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   50 |   unsigned int nnz = q.size(0);
      |                ~~~~~~^~~
[8/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/speculative/speculative_sampling.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/speculative_sampling.cu: In function â€˜void tree_speculative_sampling_target_only(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, double, double, bool, int64_t)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/speculative_sampling.cu:69:43: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   69 |   unsigned int batch_size = uniform_samples.size(0);
      |                       ~~~~~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/speculative_sampling.cu:70:43: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   70 |   unsigned int num_spec_step = accept_index.size(1);
      |                          ~~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/speculative_sampling.cu:71:44: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   71 |   unsigned int num_draft_tokens = candidates.size(1);
      |                             ~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/speculative_sampling.cu:72:40: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   72 |   unsigned int vocab_size = target_probs.size(2);
      |                       ~~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/speculative_sampling.cuh: In instantiation of â€˜cudaError_t flashinfer::sampling::TreeSpeculativeSamplingTargetOnly(IdType*, IdType*, IdType*, IdType*, IdType*, IdType*, IdType*, DType*, DType*, DType*, uint32_t, uint32_t, uint32_t, uint32_t, DType, DType, bool, cudaStream_t) [with DType = float; IdType = int; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/speculative_sampling.cu:122:79:   required from here
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/speculative_sampling.cuh:180:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  180 |   const uint32_t vec_size = std::gcd(16 / sizeof(DType), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
[9/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/bmm_fp8.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:844: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:856: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:859: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:862: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:566: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:578: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:581: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:584: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:700: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:712: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:715: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:718: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:566: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:578: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:581: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:584: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:833: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:845: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:848: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:851: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:566: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:578: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:581: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:584: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:700: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:712: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:715: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:718: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:566: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:578: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:581: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/bmm_fp8.cu:45:584: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
   45 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP8(B.scalar_type(), b_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^
[10/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/attention/merge_attn_states.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu: In instantiation of â€˜void merge_attn_states_launcher(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, at::Tensor&, at::Tensor&) [with scalar_t = float]â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:203:95:   required from here
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:160:36: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uintâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  160 |   const uint num_tokens = output.size(0);
      |                         ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:161:35: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uintâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  161 |   const uint num_heads = output.size(1);
      |                        ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:162:35: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uintâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  162 |   const uint head_size = output.size(2);
      |                        ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu: In instantiation of â€˜void merge_attn_states_launcher(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, at::Tensor&, at::Tensor&) [with scalar_t = __half]â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:203:242:   required from here
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:160:36: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uintâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  160 |   const uint num_tokens = output.size(0);
      |                         ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:161:35: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uintâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  161 |   const uint num_heads = output.size(1);
      |                        ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:162:35: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uintâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  162 |   const uint head_size = output.size(2);
      |                        ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu: In instantiation of â€˜void merge_attn_states_launcher(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, at::Tensor&, at::Tensor&) [with scalar_t = __nv_bfloat16]â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:203:400:   required from here
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:160:36: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uintâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  160 |   const uint num_tokens = output.size(0);
      |                         ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:161:35: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uintâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  161 |   const uint num_heads = output.size(1);
      |                        ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/merge_attn_states.cu:162:35: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uintâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  162 |   const uint head_size = output.size(2);
      |                        ~~~~~~~~~~~^~~
[11/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/flash_prepare_scheduler.cu.o
[12/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/speculative/packbit.cu.o
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/quantization.cuh: In function â€˜cudaError_t flashinfer::quantization::PackBits(bool*, uint8_t*, int64_t, flashinfer::quantization::BitOrder, cudaStream_t)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/quantization.cuh:87:179: warning: conversion from â€˜long intâ€™ to â€˜unsigned intâ€™ may change value [-Wconversion]
   87 |   DISPATCH_BITORDER(bitorder, BITORDER, {
      |                                                                                                                                                                                   ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/quantization.cuh:87:549: warning: conversion from â€˜long intâ€™ to â€˜unsigned intâ€™ may change value [-Wconversion]
   87 |   DISPATCH_BITORDER(bitorder, BITORDER, {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/packbit.cu: In function â€˜void segment_packbits(at::Tensor, at::Tensor, at::Tensor, at::Tensor, int64_t)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/packbit.cu:35:44: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   35 |   unsigned int batch_size = input_indptr.size(0) - 1;
      |                       ~~~~~~~~~~~~~~~~~~~~~^~~~~
[13/104] Building CUDA object CMakeFiles/common_ops.dir/_deps/repo-flash-attention-src/csrc/flash_attn/src/flash_fwd_sparse_hdim128_fp16_causal_sm80.cu.o
[14/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/speculative/eagle_utils.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/eagle_utils.cu: In function â€˜void build_tree_kernel_efficient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int64_t, int64_t, int64_t)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/eagle_utils.cu:130:26: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  130 |   int bs = parent_list.size(0);
      |          ~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/eagle_utils.cu:132:12: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  132 |   dim3 block(draft_token_num);
      |            ^~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/eagle_utils.cu: In function â€˜void verify_tree_greedy(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int64_t)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/eagle_utils.cu:230:38: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  230 |   unsigned int batch_size = candidates.size(0);
      |                       ~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/eagle_utils.cu:231:43: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  231 |   unsigned int num_spec_step = accept_index.size(1);
      |                          ~~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/speculative/eagle_utils.cu:232:44: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  232 |   unsigned int num_draft_tokens = candidates.size(1);
      |                             ~~~~~~~~~~~~~~~^~~
[15/104] Building CUDA object CMakeFiles/common_ops.dir/_deps/repo-flashinfer-src/csrc/norm.cu.o
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In function â€˜void rmsnorm(at::Tensor&, at::Tensor&, at::Tensor&, double, bool)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:32:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   32 |   unsigned int batch_size = input.size(0);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:33:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   33 |   unsigned int hidden_size = input.size(1);
      |                        ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:39:324: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   39 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                    ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:39:344: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   39 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:39:349: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
   39 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                             ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:39:308: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   39 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                    ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:39:328: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   39 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:39:333: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
   39 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                             ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In function â€˜void fused_add_rmsnorm(at::Tensor&, at::Tensor&, at::Tensor&, double, bool)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:64:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   64 |   unsigned int batch_size = input.size(0);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:65:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   65 |   unsigned int hidden_size = input.size(1);
      |                        ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:69:334: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   69 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:69:356: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   69 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                    ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:69:361: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
   69 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:69:318: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   69 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:69:340: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   69 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                    ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:69:345: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
   69 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In function â€˜void gemma_rmsnorm(at::Tensor&, at::Tensor&, at::Tensor&, double, bool)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:89:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   89 |   unsigned int batch_size = input.size(0);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:90:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   90 |   unsigned int hidden_size = input.size(1);
      |                        ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:96:329: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   96 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:96:349: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   96 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                             ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:96:354: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
   96 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                  ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:96:313: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   96 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:96:333: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   96 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                             ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:96:338: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
   96 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                  ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In function â€˜void gemma_fused_add_rmsnorm(at::Tensor&, at::Tensor&, at::Tensor&, double, bool)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:121:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  121 |   unsigned int batch_size = input.size(0);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:122:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  122 |   unsigned int hidden_size = input.size(1);
      |                        ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:126:339: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  126 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                   ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:126:361: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  126 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:126:366: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
  126 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:126:323: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  126 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                   ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:126:345: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  126 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:126:350: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
  126 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh: In instantiation of â€˜cudaError_t flashinfer::norm::RMSNorm(T*, T*, T*, uint32_t, uint32_t, uint32_t, uint32_t, float, bool, cudaStream_t) [with T = __half; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:39:150:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh:111:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  111 |   const uint32_t vec_size = std::gcd(16 / sizeof(T), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh: In instantiation of â€˜cudaError_t flashinfer::norm::RMSNorm(T*, T*, T*, uint32_t, uint32_t, uint32_t, uint32_t, float, bool, cudaStream_t) [with T = __nv_bfloat16; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:39:134:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh:111:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh: In instantiation of â€˜cudaError_t flashinfer::norm::FusedAddRMSNorm(T*, T*, T*, uint32_t, uint32_t, uint32_t, uint32_t, float, bool, cudaStream_t) [with T = __half; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:69:158:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh:239:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  239 |   const uint32_t vec_size = std::gcd(16 / sizeof(T), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh: In instantiation of â€˜cudaError_t flashinfer::norm::FusedAddRMSNorm(T*, T*, T*, uint32_t, uint32_t, uint32_t, uint32_t, float, bool, cudaStream_t) [with T = __nv_bfloat16; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:69:142:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh:239:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh: In instantiation of â€˜cudaError_t flashinfer::norm::GemmaRMSNorm(T*, T*, T*, uint32_t, uint32_t, uint32_t, uint32_t, float, bool, cudaStream_t) [with T = __half; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:96:155:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh:276:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  276 |   const uint32_t vec_size = std::gcd(16 / sizeof(T), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh: In instantiation of â€˜cudaError_t flashinfer::norm::GemmaRMSNorm(T*, T*, T*, uint32_t, uint32_t, uint32_t, uint32_t, float, bool, cudaStream_t) [with T = __nv_bfloat16; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:96:139:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh:276:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh: In instantiation of â€˜cudaError_t flashinfer::norm::GemmaFusedAddRMSNorm(T*, T*, T*, uint32_t, uint32_t, uint32_t, uint32_t, float, bool, cudaStream_t) [with T = __half; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:126:163:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh:311:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  311 |   const uint32_t vec_size = std::gcd(16 / sizeof(T), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh: In instantiation of â€˜cudaError_t flashinfer::norm::GemmaFusedAddRMSNorm(T*, T*, T*, uint32_t, uint32_t, uint32_t, uint32_t, float, bool, cudaStream_t) [with T = __nv_bfloat16; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/norm.cu:126:147:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh:311:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
[16/104] Building CUDA object CMakeFiles/common_ops.dir/_deps/repo-flashinfer-src/csrc/renorm.cu.o
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(1090): warning #177-D: variable "logits_greater_than_pivot" was declared but never referenced
      float logits_greater_than_pivot[VEC_SIZE];
            ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKMaskLogits(DType *, DType *, IdType *, uint32_t, uint32_t, uint32_t, cudaStream_t) [with DType=float, IdType=int]" at line 72 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(1090): warning #177-D: variable "logits_greater_than_pivot" was declared but never referenced
      float logits_greater_than_pivot[VEC_SIZE];
            ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKMaskLogits(DType *, DType *, IdType *, uint32_t, uint32_t, uint32_t, cudaStream_t) [with DType=float, IdType=int]" at line 72 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(1090): warning #177-D: variable "logits_greater_than_pivot" was declared but never referenced
      float logits_greater_than_pivot[VEC_SIZE];
            ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKMaskLogits(DType *, DType *, IdType *, uint32_t, uint32_t, uint32_t, cudaStream_t) [with DType=float, IdType=int]" at line 72 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(1090): warning #177-D: variable "logits_greater_than_pivot" was declared but never referenced
      float logits_greater_than_pivot[VEC_SIZE];
            ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKMaskLogits(DType *, DType *, IdType *, uint32_t, uint32_t, uint32_t, cudaStream_t) [with DType=float, IdType=int]" at line 72 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(1090): warning #177-D: variable "logits_greater_than_pivot" was declared but never referenced
      float logits_greater_than_pivot[VEC_SIZE];
            ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKMaskLogits(DType *, DType *, IdType *, uint32_t, uint32_t, uint32_t, cudaStream_t) [with DType=float, IdType=int]" at line 72 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu: In function â€˜void top_p_renorm_probs(at::Tensor, at::Tensor, std::optional<at::Tensor>, double)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:27:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   27 |   unsigned int batch_size = probs.size(0);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:28:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   28 |   unsigned int vocab_size = probs.size(1);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:33:237: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
   33 |   cudaError_t status = sampling::TopPRenormProb<float>(
      |                                                                                                                                                                                                                                             ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu: In function â€˜void top_k_renorm_probs(at::Tensor, at::Tensor, std::optional<at::Tensor>, int64_t)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:46:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   46 |   unsigned int batch_size = probs.size(0);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:47:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   47 |   unsigned int vocab_size = probs.size(1);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:52:235: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   52 |   cudaError_t status = sampling::TopKRenormProb<float>(
      |                                                                                                                                                                                                                                           ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu: In function â€˜void top_k_mask_logits(at::Tensor, at::Tensor, std::optional<at::Tensor>, int64_t)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:66:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   66 |   unsigned int batch_size = logits.size(0);
      |                       ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:67:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   67 |   unsigned int vocab_size = logits.size(1);
      |                       ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:72:235: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   72 |   cudaError_t status = sampling::TopKMaskLogits<float>(
      |                                                                                                                                                                                                                                           ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh: In instantiation of â€˜cudaError_t flashinfer::sampling::TopPRenormProb(DType*, DType*, float*, uint32_t, float, uint32_t, cudaStream_t) [with DType = float; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:33:55:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh:1321:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
 1321 |   const uint32_t vec_size = std::gcd(16 / sizeof(DType), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh: In instantiation of â€˜cudaError_t flashinfer::sampling::TopKRenormProb(DType*, DType*, IdType*, uint32_t, uint32_t, uint32_t, cudaStream_t) [with DType = float; IdType = int; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:52:55:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh:1340:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
 1340 |   const uint32_t vec_size = std::gcd(16 / sizeof(DType), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh: In instantiation of â€˜cudaError_t flashinfer::sampling::TopKMaskLogits(DType*, DType*, IdType*, uint32_t, uint32_t, uint32_t, cudaStream_t) [with DType = float; IdType = int; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/renorm.cu:72:55:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh:1362:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
 1362 |   const uint32_t vec_size = std::gcd(16 / sizeof(DType), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
[17/104] Building CUDA object CMakeFiles/common_ops.dir/_deps/repo-flash-attention-src/csrc/flash_attn/src/flash_fwd_sparse_hdim128_bf16_causal_sm80.cu.o
[18/104] Building CUDA object CMakeFiles/common_ops.dir/_deps/repo-flash-attention-src/csrc/flash_attn/src/flash_fwd_sparse_hdim128_fp16_sm80.cu.o
[19/104] Building CUDA object CMakeFiles/common_ops.dir/_deps/repo-flash-attention-src/csrc/flash_attn/src/flash_fwd_sparse_hdim128_bf16_sm80.cu.o
[20/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/attention/lightning_attention_decode_kernel.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu: In function â€˜void lightning_attention_decode(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, at::Tensor, at::Tensor)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:134:22: warning: conversion from â€˜long intâ€™ to â€˜unsigned intâ€™ may change value [-Wconversion]
  134 |   dim3 grid(batch_size * num_heads);
      |           ~~~~~~~~~~~^~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:447: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:459: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:470: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:478: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:442: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                          ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:454: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:465: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:473: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:462: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:474: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:485: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:493: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:482: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:494: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:505: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/csrc/attention/lightning_attention_decode_kernel.cu:138:513: warning: conversion from â€˜long intâ€™ to â€˜intâ€™ may change value [-Wconversion]
  138 |   AT_DISPATCH_FLOATING_TYPES_AND2(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^
[21/104] Building CUDA object CMakeFiles/common_ops.dir/_deps/repo-flashinfer-src/csrc/sampling.cu.o
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(510): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopPSamplingFromProb(T *, IdType *, IdType *, T *, uint32_t, T, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 75 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(395): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKSamplingFromProb(T *, IdType *, IdType *, T *, uint32_t, uint32_t, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 107 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(696): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKTopPSamplingFromProb(T *, IdType *, T *, IdType *, IdType *, uint32_t, IdType, T, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 174 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(510): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopPSamplingFromProb(T *, IdType *, IdType *, T *, uint32_t, T, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 75 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(395): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKSamplingFromProb(T *, IdType *, IdType *, T *, uint32_t, uint32_t, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 107 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(696): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKTopPSamplingFromProb(T *, IdType *, T *, IdType *, IdType *, uint32_t, IdType, T, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 174 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(510): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopPSamplingFromProb(T *, IdType *, IdType *, T *, uint32_t, T, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 75 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(395): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKSamplingFromProb(T *, IdType *, IdType *, T *, uint32_t, uint32_t, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 107 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(696): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKTopPSamplingFromProb(T *, IdType *, T *, IdType *, IdType *, uint32_t, IdType, T, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 174 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(510): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopPSamplingFromProb(T *, IdType *, IdType *, T *, uint32_t, T, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 75 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(395): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKSamplingFromProb(T *, IdType *, IdType *, T *, uint32_t, uint32_t, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 107 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(696): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKTopPSamplingFromProb(T *, IdType *, T *, IdType *, IdType *, uint32_t, IdType, T, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 174 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(510): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopPSamplingFromProb(T *, IdType *, IdType *, T *, uint32_t, T, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 75 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(395): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKSamplingFromProb(T *, IdType *, IdType *, T *, uint32_t, uint32_t, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 107 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh(696): warning #177-D: variable "batch_size" was declared but never referenced
    const uint32_t batch_size = gridDim.x;
                   ^
          detected during instantiation of "cudaError_t flashinfer::sampling::TopKTopPSamplingFromProb(T *, IdType *, T *, IdType *, IdType *, uint32_t, IdType, T, uint32_t, __nv_bool, uint64_t, uint64_t, cudaStream_t) [with T=float, IdType=int]" at line 174 of /sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu

/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu: In function â€˜void sampling_from_probs(at::Tensor, at::Tensor, std::optional<at::Tensor>, bool, std::optional<at::Generator>)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:34:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   34 |   unsigned int batch_size = output.size(0);
      |                       ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:35:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   35 |   unsigned int vocab_size = probs.size(1);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu: In function â€˜void top_p_sampling_from_probs(at::Tensor, at::Tensor, std::optional<at::Tensor>, std::optional<at::Tensor>, double, bool, std::optional<at::Generator>)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:62:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   62 |   unsigned int batch_size = output.size(0);
      |                       ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:63:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   63 |   unsigned int vocab_size = probs.size(1);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:75:328: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
   75 |   cudaError_t status = sampling::TopPSamplingFromProb<float, int>(
      |                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu: In function â€˜void top_k_sampling_from_probs(at::Tensor, at::Tensor, std::optional<at::Tensor>, std::optional<at::Tensor>, int64_t, bool, std::optional<at::Generator>)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:94:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   94 |   unsigned int batch_size = output.size(0);
      |                       ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:95:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   95 |   unsigned int vocab_size = probs.size(1);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:107:328: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  107 |   cudaError_t status = sampling::TopKSamplingFromProb<float, int>(
      |                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu: In function â€˜void min_p_sampling_from_probs(at::Tensor, at::Tensor, std::optional<at::Tensor>, std::optional<at::Tensor>, double, bool, std::optional<at::Generator>)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:126:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  126 |   unsigned int batch_size = output.size(0);
      |                       ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:127:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  127 |   unsigned int vocab_size = probs.size(1);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:139:328: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
  139 |   cudaError_t status = sampling::MinPSamplingFromProb<float, int>(
      |                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu: In function â€˜void top_k_top_p_sampling_from_probs(at::Tensor, at::Tensor, std::optional<at::Tensor>, std::optional<at::Tensor>, double, std::optional<at::Tensor>, double, bool, std::optional<at::Generator>)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:160:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  160 |   unsigned int batch_size = output.size(0);
      |                       ~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:161:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  161 |   unsigned int vocab_size = probs.size(1);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:174:410: warning: conversion from â€˜doubleâ€™ to â€˜intâ€™ may change value [-Wfloat-conversion]
  174 |   cudaError_t status = sampling::TopKTopPSamplingFromProb<float, int>(
      |                                                                                                                                                                                                                                                                                                                                                                                                                          ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:174:421: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
  174 |   cudaError_t status = sampling::TopKTopPSamplingFromProb<float, int>(
      |                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu: In function â€˜void chain_speculative_sampling(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, bool, std::optional<at::Generator>)â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:200:39: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  200 |   unsigned int batch_size = draft_probs.size(0);
      |                       ~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:201:49: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  201 |   unsigned int num_speculate_tokens = draft_probs.size(1);
      |                                 ~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:202:39: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  202 |   unsigned int vocab_size = draft_probs.size(2);
      |                       ~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh: In instantiation of â€˜cudaError_t flashinfer::sampling::SamplingFromProb(T*, IdType*, IdType*, uint32_t, uint32_t, bool, uint64_t, uint64_t, cudaStream_t) [with T = float; IdType = int; cudaError_t = cudaError; uint32_t = unsigned int; uint64_t = long unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:47:48:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh:809:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  809 |   const uint32_t vec_size = std::gcd(16 / sizeof(T), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh: In instantiation of â€˜cudaError_t flashinfer::sampling::TopPSamplingFromProb(T*, IdType*, IdType*, T*, uint32_t, T, uint32_t, bool, uint64_t, uint64_t, cudaStream_t) [with T = float; IdType = int; cudaError_t = cudaError; uint32_t = unsigned int; uint64_t = long unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:75:66:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh:859:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  859 |   const uint32_t vec_size = std::gcd(16 / sizeof(T), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh: In instantiation of â€˜cudaError_t flashinfer::sampling::TopKSamplingFromProb(T*, IdType*, IdType*, T*, uint32_t, uint32_t, uint32_t, bool, uint64_t, uint64_t, cudaStream_t) [with T = float; IdType = int; cudaError_t = cudaError; uint32_t = unsigned int; uint64_t = long unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:107:66:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh:830:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  830 |   const uint32_t vec_size = std::gcd(16 / sizeof(T), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh: In instantiation of â€˜cudaError_t flashinfer::sampling::MinPSamplingFromProb(T*, T*, IdType*, IdType*, uint32_t, float, uint32_t, bool, uint64_t, uint64_t, cudaStream_t) [with T = float; IdType = int; cudaError_t = cudaError; uint32_t = unsigned int; uint64_t = long unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:139:66:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh:885:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  885 |   const uint32_t vec_size = std::gcd(16 / sizeof(T), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh: In instantiation of â€˜cudaError_t flashinfer::sampling::TopKTopPSamplingFromProb(T*, IdType*, T*, IdType*, IdType*, uint32_t, IdType, T, uint32_t, bool, uint64_t, uint64_t, cudaStream_t) [with T = float; IdType = int; cudaError_t = cudaError; uint32_t = unsigned int; uint64_t = long unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:174:70:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh:911:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  911 |   const uint32_t vec_size = std::gcd(16 / sizeof(T), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh: In instantiation of â€˜cudaError_t flashinfer::sampling::ChainSpeculativeSampling(DType*, IdType*, DType*, IdType*, IdType*, IdType*, uint32_t, uint32_t, uint32_t, bool, uint64_t, uint64_t, cudaStream_t) [with DType = float; IdType = int; cudaError_t = cudaError; uint32_t = unsigned int; uint64_t = long unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/csrc/sampling.cu:220:70:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/sampling.cuh:1525:36: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
 1525 |   const uint32_t vec_size = std::gcd(16 / sizeof(DType), d);
      |                           ~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
[22/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/moe/moe_align_kernel.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:270: warning: conversion from â€˜long unsigned intâ€™ to â€˜int32_tâ€™ {aka â€˜intâ€™} may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:665: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:678: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1246: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1259: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1297: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1497: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:270: warning: conversion from â€˜long unsigned intâ€™ to â€˜int32_tâ€™ {aka â€˜intâ€™} may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:661: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:674: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1238: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1251: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1289: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1489: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:270: warning: conversion from â€˜long unsigned intâ€™ to â€˜int32_tâ€™ {aka â€˜intâ€™} may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:663: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:676: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1242: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1255: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1293: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1493: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:270: warning: conversion from â€˜long unsigned intâ€™ to â€˜int32_tâ€™ {aka â€˜intâ€™} may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:652: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:665: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1220: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1233: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1271: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1471: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:270: warning: conversion from â€˜long unsigned intâ€™ to â€˜int32_tâ€™ {aka â€˜intâ€™} may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:661: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:674: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1238: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1251: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1289: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_align_kernel.cu:178:1489: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  178 |   DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^
[23/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/attention/vertical_slash_index.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/attention/vertical_slash_index.cu: In function â€˜void convert_vertical_slash_indexes_64x64_mergehead(const int*, const int*, const int*, const int*, int*, int*, int*, int*, int*, int*, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, bool)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/vertical_slash_index.cu:358:20: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  358 |   const dim3 dimGrid(N_HEADS, BATCH_SIZE, (N_ROWS + N_THREADS - 1) / N_THREADS);
      |                    ^~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/vertical_slash_index.cu:358:29: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  358 |   const dim3 dimGrid(N_HEADS, BATCH_SIZE, (N_ROWS + N_THREADS - 1) / N_THREADS);
      |                             ^~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/vertical_slash_index.cu:358:70: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  358 |   const dim3 dimGrid(N_HEADS, BATCH_SIZE, (N_ROWS + N_THREADS - 1) / N_THREADS);
      |                                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/vertical_slash_index.cu: In function â€˜void convert_vertical_slash_indexes_mergehead(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int64_t, int64_t, int64_t, bool)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/vertical_slash_index.cu:381:36: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  381 |   int batch_size = slash_indexes.size(0);
      |                  ~~~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/vertical_slash_index.cu:382:35: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  382 |   int num_heads = slash_indexes.size(1);
      |                 ~~~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/vertical_slash_index.cu:383:35: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  383 |   int nnz_slash = slash_indexes.size(2);
      |                 ~~~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/vertical_slash_index.cu:384:41: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  384 |   int nnz_vertical = vertical_indexes.size(2);
      |                    ~~~~~~~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/vertical_slash_index.cu:385:54: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  385 |   int num_rows = (context_size + block_size_M - 1) / block_size_M;
      |                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
[24/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/allreduce/custom_all_reduce.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/allreduce/custom_all_reduce.cuh: In destructor â€˜vllm::CustomAllreduce::~CustomAllreduce()â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/allreduce/custom_all_reduce.cuh:479:304: warning: â€˜throwâ€™ will always call â€˜terminateâ€™ [-Wterminate]
  479 |       CHECK_CUDA_SUCCESS(cudaIpcCloseMemHandle(ptr));
      |                                                                                                                                                                                                                                                                                                                ^
/sgl-workspace/sglang/sgl-kernel/csrc/allreduce/custom_all_reduce.cuh:479:304: note: in C++11 destructors default to â€˜noexceptâ€™
/sgl-workspace/sglang/sgl-kernel/csrc/allreduce/custom_all_reduce.cu: In function â€˜fptr_t init_custom_ar(const std::vector<long int>&, at::Tensor&, int64_t, bool)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/allreduce/custom_all_reduce.cu:16:36: warning: conversion from â€˜std::vector<long int>::size_typeâ€™ {aka â€˜long unsigned intâ€™} to â€˜intâ€™ may change value [-Wconversion]
   16 |   int world_size = fake_ipc_ptrs.size();
      |                  ~~~~~~~~~~~~~~~~~~^~
/sgl-workspace/sglang/sgl-kernel/csrc/allreduce/custom_all_reduce.cu:25:94: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
   25 |   return (fptr_t) new vllm::CustomAllreduce(
      |                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/allreduce/custom_all_reduce.cu: In function â€˜void all_reduce(fptr_t, at::Tensor&, at::Tensor&, fptr_t, int64_t)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/allreduce/custom_all_reduce.cu:76:125: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
   76 |       fa->allreduce<float>(
      |                                                                                                                             ^
/sgl-workspace/sglang/sgl-kernel/csrc/allreduce/custom_all_reduce.cu:81:124: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
   81 |       fa->allreduce<half>(
      |                                                                                                                            ^
/sgl-workspace/sglang/sgl-kernel/csrc/allreduce/custom_all_reduce.cu:87:145: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
   87 |       fa->allreduce<nv_bfloat16>(
      |                                                                                                                                                 ^
[25/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/flash_fwd_combine.cu.o
[26/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/per_token_quant_fp8.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_quant_fp8.cu: In function â€˜void sgl_per_token_quant_fp8(at::Tensor, at::Tensor, at::Tensor)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_quant_fp8.cu:90:24: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
   90 |   const int num_blocks = num_tokens;
      |                        ^~~~~~~~~~
[27/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/elementwise/fused_add_rms_norm_kernel.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu: In function â€˜void sgl_fused_add_rmsnorm(at::Tensor, at::Tensor, at::Tensor, double, bool)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:38:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   38 |   unsigned int batch_size = input.size(0);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:39:34: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
   39 |   unsigned int hidden_size = input.size(1);
      |                        ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:333: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   43 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                             ^
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:355: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   43 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                   ^
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:360: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
   43 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:310: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   43 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:332: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   43 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                            ^
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:337: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
   43 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                 ^
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:318: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   43 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:340: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
   43 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                    ^
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:345: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
   43 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), c_type, [&] {
      |                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh: In instantiation of â€˜cudaError_t flashinfer::norm::FusedAddRMSNorm(T*, T*, T*, uint32_t, uint32_t, uint32_t, uint32_t, float, bool, cudaStream_t) [with T = float; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:157:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh:239:35: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
  239 |   const uint32_t vec_size = std::gcd(16 / sizeof(T), d);
      |                           ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh: In instantiation of â€˜cudaError_t flashinfer::norm::FusedAddRMSNorm(T*, T*, T*, uint32_t, uint32_t, uint32_t, uint32_t, float, bool, cudaStream_t) [with T = __half; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:134:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh:239:35: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh: In instantiation of â€˜cudaError_t flashinfer::norm::FusedAddRMSNorm(T*, T*, T*, uint32_t, uint32_t, uint32_t, uint32_t, float, bool, cudaStream_t) [with T = __nv_bfloat16; cudaError_t = cudaError; uint32_t = unsigned int; cudaStream_t = CUstream_st*]â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/elementwise/fused_add_rms_norm_kernel.cu:43:142:   required from here
/sgl-workspace/sglang/sgl-kernel/build/_deps/repo-flashinfer-src/include/flashinfer/norm.cuh:239:35: warning: conversion from â€˜std::common_type_t<long unsigned int, unsigned int>â€™ {aka â€˜long unsigned intâ€™} to â€˜uint32_tâ€™ {aka â€˜unsigned intâ€™} may change value [-Wconversion]
[28/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/grammar/apply_token_bitmask_inplace_cuda.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/grammar/apply_token_bitmask_inplace_cuda.cu: In function â€˜void ApplyTokenBitmaskInplace(at::Tensor, at::Tensor, std::optional<at::Tensor>)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/grammar/apply_token_bitmask_inplace_cuda.cu:209:26: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜int32_tâ€™ {aka â€˜intâ€™} may change value [-Wconversion]
  209 |     num_rows = indices->size(0);
      |            ~~~~~~~~~~~~~~^~~~
[29/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/nvfp4_scaled_mm_kernels.cu.o
[30/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/per_token_group_quant_8bit.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu: In function â€˜void sgl_per_token_group_quant_8bit(at::Tensor, at::Tensor, at::Tensor, int64_t, double, double, double)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:103:38: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  103 |   const int num_groups = input.numel() / group_size;
      |                        ~~~~~~~~~~~~~~^~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:129:41: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  129 |   const int scale_num_rows = output_s.size(1);
      |                            ~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:130:41: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  130 |   const int scale_stride = output_s.stride(1);
      |                          ~~~~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163:478: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  163 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), scalar_t, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163:856: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  163 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), scalar_t, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163:1366: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  163 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), scalar_t, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163:1751: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  163 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), scalar_t, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163:2335: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  163 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), scalar_t, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163:2714: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  163 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), scalar_t, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163:3225: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  163 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), scalar_t, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163:3611: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  163 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), scalar_t, [&] {
      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu: In lambda function:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  163 |   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FLOAT_FP16(input.scalar_type(), scalar_t, [&] {
      |
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_token_group_quant_8bit.cu:163: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
[31/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/per_tensor_quant_fp8.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_tensor_quant_fp8.cu: In function â€˜void sgl_per_tensor_quant_fp8(at::Tensor, at::Tensor, at::Tensor, bool)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/per_tensor_quant_fp8.cu:102:37: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  102 |   const int num_elements = input.numel();
      |                          ~~~~~~~~~~~^~
[32/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/nvfp4_quant_kernels.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(89): warning #940-D: missing return statement at end of non-void function "fp32_vec_to_e2m1"
  }
  ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(123): warning #940-D: missing return statement at end of non-void function "fp32_vec_to_e2m1(float2 (&)[4])"
  }
  ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(54): warning #177-D: variable "CVT_FP4_ELTS_PER_THREAD" was declared but never referenced
  constexpr int CVT_FP4_ELTS_PER_THREAD = 8;
                ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(55): warning #177-D: variable "CVT_FP4_SF_VEC_SIZE" was declared but never referenced
  constexpr int CVT_FP4_SF_VEC_SIZE = 16;
                ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(89): warning #940-D: missing return statement at end of non-void function "fp32_vec_to_e2m1"
  }
  ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(123): warning #940-D: missing return statement at end of non-void function "fp32_vec_to_e2m1(float2 (&)[4])"
  }
  ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(54): warning #177-D: variable "CVT_FP4_ELTS_PER_THREAD" was declared but never referenced
  constexpr int CVT_FP4_ELTS_PER_THREAD = 8;
                ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(55): warning #177-D: variable "CVT_FP4_SF_VEC_SIZE" was declared but never referenced
  constexpr int CVT_FP4_SF_VEC_SIZE = 16;
                ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(89): warning #940-D: missing return statement at end of non-void function "fp32_vec_to_e2m1"
  }
  ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(123): warning #940-D: missing return statement at end of non-void function "fp32_vec_to_e2m1(float2 (&)[4])"
  }
  ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(54): warning #177-D: variable "CVT_FP4_ELTS_PER_THREAD" was declared but never referenced
  constexpr int CVT_FP4_ELTS_PER_THREAD = 8;
                ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(55): warning #177-D: variable "CVT_FP4_SF_VEC_SIZE" was declared but never referenced
  constexpr int CVT_FP4_SF_VEC_SIZE = 16;
                ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(89): warning #940-D: missing return statement at end of non-void function "fp32_vec_to_e2m1"
  }
  ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(123): warning #940-D: missing return statement at end of non-void function "fp32_vec_to_e2m1(float2 (&)[4])"
  }
  ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(54): warning #177-D: variable "CVT_FP4_ELTS_PER_THREAD" was declared but never referenced
  constexpr int CVT_FP4_ELTS_PER_THREAD = 8;
                ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(55): warning #177-D: variable "CVT_FP4_SF_VEC_SIZE" was declared but never referenced
  constexpr int CVT_FP4_SF_VEC_SIZE = 16;
                ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(89): warning #940-D: missing return statement at end of non-void function "fp32_vec_to_e2m1"
  }
  ^

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(123): warning #940-D: missing return statement at end of non-void function "fp32_vec_to_e2m1(float2 (&)[4])"
  }
  ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(54): warning #177-D: variable "CVT_FP4_ELTS_PER_THREAD" was declared but never referenced
  constexpr int CVT_FP4_ELTS_PER_THREAD = 8;
                ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu(55): warning #177-D: variable "CVT_FP4_SF_VEC_SIZE" was declared but never referenced
  constexpr int CVT_FP4_SF_VEC_SIZE = 16;
                ^

/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu: In function â€˜void scaled_fp4_quant_sm100a(at::Tensor&, const at::Tensor&, at::Tensor&, const at::Tensor&)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu:367:23: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜int32_tâ€™ {aka â€˜intâ€™} may change value [-Wconversion]
  367 |   int32_t m = input.size(0);
      |             ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/nvfp4_quant_kernels.cu:368:23: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜int32_tâ€™ {aka â€˜intâ€™} may change value [-Wconversion]
  368 |   int32_t n = input.size(1);
      |             ~~~~~~~~~~^~~
[33/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/nvfp4_scaled_mm_entry.cu.o
[34/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/nvfp4_quant_entry.cu.o
[35/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/moe/moe_topk_softmax_kernels.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_topk_softmax_kernels.cu: In function â€˜void topk_softmax(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_topk_softmax_kernels.cu:484:43: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  484 |   const int num_experts = gating_output.size(-1);
      |                         ~~~~~~~~~~~~~~~~~~^~~~
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_topk_softmax_kernels.cu:485:46: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  485 |   const int num_tokens = gating_output.numel() / num_experts;
      |                        ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_topk_softmax_kernels.cu:486:35: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  486 |   const int topk = topk_weights.size(-1);
      |                  ~~~~~~~~~~~~~~~~~^~~~
[36/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/moe/moe_fused_gate.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu: In function â€˜std::vector<at::Tensor> moe_fused_gate(at::Tensor&, at::Tensor&, int64_t, int64_t, int64_t, int64_t, double)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:371:33: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜int32_tâ€™ {aka â€˜intâ€™} may change value [-Wconversion]
  371 |   int32_t num_experts = input.size(1);
      |                       ~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:394:32: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  394 |   int computed_vpt = num_experts / num_expert_group;
      |                    ~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:416:198: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  416 |           LAUNCH_MOE_GATE_CONFIG(bfloat16_t, 256, 8);
      |                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:418:198: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  418 |           LAUNCH_MOE_GATE_CONFIG(float16_t, 256, 8);
      |                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:420:198: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  420 |           LAUNCH_MOE_GATE_CONFIG(float32_t, 256, 8);
      |                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:424:201: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  424 |             LAUNCH_MOE_GATE_CONFIG(bfloat16_t, 256, 16);
      |                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:426:201: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  426 |             LAUNCH_MOE_GATE_CONFIG(float16_t, 256, 16);
      |                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:428:201: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  428 |             LAUNCH_MOE_GATE_CONFIG(float32_t, 256, 16);
      |                                                                                                                                                                                                         ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:435:198: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  435 |           LAUNCH_MOE_GATE_CONFIG(bfloat16_t, 128, 4);
      |                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:437:198: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  437 |           LAUNCH_MOE_GATE_CONFIG(float16_t, 128, 4);
      |                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:439:198: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  439 |           LAUNCH_MOE_GATE_CONFIG(float32_t, 128, 4);
      |                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:443:198: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  443 |             LAUNCH_MOE_GATE_CONFIG(bfloat16_t, 128, 8);
      |                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:445:198: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  445 |             LAUNCH_MOE_GATE_CONFIG(float16_t, 128, 8);
      |                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:447:198: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  447 |             LAUNCH_MOE_GATE_CONFIG(float32_t, 128, 8);
      |                                                                                                                                                                                                      ^
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:457:30: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  457 |       moe_fused_gate_kernel_dynamic<bfloat16_t><<<num_blocks, block_dim, 0, stream>>>(
      |                              ^~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:470:30: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  470 |       moe_fused_gate_kernel_dynamic<float16_t><<<num_blocks, block_dim, 0, stream>>>(
      |                              ^~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/moe/moe_fused_gate.cu:483:30: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜unsigned intâ€™ may change value [-Wconversion]
  483 |       moe_fused_gate_kernel_dynamic<float32_t><<<num_blocks, block_dim, 0, stream>>>(
      |                              ^~~~~~~~~~
[37/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/awq_kernel.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/awq_kernel.cu: In function â€˜at::Tensor awq_dequantize(at::Tensor, at::Tensor, at::Tensor)â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/awq_kernel.cu:185:32: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  185 |   int qweight_rows = qweight.size(0);
      |                    ~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/awq_kernel.cu:186:32: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  186 |   int qweight_cols = qweight.size(1);
      |                    ~~~~~~~~~~~~^~~
/sgl-workspace/sglang/sgl-kernel/csrc/gemm/awq_kernel.cu:187:31: warning: conversion from â€˜int64_tâ€™ {aka â€˜long intâ€™} to â€˜intâ€™ may change value [-Wconversion]
  187 |   int group_size = qweight_rows / scales.size(0);
      |                  ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~
[38/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/moe/fp8_blockwise_moe_kernel.cu.o
[39/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/fp8_blockwise_gemm_kernel.cu.o
[40/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_e4m3_packgqa_sm90.cu.o
[41/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_e4m3_sm90.cu.o
[42/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_e4m3_split_sm90.cu.o
[43/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_e4m3_split_softcap_sm90.cu.o
[44/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/attention/cutlass_mla_kernel.cu.o
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu: In instantiation of â€˜typename T::Fmha::Arguments args_from_options(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&) [with T = MlaSm100<cutlass::half_t>; typename T::Fmha::Arguments = cutlass::fmha::kernel::Sm100FmhaMlaKernelTmaWarpspecialized<cute::tuple<cute::C<128>, cute::C<128>, cute::tuple<cute::C<512>, cute::C<64> > >, cutlass::half_t, float, cutlass::half_t, float, cutlass::fmha::kernel::Sm100MlaPersistentTileScheduler, true>::Arguments; typename T::Fmha = cutlass::fmha::device::MLA<cutlass::fmha::kernel::Sm100FmhaMlaKernelTmaWarpspecialized<cute::tuple<cute::C<128>, cute::C<128>, cute::tuple<cute::C<512>, cute::C<64> > >, cutlass::half_t, float, cutlass::half_t, float, cutlass::fmha::kernel::Sm100MlaPersistentTileScheduler, true> >]â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu:162:58:   required from â€˜void runMla(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, cudaStream_t) [with Element = cutlass::half_t; cudaStream_t = CUstream_st*]â€™
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu:182:26:   required from here
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu:102:21: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
  102 |   float scale = 1.0 / sqrt(1.0 * (D_non_latent + D_rope));
      |               ~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu: In instantiation of â€˜typename T::Fmha::Arguments args_from_options(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&) [with T = MlaSm100<cutlass::bfloat16_t, IsPersistent<true> >; typename T::Fmha::Arguments = cutlass::fmha::kernel::Sm100FmhaMlaKernelTmaWarpspecialized<cute::tuple<cute::C<128>, cute::C<128>, cute::tuple<cute::C<512>, cute::C<64> > >, cutlass::bfloat16_t, float, cutlass::bfloat16_t, float, cutlass::fmha::kernel::Sm100MlaPersistentTileScheduler, true>::Arguments; typename T::Fmha = cutlass::fmha::device::MLA<cutlass::fmha::kernel::Sm100FmhaMlaKernelTmaWarpspecialized<cute::tuple<cute::C<128>, cute::C<128>, cute::tuple<cute::C<512>, cute::C<64> > >, cutlass::bfloat16_t, float, cutlass::bfloat16_t, float, cutlass::fmha::kernel::Sm100MlaPersistentTileScheduler, true> >]â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu:162:58:   required from â€˜void runMla(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, cudaStream_t) [with Element = cutlass::bfloat16_t; cudaStream_t = CUstream_st*]â€™
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu:184:30:   required from here
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu:102:21: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu: In instantiation of â€˜typename T::Fmha::Arguments args_from_options(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&) [with T = MlaSm100<cutlass::float_e4m3_t, IsPersistent<true> >; typename T::Fmha::Arguments = cutlass::fmha::kernel::Sm100FmhaMlaKernelTmaWarpspecialized<cute::tuple<cute::C<128>, cute::C<128>, cute::tuple<cute::C<512>, cute::C<64> > >, cutlass::float_e4m3_t, float, cutlass::float_e4m3_t, float, cutlass::fmha::kernel::Sm100MlaPersistentTileScheduler, true>::Arguments; typename T::Fmha = cutlass::fmha::device::MLA<cutlass::fmha::kernel::Sm100FmhaMlaKernelTmaWarpspecialized<cute::tuple<cute::C<128>, cute::C<128>, cute::tuple<cute::C<512>, cute::C<64> > >, cutlass::float_e4m3_t, float, cutlass::float_e4m3_t, float, cutlass::fmha::kernel::Sm100MlaPersistentTileScheduler, true> >]â€™:
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu:162:58:   required from â€˜void runMla(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, cudaStream_t) [with Element = cutlass::float_e4m3_t; cudaStream_t = CUstream_st*]â€™
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu:186:32:   required from here
/sgl-workspace/sglang/sgl-kernel/csrc/attention/cutlass_mla_kernel.cu:102:21: warning: conversion from â€˜doubleâ€™ to â€˜floatâ€™ may change value [-Wfloat-conversion]
[45/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_e4m3_softcap_sm90.cu.o
[46/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_e4m3_paged_softcap_sm90.cu.o
[47/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_e4m3_paged_sm90.cu.o
[48/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_e4m3_paged_split_softcap_sm90.cu.o
[49/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_e4m3_paged_split_sm90.cu.o
[50/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_e4m3_softcap_packgqa_sm90.cu.o
[51/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/int8_gemm_kernel.cu.o
[52/104] Building CUDA object CMakeFiles/common_ops.dir/csrc/gemm/fp8_gemm_kernel.cu.o
[53/104] Linking CXX shared module common_ops.abi3.so
[54/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_fp16_paged_softcap_sm90.cu.o
[55/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_fp16_paged_sm90.cu.o
[56/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_sm90.cu.o
[57/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_bf16_paged_split_softcap_sm90.cu.o
[58/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_bf16_paged_sm90.cu.o
[59/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_bf16_paged_split_sm90.cu.o
[60/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_e4m3_paged_softcap_sm90.cu.o
[61/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_bf16_paged_softcap_sm90.cu.o
[62/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_fp16_paged_split_softcap_sm90.cu.o
[63/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_e4m3_sm90.cu.o
[64/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_bf16_packgqa_sm90.cu.o
[65/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_e4m3_paged_sm90.cu.o
[66/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_fp16_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb0ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb0ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[67/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_fp16_split_sm90.cu.o
[68/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_e4m3_split_sm90.cu.o
[69/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_fp16_split_softcap_sm90.cu.o
[70/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_e4m3_paged_split_softcap_sm90.cu.o
[71/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_bf16_softcap_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb0ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb0ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[72/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_fp16_paged_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb1ELb0ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
[73/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_e4m3_paged_split_sm90.cu.o
[74/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_e4m3_softcap_packgqa_sm90.cu.o
[75/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_e4m3_packgqa_sm90.cu.o
[76/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_fp16_packgqa_sm90.cu.o
[77/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_fp16_sm90.cu.o
[78/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_bf16_softcap_packgqa_sm90.cu.o
[79/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_fp16_paged_softcap_sm90.cu.o
ptxas info    : (C7510) Potential Performance Loss: wgmma.mma_async instructions are serialized due to wgmma pipeline crossing function boundary at a function call in the function '_ZN7cutlass13device_kernelIN5flash20enable_sm90_or_laterINS1_16FlashAttnFwdSm90INS1_25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS5_1CILi1EEES8_S8_EEENS6_IJNS7_ILi64EEESA_SA_EEELi512ENS_6half_tEfNS_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb0ELb1ELb0ELb0EEENS1_21CollectiveEpilogueFwdINS6_IJSA_NS7_ILi512EEESA_EEES9_SC_SE_Li256ELb1ELb1ELb0ELb0EEENS1_36VarlenDynamicPersistentTileSchedulerILi64ELi256ELi128ELb0ELb1ELb1EEEEEEEEEvNT_6ParamsE'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
[80/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_bf16_sm90.cu.o
[81/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_bf16_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb0ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb0ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[82/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_bf16_split_sm90.cu.o
[83/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_bf16_paged_softcap_sm90.cu.o
ptxas info    : (C7510) Potential Performance Loss: wgmma.mma_async instructions are serialized due to wgmma pipeline crossing function boundary at a function call in the function '_ZN7cutlass13device_kernelIN5flash20enable_sm90_or_laterINS1_16FlashAttnFwdSm90INS1_25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS5_1CILi1EEES8_S8_EEENS6_IJNS7_ILi64EEESA_SA_EEELi512ENS_10bfloat16_tEfNS_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb0ELb1ELb0ELb0EEENS1_21CollectiveEpilogueFwdINS6_IJSA_NS7_ILi512EEESA_EEES9_SC_SE_Li256ELb1ELb1ELb0ELb0EEENS1_36VarlenDynamicPersistentTileSchedulerILi64ELi256ELi128ELb0ELb1ELb1EEEEEEEEEvNT_6ParamsE'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
[84/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_fp16_paged_split_softcap_sm90.cu.o
ptxas info    : (C7510) Potential Performance Loss: wgmma.mma_async instructions are serialized due to wgmma pipeline crossing function boundary at a function call in the function '_ZN7cutlass13device_kernelIN5flash20enable_sm90_or_laterINS1_16FlashAttnFwdSm90INS1_25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS5_1CILi1EEES8_S8_EEENS6_IJNS7_ILi64EEESA_SA_EEELi512ENS_6half_tEfNS_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb0ELb1ELb1ELb0EEENS1_21CollectiveEpilogueFwdINS6_IJSA_NS7_ILi512EEESA_EEES9_SC_SE_Li256ELb1ELb1ELb1ELb0EEENS1_36VarlenDynamicPersistentTileSchedulerILi64ELi256ELi128ELb1ELb1ELb1EEEEEEEEEvNT_6ParamsE'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
[85/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_fp16_softcap_packgqa_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[86/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_fp16_softcap_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb0ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb0ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb1ELb0ELb0ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb0ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi32ELb0ELb0ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[87/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_bf16_softcap_sm90.cu.o
[88/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_bf16_packgqa_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[89/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_fp16_softcap_sm90.cu.o
[90/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_fp16_softcap_packgqa_sm90.cu.o
[91/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_fp16_split_softcap_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[92/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_fp16_split_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[93/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_bf16_paged_split_softcap_sm90.cu.o
ptxas info    : (C7510) Potential Performance Loss: wgmma.mma_async instructions are serialized due to wgmma pipeline crossing function boundary at a function call in the function '_ZN7cutlass13device_kernelIN5flash20enable_sm90_or_laterINS1_16FlashAttnFwdSm90INS1_25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS5_1CILi1EEES8_S8_EEENS6_IJNS7_ILi64EEESA_SA_EEELi512ENS_10bfloat16_tEfNS_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb0ELb1ELb1ELb0EEENS1_21CollectiveEpilogueFwdINS6_IJSA_NS7_ILi512EEESA_EEES9_SC_SE_Li256ELb1ELb1ELb1ELb0EEENS1_36VarlenDynamicPersistentTileSchedulerILi64ELi256ELi128ELb1ELb1ELb1EEEEEEEEEvNT_6ParamsE'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb1ELb0ELb1ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
[94/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_bf16_softcap_packgqa_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[95/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_bf16_paged_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb1ELb0ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
[96/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_bf16_paged_split_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb1ELb0ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
[97/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_fp16_packgqa_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb0ELb0EEENS_30DynamicPersistentTileSchedulerILi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb0ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb0ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[98/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_e4m3_softcap_sm90.cu.o
[99/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_bf16_split_softcap_sm90.cu.o
[100/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_fp16_paged_split_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE2_NS3_ILb0EEES1W_EEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1P_S1Q_S1R_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass6half_tEfNSA_4arch4Sm90ELb1ELb0ELb0ELb1ELb1ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_13PipelineAsyncILi2EEES19_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S19_S19_S1C_S1E_S1G_iS1H_S1L_S1M_S1O_EUlRS1P_iE0_NS3_ILb1EEES1W_EEDaiS1P_S1Q_S1R_'
[101/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_bf16_split_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb0ELb0ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[102/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimall_e4m3_split_softcap_sm90.cu.o
[103/104] Building CUDA object CMakeFiles/flash_ops.dir/_deps/repo-flash-attention-src/hopper/instantiations/flash_fwd_hdimdiff_bf16_split_softcap_sm90.cu.o
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb0ELb0ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb0ELb1ELb1ELb0EEENS_19SingleTileSchedulerILb0ELb1ELb1ELi128EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb0ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb0ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE0_NS3_ILb1EEES1Y_EEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb0ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb0EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
ptxas info    : (C7515) Potential Performance Loss: wgmma.mma_async instructions are serialized due to non wgmma instructions defining accumulator registers of a wgmma between start and end of the pipeline stage in the function '_ZZN5flash25CollectiveMainloopFwdSm90ILi2EN4cute5tupleIJNS1_1CILi1EEES4_S4_EEENS2_IJNS3_ILi128EEENS3_ILi112EEENS3_ILi64EEEEEELi256EN7cutlass10bfloat16_tEfNSA_4arch4Sm90ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb1ELb0ELb1ELb1ELb0EE3mmaINS_16FlashAttnFwdSm90ISE_NS_21CollectiveEpilogueFwdINS2_IJS6_NS3_ILi256EEES7_EEES5_SB_SD_Li256ELb1ELb1ELb1ELb0EEENS_36VarlenDynamicPersistentTileSchedulerILi128ELi256ELi128ELb1ELb1ELb1EEEE13SharedStorageENS1_6TensorINS1_11ArrayEngineIfLm128EEENS1_6LayoutINS2_IJNS2_IJNS3_ILi2EEEST_NS3_ILi32EEEEEES4_S4_EEENS2_IJNS2_IJS4_ST_NS3_ILi4EEEEEENS3_ILi0EEESZ_EEEEEEENS_7SoftmaxILi2ELi0EEEEEbRKNSE_6ParamsENSA_25PipelineTmaAsyncNoClusterILi2ENSA_16PipelineTmaAsyncILi2EEEEES1B_RNSA_13PipelineStateILj2EEERT0_RT1_iRiRKNS_16SeqlenInfoQKNewKILb1ELb1EEENS2_IJiiiiEEERT_ENKUliT_T0_T1_E_clIZSF_ISO_S12_S14_EbS17_S1B_S1B_S1E_S1G_S1I_iS1J_S1N_S1O_S1Q_EUlRS1R_iE1_NS3_ILb0EEENS3_ILb1EEEEEDaiS1R_S1S_S1T_'
[104/104] Linking CXX shared module flash_ops.abi3.so
*** Installing project into wheel...
-- Install configuration: "Release"
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/sgl_kernel/common_ops.abi3.so
-- Set non-toolchain portion of runtime path of "/tmp/tmp7a4czbkd/wheel/platlib/sgl_kernel/common_ops.abi3.so" to ""
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/sgl_kernel/flash_ops.abi3.so
-- Set non-toolchain portion of runtime path of "/tmp/tmp7a4czbkd/wheel/platlib/sgl_kernel/flash_ops.abi3.so" to ""
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/deep_gemm
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/deep_gemm/fp8_gemm.cuh
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/deep_gemm/mma_utils.cuh
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/deep_gemm/scheduler.cuh
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/deep_gemm/tma_utils.cuh
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/deep_gemm/utils.cuh
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit/interleave_ffma.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit/template.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit/runtime.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit/compiler.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit/__init__.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit_kernels
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit_kernels/m_grouped_gemm.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit_kernels/gemm.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit_kernels/tuner.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit_kernels/__init__.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/jit_kernels/utils.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/__init__.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/utils.py
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/int_tuple.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/util
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/util/print.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/util/type_traits.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/util/debug.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/partitioner.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_traits_sm90.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm90_gmma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm70.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm89.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_traits_sm90_im2col.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm90.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_traits_sm100_im2col.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm120_sparse.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_traits.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_traits_sm50.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_traits_sm75.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm100.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_traits_sm100_tma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_traits_sm100.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm120.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm75.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_atom.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm61.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_atom.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm80.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_traits_sm90_tma_swizzle.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm90_gmma_sparse.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_traits_sm90_tma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm90_gmma_ext.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/mma_traits_sm90_gmma_sparse_ext.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/atom/copy_traits_sm80.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/numeric
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/numeric/arithmetic_tuple.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/numeric/numeric_types.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/numeric/integer_sequence.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/numeric/complex.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/numeric/math.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/numeric/int.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/numeric/integral_ratio.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/numeric/real.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/numeric/integral_constant.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/pointer.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/pointer_flagged.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/pointer_sparse.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/config.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/layout_composed.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/swizzle_layout.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/swizzle.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/prefetch.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/clear.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/cooperative_gemm.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/functional.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/prefer.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/tensor_algorithms.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/gemm.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/cooperative_copy.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/copy.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/fill.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/tuple_algorithms.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/algorithm/axpby.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/stride.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/pointer_swizzle.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/tensor_zip.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/tensor.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/pointer_base.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/tensor_impl.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/simd_sm100.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm120.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/copy_sm100_tma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/copy_sm90_desc.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm90_gmma_ext.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm90_gmma_sparse_ext.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/copy_sm80.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm80.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/config.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm90_gmma_sparse.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/copy_sm90_tma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/cluster_sm90.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/copy_sm50.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm75.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/copy_sm100.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm100_desc.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm61.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm89.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm90_gmma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/copy_sm90.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/copy_sm75.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/copy.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm100_umma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm120_sparse.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm90_desc.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm100.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm90.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/util.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/cluster_sm100.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/tmem_allocator_sm100.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/arch/mma_sm70.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/container
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/container/array_aligned.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/container/type_list.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/container/array.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/container/alignment.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/container/bit_field.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/container/array_subbyte.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/container/cuda_types.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/container/tuple.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/underscore.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/tensor_predicate.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cute/layout.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/kernel_launch.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/device
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/device/reduce_split_k.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/device/tensor_reduce_affine_strided.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/device/tensor_reduce.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/thread
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/thread/reduce.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/thread/reduction_operators.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/threadblock_swizzle.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/kernel
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/kernel/reduce_softmax_final.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/kernel/reduce_split_k.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm_coord.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed/device
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed/device/dist_gemm_universal_wrapper.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed/device/full_barrier.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed/device/detail.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed/schedules
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed/schedules/dist_gemm_1d_schedules.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed/schedules/dist_gemm_base_schedule.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed/kernel
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed/kernel/dist_gemm_kernel_wrapper.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed/kernel/full_barrier.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/experimental/distributed/kernel/detail.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/subbyte_reference.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/floating_point_nvrtc.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/wmma_array.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/blas3.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/array_planar_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/predicate_vector.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/float8.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/pitch_linear_thread_map.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/device
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/device/transform_universal_adapter.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/thread
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/thread/transpose.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/thread/unary_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/warp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/warp/vector_fragment_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/collective
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/collective/sm90_wgmma_transpose.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/ell_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/regular_tile_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/regular_tile_access_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/vector_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/predicated_tile_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/kernel
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/kernel/sm90_sparse_gemm_compressor.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/kernel/filter_format_transformer.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/transform/kernel/sparse_gemm_compressor.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/blas3_types.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/fast_math.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/scale_type.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_tensor_broadcast.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_residual_block.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_relu0.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_generic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_relu.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/activation.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_gelu.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/reduction_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_dgelu.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_planar_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_hardswish.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/conversion_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_params.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_generic_with_scaling.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_clamp.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_sigmoid.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_bias_relu.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/detail.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_silu.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_drelu.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/tile_iterator_simt.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/tensor_op_policy.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/volta_tensor_op_policy.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/simt_policy.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/warp/fragment_iterator_simt.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/callbacks.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/sm120_visitor_store_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/sm90_visitor_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/sm90_visitor_load_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/sm100_visitor_store_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/operations.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/sm90_visitor_store_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/sm100_visitor_compute_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/sm120_callbacks_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/sm90_visitor_compute_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/sm100_callbacks_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/sm90_visitor_topk_softmax.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/fusion/sm90_callbacks_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/epilogue_tensor_broadcast.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/default_epilogue.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/sm100_epilogue_array_nosmem.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/sm70_epilogue_vectorized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/default_epilogue_array.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/sm100_epilogue_nosmem.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/collective_epilogue.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/builders
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/builders/sm90_common.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/builders/sm120_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/builders/sm100_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/builders/sm90_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/builders/sm120_common.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/sm100_epilogue_array_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/collective_builder.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/sm70_epilogue_vectorized_array.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized_bias_elementwise.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/sm90_epilogue_array_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/detail.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/sm90_epilogue_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/collective/sm100_epilogue_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/dispatch_policy.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_simt.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/shared_load_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/output_iterator_parameter.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_base.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_workspace.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_thread_map_simt.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/predicated_tile_iterator_conv.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/interleaved_epilogue.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_linear.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/output_tile_thread_map.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_streamk_with_broadcast.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_depthwise.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/fusion
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/fusion/visitor_2x.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/fusion/visitor_load.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/fusion/visitor_compute.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/fusion/visitors.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/fusion/visitor_store.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_with_visitor_callbacks.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_direct_store.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/epilogue_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/barrier.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/tensor_ref_planar_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/trace.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/cuda_host_adapter.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/tfloat32.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/thread
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/thread/matrix.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/exmy_base.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/kernel_hardware_info.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/semaphore.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/matrix_coord.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/array_subbyte.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/device_kernel.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/convnd_problem_shape.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/device
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/device/conv_universal_adapter.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/device/direct_convolution.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/device/implicit_gemm_convolution.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/thread
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/thread/depthwise_mma.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/conv3d_problem_size.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/warp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/warp/mma_depthwise_simt.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/warp/scale_bias_relu_transform.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/convolution.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/conv2d_problem_size.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/collective
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/collective/sm90_implicit_gemm_gmma_ss_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/collective/collective_conv.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/collective/builders
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/collective/builders/sm90_common.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/collective/builders/sm100_common.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/collective/builders/sm90_gmma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/collective/builders/sm100_umma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/collective/collective_builder.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/collective/sm100_implicit_gemm_umma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/collective/detail.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/dispatch_policy.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/depthwise_mma_base.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_params.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/threadblock_swizzle.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/implicit_gemm_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_params.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/threadblock/conv2d_tile_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/detail.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/sm90_implicit_gemm_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv3d_fprop.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_deconv3d_with_broadcast.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv2d_fprop.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv2d.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_depthwise_fprop.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv3d_dgrad.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv2d_fprop_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_deconv3d.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv3d_wgrad.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/conv_universal.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv2d_wgrad.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/sm100_implicit_gemm_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/direct_convolution.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/implicit_gemm_convolution_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv3d_fprop_with_broadcast.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/implicit_gemm_convolution.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv2d_dgrad.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_deconv2d.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_deconv2d_with_broadcast.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_with_k_reduction.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/rank_2k_grouped.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/base_grouped.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/symm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/trmm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_sparse_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_sparse.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemv.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/ell_gemm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_universal_adapter.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_sparse_universal_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_batched.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/rank_2k.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_splitk_parallel.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_array.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_grouped.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_universal_streamk_with_broadcast.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/rank_k.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_universal_base.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_sparse_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/default_gemm_configuration.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_sparse_with_visitor.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/device/gemm_universal_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/thread
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/thread/mma.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/thread/mma_sm50.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/thread/mma_sm60.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/thread/mma_sm61.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/gemm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_sparse_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_simt_tile_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/softmax_scale_bias_transform.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_simt.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_mixed_input_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_complex_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_simt_policy.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/tile_iterator_planar_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op_wmma.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op_policy.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/default_mma_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op_sm70.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_planar_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/scale_bias_tile_iterator.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/gemm_enumerated_types.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm100_blockscaled_mma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm100_mma_array_warpspecialized_blockwise_scaling.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm100_mma_warpspecialized_blockwise_scaling.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm80_mma_multistage.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized_mixed_input.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_array_tma_gmma_ss_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm100_mma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm100_blockscaled_sparse_mma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_array_tma_gmma_rs_warpspecialized_mixed_input.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_sparse_mma_tma_gmma_ss_warpspecialized_fp8.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm120_mma_tma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_ss_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8_blockwise_scaling.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_array_tma_gmma_ss_warpspecialized_fp8_blockwise_scaling.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm100_mma_array_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm100_sparse_mma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_sparse_mma_tma_gmma_ss_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm90_common.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm120_mma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm100_9xBF16_umma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm90_sparse_gmma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm100_blockscaled_umma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm120_sparse_mma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm100_blockscaled_sparse_umma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm100_common.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm100_blockwise_umma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm1xx_sparse_config.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm120_blockscaled_sparse_mma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm90_gmma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm120_blockscaled_mma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm120_common.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm90_sparse_config.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm100_pipeline_carveout.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm100_umma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm100_sparse_umma_builder.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/builders/sm1xx_common.inl
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm100_mma_warpspecialized_emulated.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/collective_builder.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm120_sparse_mma_tma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm70_mma_twostage.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/fp8_accumulation.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_multistage_gmma_rs_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm120_blockscaled_mma_tma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_tma_gmma_rs_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/collective_mma_decl.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm90_mma_array_tma_gmma_ss_warpspecialized_fp8.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm120_blockscaled_sparse_mma_tma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm100_mma_array_warpspecialized_emulated.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/sm100_blockscaled_mma_array_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/collective_mma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/collective/collective_builder_decl.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/dispatch_policy.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_base.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_planar_complex_base.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_core_simt.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_core_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/gemv.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_core.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_sparse_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/ell_mma_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_blas3_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/threadblock_swizzle.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_gemv_core.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_core_wmma.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_trmm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_with_reduction.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_sparse_mma.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_core_sm75.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/index_remat.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_singlestage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_sparse_base.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/ell_mma_pipelined.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_pipelined.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_ell_mma.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/threadblock/default_mma_core_sm70.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/group_array_problem_shape.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemv_batched_strided.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm100_gemm_array_tma_warpspecialized_input_transform.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/tile_scheduler_params.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_gemm_array_tma_warpspecialized_cooperative.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/rank_2k_grouped.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_pingpong.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_rank_k_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_ell_gemm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm100_tile_scheduler_stream_k.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_rank_2k_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/rank_2k_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_sparse_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sparse_gemm_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_tile_scheduler.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemv.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/ell_gemm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_gemm_array_tma_warpspecialized_pingpong.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_universal_streamk.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/rank_k_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/tile_scheduler.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_grouped_per_group_scale.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_universal_with_visitor.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_pipelined.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_rank_2k_grouped.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_sparse_universal_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_planar_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_gemm_tma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm100_gemm_array_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_batched.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm100_gemm_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_trmm_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm100_gemm_tma_warpspecialized_input_transform.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_tile_scheduler_group.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm100_tile_scheduler.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_universal_with_visitor.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_planar_complex_array.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm100_gemm_array_tma_warpspecialized_mma_transform.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_splitk_parallel.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_sparse_with_visitor.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm120_gemm_tma_warpspecialized_cooperative_asymmetric_dma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_array.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sparse_gemm_with_visitor.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_grouped.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_tile_scheduler_stream_k.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_cooperative.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm100_sparse_gemm_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_rank_k.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_symm_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/grouped_problem_visitor.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_grouped_per_group_scale.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/tile_scheduler_detail.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/trmm_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_sparse_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_params.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm100_static_tile_scheduler.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_trmm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_streamk_with_fused_epilogue.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/static_tile_scheduler.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm70_gemm.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_sparse.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/symm_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized_cooperative.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_grouped.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/params_sparse_base.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm100_tile_scheduler_group.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_universal.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_symm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_symm_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_universal_decl.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/params_universal_base.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sparse_gemm.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm100_gemm_tma_warpspecialized_mma_transform.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_rank_2k.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_streamk_with_broadcast.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_trmm_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_transpose_operands.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_rank_2k_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_sparse_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_rank_k_universal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_universal_with_visitor_streamk.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemm_sparse_universal_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/gemm_with_absmax.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/default_gemv.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm/kernel/sm90_gemm_warpspecialized.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/matrix.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/cutlass.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/tensor_ref.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/uint128.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/integer_subbyte.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/cluster_launch.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/platform
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/platform/platform.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/numeric_types.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/tensor_coord.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/gemm_coord.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/constants.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/pitch_linear_coord.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/pipeline
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/pipeline/sm90_pipeline.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/pipeline/pipeline.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/pipeline/sm100_pipeline.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/functional.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/half.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/real.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/aligned_buffer.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/array.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/quaternion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/mma_sm90.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/mma_sm70.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/wmma_sm70.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/simd.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/memory_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/barrier.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/memory_sm75.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/wmma_sm72.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/simd_sm60.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/mma_sm75.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/memory.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/simd_sm61.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/config.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/synclog.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/wmma.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/wmma_sm75.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/arch.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/mma_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/cache_operation.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/mma_sm89.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/mma.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/mma_sparse_sm89.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/mma_sm50.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/mma_sm60.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/mma_sparse_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/reg_reconfig.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/grid_dependency_control.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/arch/mma_sm61.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/core_io.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/layout
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/layout/tensor_op_multiplicand_sm75.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/layout/permute.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/layout/tensor.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/layout/tensor_op_multiplicand_sm70.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/layout/layout.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/layout/pitch_linear.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/layout/matrix.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/layout/tensor_op_multiplicand_sm80.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/layout/vector.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/kernel_hardware_info.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/block_striped.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/tensor_view_planar_complex.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/collective.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/sm100_tmem_helper.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/mma.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/sm100_blockscaled_layout.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/dependent_false.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/cluster.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/mainloop_fusion_helper_scale_factor.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/sm100_blockwise_scale_layout.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/helper_macros.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/collective
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/collective/mixed_input_utils.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/detail/layout.hpp
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/relatively_equal.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/matrix_shape.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/numeric_size.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/tensor_view.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/float_subbyte.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/version.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/workspace.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/numeric_conversion.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/bfloat16.h
-- Installing: /tmp/tmp7a4czbkd/wheel/platlib/deep_gemm/include/cutlass/coord.h
*** Making wheel...
*** Created sgl_kernel-0.1.0-cp39-abi3-linux_x86_64.whl
Successfully built [36m[1mdist/sgl_kernel-0.1.0-cp39-abi3-linux_x86_64.whl[0m[39m
Processing ./dist/sgl_kernel-0.1.0-cp39-abi3-linux_x86_64.whl
Installing collected packages: sgl-kernel
  Attempting uninstall: sgl-kernel
    Found existing installation: sgl-kernel 0.0.9.post2
    Uninstalling sgl-kernel-0.0.9.post2:
      Successfully uninstalled sgl-kernel-0.0.9.post2
Successfully installed sgl-kernel-0.1.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.

[notice] A new release of pip is available: 25.0.1 -> 25.1
[notice] To update, run: python3 -m pip install --upgrade pip
Succeed to rebuild
