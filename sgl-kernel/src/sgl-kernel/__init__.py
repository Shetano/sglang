import torch

if torch.version.hip is not None:
    from sgl_kernel.ops import (
        all_reduce_reg,
        all_reduce_unreg,
        allocate_meta_buffer,
        apply_rope_with_cos_sin_cache_inplace,
        bmm_fp8,
        dispose,
        fp8_scaled_mm,
        fused_add_rmsnorm,
        gelu_and_mul,
        gelu_tanh_and_mul,
        gemma_fused_add_rmsnorm,
        gemma_rmsnorm,
        get_graph_buffer_ipc_meta,
        get_meta_buffer_ipc_handle,
        init_custom_ar,
        int8_scaled_mm,
        lightning_attention_decode,
        meta_size,
        min_p_sampling_from_probs,
        moe_align_block_size,
        register_buffer,
        register_graph_buffers,
        rmsnorm,
        sampling_scaling_penalties,
        silu_and_mul,
        top_k_renorm_prob,
        top_k_top_p_sampling_from_probs,
        top_p_renorm_prob,
    )

    __all__ = [
        "all_reduce_reg",
        "all_reduce_unreg",
        "allocate_meta_buffer",
        "apply_rope_with_cos_sin_cache_inplace",
        "bmm_fp8",
        "dispose",
        "fp8_scaled_mm",
        "fused_add_rmsnorm",
        "gelu_and_mul",
        "gelu_tanh_and_mul",
        "gemma_fused_add_rmsnorm",
        "gemma_rmsnorm",
        "get_graph_buffer_ipc_meta",
        "get_meta_buffer_ipc_handle",
        "init_custom_ar",
        "int8_scaled_mm",
        "lightning_attention_decode",
        "meta_size",
        "min_p_sampling_from_probs",
        "moe_align_block_size",
        "register_buffer",
        "register_graph_buffers",
        "rmsnorm",
        "sampling_scaling_penalties",
        "silu_and_mul",
        "top_k_renorm_prob",
        "top_k_top_p_sampling_from_probs",
        "top_p_renorm_prob",
    ]
else:
    from sgl_kernel.ops import (
        apply_rope_with_cos_sin_cache_inplace,
        bmm_fp8,
        custom_dispose,
        custom_reduce,
        fp8_scaled_mm,
        fused_add_rmsnorm,
        gelu_and_mul,
        gelu_tanh_and_mul,
        gemma_fused_add_rmsnorm,
        gemma_rmsnorm,
        get_graph_buffer_ipc_meta,
        init_custom_reduce,
        int8_scaled_mm,
        lightning_attention_decode,
        min_p_sampling_from_probs,
        moe_align_block_size,
        register_graph_buffers,
        rmsnorm,
        sampling_scaling_penalties,
        silu_and_mul,
        top_k_renorm_prob,
        top_k_top_p_sampling_from_probs,
        top_p_renorm_prob,
    )

    __all__ = [
        "apply_rope_with_cos_sin_cache_inplace",
        "bmm_fp8",
        "custom_dispose",
        "custom_reduce",
        "fp8_scaled_mm",
        "fused_add_rmsnorm",
        "gelu_and_mul",
        "gelu_tanh_and_mul",
        "gemma_fused_add_rmsnorm",
        "gemma_rmsnorm",
        "get_graph_buffer_ipc_meta",
        "init_custom_reduce",
        "int8_scaled_mm",
        "lightning_attention_decode",
        "min_p_sampling_from_probs",
        "moe_align_block_size",
        "register_graph_buffers",
        "rmsnorm",
        "sampling_scaling_penalties",
        "silu_and_mul",
        "top_k_renorm_prob",
        "top_k_top_p_sampling_from_probs",
        "top_p_renorm_prob",
    ]
